#!/usr/bin/perl

#{{{1----------------------------------------------
#-----------------------------------------------------------------------------------------
# MOLLY - A MO-DULE FOR LI-TERATE PROGRAMMING
#-----------------------------------------------------------------------------------------
# ....../Literate source of the script/.......
# ............................................
# .......licensed under GPL version 3.........
# ............................................
# ....... Author: unixtechie; email: .........
# .. /same/ ..  at .. yahoo .. dot .. com ....
# ............................................
# ..... Git depos. - docs and download: ......
# http://github.com/unixtechie/Literate-Molly/
# ............................................
#
#-----------------------------------------------------------------------------------------
#---|-4-|-8-|-12----|-20------|-30------|-40------|-50------|-60------|-70------|-80------|



#--------------CONFIGURATION-----------------------
# -------TOC and INDEX behaviour------

# print TOC? 1:0
#$print_toc=1;  # default is to print

# should we keep TOC expanded? "block":"none"
#$toc_expanded="block"; # default is to unfold

# should we keep Chunks Index expanded? "block":"none"
#$ind_expanded="none";  # default is to keep folded


# should we number lines in code sections? 1 : else
#$line_numbering = 1;   # default is to number


#-------------------------------------
#---------MathML options--------------
# should we enable MathML via ASCIIMathML.js or LaTeXMathML.js library? 1:0
#$enable_ASCIIMathML = 0; #default is to disable as it slows Molly down a lot
#$enable_ASCIIMathML = 1;

# If yes, what is the full path to the lib? Remember to get the one with proper
# escapes for your work, default or modified (see documenation)
# CAN BE: (a) local "/full/path/from/root/to/ASCIIMathML_with_modified_escapes.js" or 
# (b) in current dir "ASCIIMathML_with_modified_escapes.js" or
# (c) on the web, e.g. the original site of the library (unmodified) is:
#$path_to_ASCIIMathML = "http://www1.chapman.edu/~jipsen/mathml/ASCIIMathML.js";
#$path_to_ASCIIMathML = "ASCIIMathML_with_modified_escapes.js"; # default is current dir


#-------------------------------------
#--------Document Markup lang---------

# how are doc sections marked? "dotHTML":"rawHTML"
 $weave_markup = "dotHTML"; # default is "rawHTML"


#--------------------------------------
#---------File extensions and tangling options------
 
# what is the file extention to weave it? (perms must allow execution!)
# e.g. "scriptname.weave" or "scriptname.cgi" etc.
#$weave_extension = "weave";    # default is "weave"

# what is the file extention to tangle it? (perms must allow execution!)
# e.g. "scriptname.tangle",  "scriptname.pl" etc.
#$tangle_extension = "tangle";  # default is "tangle"

#When tangling, should I use the built-in tangler? 0:1
# (if 0, the "pass-through" tangling will call "notangle"
# from Ramsey's "noweb" tools, must be installed and in your path)
# use_builtin_tangler = 0; # default for now is to use external "notangle"
$use_builtin_tangler = 1; 

# Print LitSource's line no's as a reference in the tangled output?
#$print_ref_linenos_when_tangling = 0; # default = 0, no.
#$print_ref_linenos_when_tangling = 1;

# ..and the comment for this line-numbering tangle-out and the language will be: 
#$code_sections_comment_symbol = "# "; #default is the hash-space


#--------------------------------------
#---------invocation of MOLLY.pl-------
do "MOLLY.pl";
exit;
__DATA__
#--------------start of script---------------------------
#--------------------------------------------------------
#-----------------------------------------------------}}}1

.<. center .>. .b. Licensing ./b.
MOLLY is licensed under GNU Public License version 3. .<. /center .>.




#--------------------------------------------------
.+h1. MOLLY MODULE ./h1.
#--------------------------------------------------

.b. What is it? ./b.

.i. Answer: ./i. Technically, this is "MOLLY.pl", a MO-dule for LI-terate programming, a
    small perl script that implements a new type of "weaver" for Literate Programming.
    This is is an attempt to combine Literate Programming (as practiced with
    "noweb" tools by Norman Ramsey) and autogenerated document folding/outlining with
    Javascript and HTML -- to escape scalability limitation of pure L.P. and enable a number
    of new source file management techniques.

.br.
#------------------------------------------------------------------
{{{2 .h2. THE CHUNK TO TANGLE BY  DEFAULT ./h2.
#------------------------------------------------------------

.b. .i. All disabled for now ./i. ./b. - I do not tangle anything from here at the moment
(although meaningful pieces exist, such as configuration templates).
I am not going to tangle the 2 pieces together at the moment -- therefore "*" is disabled

<<*>>=
<<MOLLY.pl module>>
@

<<*-dis>>=
<<seek-peek tangler>>
@

<<*-dis>>=
<<spacing.pl>>
@
<<*-dis>>=
<<tanglediff.test2.pl>>
@

<<*-dis>>=
<<md5 hashing from perl>>
@

<<*-dis>>=
<<check topo sort>>
@


<<*-dis>>=
<<check strings expansion>>
@

}}}2

.h2. ./h2.

#-------------------------------------------------------------------------------------
{{{2 .h2. PART I ----------- CONCEPTS ------------ ./h2.
#-------------------------------------------------------------------------------------

If you do not know what Literate Programming is already, skip this section.
This is mostly argument.

#-------------------------------------------------------------------------------------
{{{3 .h3. Краткое описание для русских пользователей ./h3.
#--------------------------------------------------------------

.b. 1. Что такое Molly? ./b. 

Это МО-дуль для ЛИ-тературного программирования (возможно, его лучше называть "Грамотным
Программированием" - термин Дональда Кнута, изобретателя подхода, намеренно двусмыслен).

Традиционно написанный "на литературном" исходный файл обрабатывался двумя программами:
"weave", дословно "сплетателем", который создавал красиво отформатированный, готовый для
печати (с помощью кнутовского ТеХ'а) текст, или "tangle", "спутывателем",  который выделял
из литературного готовый к компилированию программный код. Философия заключалась в том, что
код надо писать в последовательности естественной для логики мысли, "запутывая" его 
в "сеть" или "паутину" как того требует машина. 
.i. [Кроме того, в названии заключена игра слов отсылающая на стихотворение  Мармион 
Вальтера Скотта, одна фраза из которого осталась популярна в современном журналистском 
английском: "oh, what a tangled web we weave when we practice to deceive" Первая программа 
для литпрограммирования, написанная Кнутом, называлась WEB. В те былинные времена  
интернет-веба еще не существовало] ./i.

.b. Во-первых./b., Молли подменяет ТеХ более простым и общеизвестным сегодня  HTML-форматированием,
с одним дополнением: результат создается в формате "складного HTML", который облегчает работу
программиста и позволяет масштабировать лит. программирование.

.b.Во-вторых./b., Молли может работать в традиционном режиме с командной строки, но позволяет и
упрощенный, ускоренный режим, когда в лит-исходник вставляется вызов Molly, так, что ваш
текст сам становится скриптом на пёрле. Затем держа вашу директорию под местным веб сервером, 
вы можете не отдавая промежуточных команд читать отформатированный документ, который вы создаете,
сразу через веб браузер, обновляя страницу  клавишей Ctrl-R.

При этом ориентироваться в большом исходник и думать лучше глядя на форматированный документ
"орлиным глазом" сверху, видя его структуру и полностью разгрузив мозг от слепого метания
в редакторе (даже если он позволяет "складывать" текст, редактор все равно предполагает, что
общую структуру вы обязаны держать в памяти). 
Изменения же в файл легко вносить в "локальном режиме", ссылаясь на нужное место в редакторе
по номерам строк из общего документа. Подробнее об идее "складного HTML" см. ниже.


Molly  сейчас .b. самодостаточный скрипт./b. на простом пёрле без дополнительных модулей
и не требует внешних программ. Он использует, однако, .b.разметку Лит. Программирования
примененную в "noweb" tools by Norman Ramsey./b. (с одним ужесточением к формату, 
.i. каждый раздел с кодом обязан заканчиваться "@ в первой позиции на отдельной строке"./i.
- Ramsey's tool  позволяет эти символы по желанию выкидывать, если за разделом идет другой
раздел с кодом.). Для сравнения результатов (noweb применется больше 10 лет) или ради иных
вариантов обработки вы можете установить noweb  и обрабатывать им файлы, написанные из-под
Molly.


.br.
.b. 2. Что такое Literate programming? ./b.

Нет, это не "программы утопающие в море слов", и не "жирные комментарии, которые можно
вставить в исходник с кодом", самое частое заблуждение относительно Л.П. Это пожалуй даже не
"программы написанные как литературные эссе, с блеском", как тоже часто считают, подслушав
одну из цитат Кнута.

Во-первых, это .b."программирование спецификациями, изложенными на человеческом языке"./b..
Можно сказать, что это написание точных программ произвольными фразами на псевдокоде. Мы
все знаем, что "на псевдокоде - это понятно", потому что чуть ли не все учебники программирования
так нас впервые знакомили с предметом.
Оказывается, и довольно очевидно, такой подход может применяться к строгому программированию.

Во-вторых, это .b. альтернатива делению программы на подпрограммы/функции ./b., там, где они
не нужны машине (напр. рекурсии используют встроеный конструкт функций по существу), но 
применяются лишь для того, чтобы "не потерять нить рассуждений". Наверно, 90% дроблений
делается программистами чтобы не сойти с ума, и, по иронии, один из методов "оптимизации кода"
состоитв "unrolling", разворачивании созданных подделений.

Кнут, вначале создававший удобную систему документирования мыслей, позже переназвал Literate
Programming "альтернативой процедурному программированию", парадигме победившей после 
1970х своего исторического предшественника, который они обозвали spaghetti code и провели
под предлогом угрозы миру программирования от одного мелкого, но террористического
оператора. "Goto" was "considered harmful".
Yep, computer geeks are passionate souls. Программисты пассионарные личности. Интересно, 
что Дийкстра и Кнут тогда оказались по разную сторону баррикад, последний издал статью
с примерами "умного" применения goto.

Поэтому в-третьих, это .b. прием "управления вниманием"./b., который годаздо более общ, чем
программирование. Как "литературную программу" можно писать что угодно, отчет по работе,
заметки для памяти научную статью или приключенческий роман.

Вот простой конкретный пример.

.i. .b. ..not finished yet.. раздел не завершен.. ./b../i.

}}}3



.h3. LP as "management of thinking" ./h3. 

    If you never heard of Literate Programming, please read something like the Wikipedia
    article .a. http://en.wikipedia.org/wiki/Literate_programming ./a.
    and then something like documentation to Norman Ramsey's "noweb" tools and his old
    but excellent article  on the subject (linked from the site).
    .a. http://www.cs.tufts.edu/~nr/noweb ./a.

    I could actually offer three different definitions of L.P.

    .b. (1) L.P. is writing programs with arbitrary phrases as pseudocode, which 
    then become precise new operators of the literate "meta-language", turned
    into correctly ordered programming statements after "tangling". ./b.

    .b. (2) L.P. is writing programs from precise specifications, expressed in
    "normal" human language, rather than with formalized symbols ./b.

    .b. (3) L.P. is a technique for "management of thinking" that helps to
    relieve short-term memory of "housekeeping" tasks when creating complicated
    texts and "splits attention levels" for "global thinking" and "local changing"./b.
    In this sense the technique is not about programming and is applicable to any
    texts.

    The general feeling when working with L.P. tools is that of "thinking aloud"
    and writing down notes of your thoughts in the order of your thinking.

    The biggest problem in managing complex texts of any kind - programming simply
    being the most complicated one - the texts are supposed to contain no errors at
    all - is in holding it all together in one's mind.

    Human short-term memory is surprisingly tiny, up to 7 to 9 objects simultaneously
    at any one time, psychologists tell us. Therefore, when faced with larger tasks,
    humans need to use a number of "aoivdance techniques" to dance around the limitation.

    In my experience a script or a program longer than, say 1000 to 1200 lines becomes
    a major problem unless special techniques are used to help one's attention.

    In programming, probably 90% of "chunking", i.e. division into procedures, or "objects" or
    "functions" are not needed for the machine. They are done .b. simply for the humans not
    to lose the thread of their thinking. ./b. Whithout this things would become totally 
    unmanageable.

    This is why people write "domain-specific languages": humans chunk lower-level
    operations into a new 'unit' or 'abstraction', then manipulate higher-level
    abstractions over abstractions over abstractions over abstractions..

    "Literate Programming" is not a programming-specific technique that allows us to
    create arbitrary systems of abstractions .i. with phrases in a human language ./i.
    - "in pseudocode" - while staying precise in our specification of the final product.
    Everyone knows that explanation with pseudocode are helpful - that was the way we
    learned of the subject of programming in the first place. Most textbooks are written
    in this way.

    So L.P. .b..i. is an alternative to procedures and subroutines in the machine language ./i../b.

    For example, I can easily write and maintain na LPSource file with

<<logical switch>>=
 if (<<condition A>>) {
    <<a thousand-line-long piece of code>>}
 else { 
    <<another thousand-line-long piece of code>>}
@

    (and specify all those references elsewhere), while such code written directly would be
    unmaintainable and almost impossible to understand to a human. As a result, a human would
    .i. have to ./i. wrap his code into some subs, with inherent limitations and complications
    on their parameters and return result, built into his programming language.
    The machine would slow down its execution because internally it will maintain environment
    to switch to when returning etc.
    Isnt't it ironic that "hand-made optimisations" include "unrolling" programming constructs
    programmers introduced on the first place? The reason most probably was that otherwise
    a human mind would not be able to keep the thread, it would have lost it, while  all the
    introduced paraphenalia is totally unnecessary for the machine execution.

    Subroutines or functions are sometimes necessary. To give one example - to create a recursive
    invocation the programmer uses the built-in ability of a function to push its current state
    on the stack and then to automatically pick up where left upon returning.
    But probably 90% of all sub-dividing in programming is done just for the humans not to get
    lost in the sea of code.

    So, a slightly expanded third definition, which is relevant to my attempt to marry L.P. and
    outlining is:
    .ul. .i. .b. Literate Programming is a system of free-form macros in a natural human 
    language used as an alternative to programming language procedures 
    (functions) when those serve as props to limit load on human attention
    span, and totally unnecessary for machine execution in themselves ./b. ./i. ./ul.


.h3. Three problems with L.P. -- unification of L.P. and folding ./h3.

    Donald Knuth first was thinking of how to document his code when he created a mish-mash of
    previous attempts at similar management techniques with macros, which he called "WEB" (at
    the time when no "www-web" existed yet).
    Then he proclaimed this was the best way to write programs he knows, and repositioned it
    as "a programming paradigm", in direct competition to the "structural programming",
    introduced in the previous decade.

    Acceptance of this new method of program writing however was disappointing. I believe, there
    were and still are three factors at play, which hurt it, even though the name of L.P. creator
    ensured L.P. would never fall into complete obscurity.

    .ul.
        .li. .b. First, L.P. used TeX as a formatting tool ./b., which is a sort of programming language in
        itself. Therefore, the first thing to blame L.P. for became "but I'd need to write - .i. and 
        debug! ./i. - two programs at the same time, in TeX and in my own machine language.

        Today when much, much simpler markup languages abound - "html" or even "markdown 
        html" or even "wiki" for the majority of uses that do not require typesetting complicated
        equations or graphs - this problem has long been solved.
        You do not need to "debug" basic HTML markup - any error is displayed in your browser as a
        distortion of the web page, I do not need to "debug" html when writing with my Molly script
        which uses html markup.

        ./li. .li. .b. Secondly, the first generation of L.P. tools was language-specific. ./b. You could 
        do Pascal, but not Fortran. Later "C" and "Fortran" literate tools were added, but you could
        not do Lisp or Ada etc.

        Today with tools like "noweb" by Norman Ramsey this problem has long been solved. Simplified
        to its core idea Literate Programming is now totally language-independent to the degree that
        you can write books or essays with L.P. tools not even knowing how to program for the computer.

        ./li. .li. .b. And the third problem is the flat file structure of Literate Source files ./b. and
        (probably that could be listed as the fourth, psychological stumbling block) the insistence
        on "producing clean documentation" or "polished essays" of one's programs other could read
        and enjoy.

        Flat files continue to limit the size of the literate source files: your mind still .i. has to
        keep internally the overall structure of your program ./i. and switch from it to doing concrete
        tasks.
        Technically a Literate Source file can include .i. many ./i. program files ( e.g. project.h,
        project.c, a makefile and a README). In reality the size is limited by your "attention horizon"
    ./li. ./ul.

    .b. I propose to solve  this last problem by combining L.P. with folding or outlining ./b. ,
    using html and javascript as the simplest and ubiquitous tools.

    The Literate Source file is written in basic HTML, or even in "markdown" or "wiki"-style markup
    notation. The script then automatically creates a document with folds at each heading line
    (i.e. "h1", "h2", ...). The file can be "weaved" from command line or produced dynamically 
    under a web  server as a CGI document.

    Folding must allow opening an arbitrary number of sections from any place in the file to create
    a sub-view of the document, a small document which is relevant to some particular sub-task.
    These sub-views must persist for the time of this particular work, i.e. the user should not
    be forced to reopen his subview over and over again. That is, I should in effect be able to read
    snippets constituting 2 pages of text relevant to my immediate task contiguously from  a 
    5MB file  and  not even notice that, as the rest is tucked out of the way.
    
    Common practice in IDEs and outlining editors is an arrangement of folding in which  a 
    programmer is allowed to see only one section at a time, not, say, 5 of them from many places,
    opened as a sort of "view". Opening only a single subsection at a time is the death of the idea
    of folding or outlining. It simply strips it of its usefulness. The human perception then becomes 
    "mosaic".
    
    .i. As an example of totally wrong presentation, ./i. which  fragments perception into poorly
    connected pieces and does not allow a reader to see the forest for the trees and see 
    documentation of GNU libavl .a. http://adtinfo.org/libavl.html/ ./a. 
    It is not humanly possible to follow the train of thought from such a presentation: one has to
    keep an internal mental map of the document .i. before ./i. reading it and inability to see more than
    one snippet puts a stop to perception: once the snippet is left, one has to rely on memory of
    previously read pieces to relate things to each other.

    Nor the split HTML, nor the flat file format provide for human-friendly perception of code
    unless it is  a small short self-contained piece.
    

    .i. A document that is based on a structure of folding subsections ./i. drastically increases
    one's attention span, allows to manage easily thousands and thousands of lines of text, and
    makes easy an inclusion of many different types of files into what becomes one .i. Literate 
    Source Project file ./i. .

    A number of techniques becomes possible which have not been possible before in a flat file. 
    Snippets testing language constructs, doubling certain "chunks" while keeping a pointer in a
    "linker chunk" to preserve the previous working version of your program while experimenting
    with a new one (previously one would have to use version control or rename files for that), 
    making collections of loosely related documents and script, each still obtainable through
    tangling, such as a collection of system administration policies, readmes, config files and 
    many scripts -- all become easy with a folding literate source file.

    And consequently emphasis moves from the creation of usually comparatively compact programs
    exposed as a sort of polished essays (or documented for their logic in the order of thinking)
    --  to using Literate Source folding files as your "work log" or "a log of thinking" when
    developing a program, like a scientist might keep a log of his experiments.

    Clean version of documentation or program presentation might emerge from it, later, but at the
    moment of conception the most urgent task is to preserve your thinking, however "clever" or
    "stupid" that might be and unload all "houskeeping" from your mind, to stop straining to
    remember where which change must go.

    Such literate project files however do become good documentation, it seems, even without much
    further polishing, as polishing occurs naturally with developing and rewriting your program.


}}}2        

.h2. ./h2.

#------------------------------------------------------------------------------
{{{2 .+h2. PART II ------------ HOW TO USE MOLLY.pl ------------ ./h2.
#-------------------------------------------------------


#--------------------------------------------------------
{{{3 .h3. My workflow - literate programming with Molly ./h3.
#--------------------------------------------------------

 .<. img src=eeerotate.jpg align=right hspace=5 width=200 border=2 .>.
.i. .b. A prerequisite of sorts ./b.
Use a vertically rotated screen (with an external keyboard and mouse
on a notebook) or a large  enough screen to see a vertically positioned
A4 sheet of paper to obtain full advantages of outlining and folding 
Psychologically, the more we see, the more "clever" we are.

.b.Stop reading compicated documents through a slit./b.: with mental
perception crippled,  you'll just jump between fragments, and
never truly "get the whole picture". ./i. .b. >>> ./b.

Rotation on Linux is done by running .pre. xrandr -o right|left|normal ./pre.
from an X terminal window. Most modern video cards support this.

.b. The Workflow ./b. :
.ol.
    .li. I run a web server on localhost:8000, for myself only. My
    development directory is under the web server - or is linked to
    the web server document tree.

    ./li. .li. I write my Literate Source file in a vim editor. I mark up
    the documentation section (i.e. not code) with most basic HTML tags
    (e.g. <h2>, <b>, <ul> ...). 
    The code sections live between "<<section name>> =" and "@" 
    (in the first position in the line), as is required by "noweb"
    literate programming tools. Other code sections can be referred to
    from inside code section with "<<name of section referred to>>".

    ./li. .li. I view my file in a web browser, and reload the page after editing updates.
    Because I write my Literate Source with help from the MOLLY.pl script
    (see below how to  set it up), every "heading" in HTML (i.e.  h1,  h2  etc. )
    automatically becomes a .b. folding subsection ./b. of the document I see in
    the browser, and every code section is  automatically formatted with line
    numbers and a frame.

    ./li. .li. When I think ("globally"), I look at the formatted, immediately updating
    document in the browser, keeping open only the sections relevant to my thinking.
    When I edit, I refer to the line numbers, and avoid navigating "blindly" in the
    programming editor, stressing my memory. I edit "locally".
    This is what makes all the difference.

    ./li. .li. Automatically created folding documents allow me a number of techniques
    and uses impossible previously with flat "literate programming" files.
    Frist, folding HTML and Molly script scale traditional Literate Programming up.
    Then, not only can I avoid stressing memory, but I can also create "virtual
    views" on code that allow me to maintain, debug or develop some aspects of the
    code while totally excluding everything irrelevant, or allow to document overviews
    of functionality, while keeping only scribbles and notes needed for immediate 
    programming alongside actual code, etc. etc. etc.

    ./li. .li. I tangle the created code either directly with a built-in Molly tangler, with
    "notangle" tools from "noweb", or by running "MyProject.tangle", which is a link to 
    "MyProject.weave", the actual main Literate Source file turned into a perl script by
    inclusion of the MOLLY.pl into it.
./ol.

See details about this setup and possible uses of it below.
}}}3

#-------------------------------------------------------------------
{{{3 .h3. SET UP a Literate Source file and project ./h3.
#--------------------------------------------------------



#--------------------------------------------------------
.h4. Prerequisites ./h4.
#--------------------------------------------------------

    Strictly speaking, none except for core Perl (and any web server and web browser
    for operation in CGI mode).

    The script now has a built-in tangler, which can tangle arbitrary roots from the
    Literate Source file.

    However it is possible to  use a "pass-through" tangler, i.e. delegate actual
    tangling to the "notangle" utility from the "noweb" suite - or to tangle from
    your Literate Source file directly with "notangle", as Molly shares markup
    with it.

    "Noweb" set of tools has been around for more than 10 years, and is well tested.
    It is also more flexible, at least for now.
    It may be prudent to compare tangled output of Molly with that of noweb, until you
    feel sure you can trust it.

    Please obtain and install "noweb" . .i. Make sure the tangler is in your path ./i.
     .a. http://www.cs.tufts.edu/~nr/noweb ./a.

    Any version will do, e.g. the one based on shell and awk. I created the MOLLY script 
    to work with the overall markup of the "noweb" tools. It means your project file(s) 
    can be directly processed with "noweb" tools. MOLLY is mostly intended as an alternative
    weaver which creates folding HTML documents on the fly.


#------------------------------
.h4. Two ways to invoke Molly  ./h4.
#------------------------------

        Molly like any traditional Literate Programming tool can be used on its target files from
        command line. So, to tangle to STDOUT, do:
        .pre.   MOLLY,pl my_literate_source_file  # this will tangle out from "*" 
        or MOLLY.pl -R 'root chunk name' my_literate_source_file ./pre.
        and to weave to STDOUT
        .pre.   MOLLY.pl -w my_literate_source_file ./pre.
        
        However,  another "shorthand" way to treat your LitSrc file, more convenient during
        development is to include a short section with configuration parameters and a call to
        Molly at the top of it. Your LitSrc file (which you make executable) becomes then a perl 
        script that works on itself, choosing weaving or tangling according to its file extension.,
        and you can do a Quick Weave and a Quick Tangle, correspondingly, by running from CL:
        .pre.   ./My_Lit_Src.weave > formatted_documentation.html
        and ./My_Lit_Src.tangle > extracted_code ./pre.
        (Quick Tangle tangles only from the "default root" chunk, "*").
        
        The main idea for this mode was to enable a quick use of it under a local web server:
        the print to STDOUT is picked as CGI and can be immediately seen in your browser 
        upon reload of the page ("Ctrl-R").
        So a developer can  read a nicely formatted folding HTML document while he explores
        and thinks, having an "eagle's eye view" of the whole code and concepts,  i.e. "to think
        globally", and use line numbers to get to the right place in his editor, which
        is no longer used for navigating code blindly (and have to keep in his mind the whole 
        structure of his program, polluting and  maiming his thinking) , i.e. to "edit locally".
        
        The next section explains details of the setup.
 
#------------------------------
.h4. Setting up the file ./h4.
#------------------------------


    I aimed at simplifying and eliminating unnecessary steps, and wanted to automate some others.
    Therefore practical work with MOLLY.pl looks like this:

    .ol. .li. .b. .i. Your Literate Source file must be "mollified" ./i. ./b., i.e. you have
    to add an invocation of MOLLY.pl as the first lines of it. Think of it as a template. You
    will write your Literate Source text right after these initial lines.  The minimal template
    is this:

<<minimal MOLLY template>>=
    #!/full/path/to/perl
    do "/full/path/to/script/MOLLY.pl";
    exit;
    # delete first space(s) in the next line
    __DATA__
    #----------------start of the script------------

    <h1> And here I start My Project </h1>
    Your Literate Source file can be typed here

    <h2> Subsection </h2>
    level 0-9 of subsections are allowed now.

    Each subsection will be automatically created as "folding". 
    Javascript must be enabled in your browser to use this functionality
@

    The perl invocaton and the "__DATA__"  line must start .x. at the first position ./x.

    You do not need any extra perl modules  as pre-requsites, the script uses core perl.  

    Full template will also set configuration variables which adjust its behaviour.

    Save this under some name, e.g. MyProject.txt. This turns your Literate Source file into
    a perl script

    ./li. .li. .b. .i. Make it executable. ./i. ./b. 
        .pre. chmod 755 MyProject.txt ./pre.

    ./li. .li.  .b. .i. The script behaves according to its name extension ./i. ./b.
    The script will produce HTML-formatted documentation, if its name ends with "weave" 
    (default - or whatever you selected when setting configuration variables in the 
    "mollification" template). 
    Invoked under a name with file extension of "*.tangle" (default) it will "tangle",
    i.e. create machine code.
    So,  name or rename your LSFile "MyProject.weave" and create a soft link to it named
    "MyProject.tangle" (or whatever alternative you set in configuration for tangling):
    .pre.
        mv MyProject.txt MyProject.weave
        ln -s MyProject.weave MyProject.tangle ./pre.

    Now if you run MyProject.weave, it will dump HTML-formatted documentation of your work to STDOUT
        .pre. ./MyProject.weave > MyProject-formatted.foldingdocument.html ./pre.
    It you run MyProject.tangle, it will dump machine code from your Literate Source file to STDOUT.
        .pre. ./MyProject.tangle > MyProject-runnable.script.pl ./pre.
    ( Of course, tangling is independent of the language you program in )

    For example, if you develop in perl, you can run the resulting script like this:
        .pre. ./MyProject.tangle | perl [perl options here] - ./pre.

    Additionally, as the markup is the same as that of "noweb" tools. one can use them directly
    on the My.Project.weave file, with all of their flexibility and extra options.

    However the idea was to avoid running some of these commands after each change in
    the file manually and to avoid saving the weaved docs: STDOUT can be picked up by
    your local web server as CGI, and you will have an HTML-formatted and which is more
    important, .i. folding ./i. document in your browser by simply reloading the page 
    (usually with Ctrt+R or a mouse click)

    ./li. .li. Now, .b. .i.  the only other step is to run a local web server ./i. ./b. 
    on localhost, any tiny very simple httpd will do, and tell it to treat "*.weave" as
    valid cgi file extension. (Alternatively, I can set "weaving" to files that end in 
    '*.cgi" or "*.pl" etc. in the configuration part of my MOLLY template)

    One of the simplest WWW servers for such use could be "thttpd", and I will provide you with its
    minimal configuration file below.
    ./li. ./ol.



#------------------------------------------------
.h4. "Mollification" - full configuration template ./h4.
#------------------------------------------------
Should be self-explanatory, at least after reading this documentation section ;). 
May add to this section later. Uncomment to set, otherwise defaults apply.


<<full MOLLY template>>=
#!/usr/bin/perl

#-------------------------------------
# -------TOC and INDEX behaviour------

# print TOC? 1:0
#$print_toc=1;  # default is to print

# should we keep TOC expanded? "block":"none"
#$toc_expanded="block"; # default is to unfold

# should we keep Chunks Index expanded? "block":"none"
#$ind_expanded="none";  # default is to keep folded


# should we number lines in code sections? 1 : else
#$line_numbering = 1;   # default is to number


#-------------------------------------
#---------MathML options--------------

# should we enable MathML via ASCIIMathML.js or LaTeXMathML.js library? 1:0
#$enable_ASCIIMathML = 0; #default is to disable as it slows Molly down a lot
#$enable_ASCIIMathML = 1;

# If yes, what is the full path to the lib? Remember to get the one with proper
# escapes for your work, default or modified (see documenation)
# CAN BE: (a) local "/full/path/from/root/to/ASCIIMathML_with_modified_escapes.js" or 
# (b) in current dir "ASCIIMathML_with_modified_escapes.js" or
# (c) on the web, e.g. the original site of the library (unmodified) is:
#$path_to_ASCIIMathML = "http://www1.chapman.edu/~jipsen/mathml/ASCIIMathML.js";
#$path_to_ASCIIMathML = "ASCIIMathML_with_modified_escapes.js"; # deflt is curr. dir


#-------------------------------------
#--------Document Markup lang---------

# how are doc sections marked? "dotHTML":"rawHTML"
#$weave_markup = "rawHTML"; # default is "rawHTML"


#--------------------------------------
#---------File extensions and tangling options------
 
# what is the file extention to weave it? (perms must allow execution!)
# e.g. "scriptname.weave" or "scriptname.cgi" etc.
#$weave_extension = "weave";    # default is "weave"

# what is the file extention to tangle it? (perms must allow execution!)
# e.g. "scriptname.tangle",  "scriptname.pl" etc.
#$tangle_extension = "tangle";  # default is "tangle"

#When tangling, should I use the built-in tangler? 0:1
# (if 0, the "pass-through" tangling will call "notangle"
# from Ramsey's "noweb" tools, must be installed and in your path)
# $use_builtin_tangler = 0; # default for now is to use external "notangle"

# Print LitSource's line no's as a reference in the tangled output?
#$print_ref_linenos_when_tangling = 0; # default = 0, no.

# ..and the comment for this line-numbering tangle-out and the language will be: 
#$code_sections_comment_symbol = "# "; #default is the hash-space

# find and print root chunks in the LitSrc (i.e. instead of tangling when run
# as "./LitSrc.tangle" from command line) ? 0:1
# $show_all_roots = 0; default is not (i.e. to tangle)


#--------------------------------------
#---------invocation of MOLLY.pl-------

# make sure to provide correct and full path to the script here
# (default is "current directory"):
do "MOLLY.pl";
exit;

# delete the space before the __DATA__ marker
 __DATA__
#---------------start of script-------------

<h1> MY PROJECT </h1>

The main idea for my new project is ..

<h2> Subsection 1 </h2>

Here I explain things that come first
................................

@

.b. "__DATA__" must start at the first position in its line ./b.

A better example can be seen in a new project template file that comes with
MOLLY distribution. Copy it to a different name and start editing.


#------------------------------------------------
.h4. Sample minimal configuration of thttpd ./h4.
#------------------------------------------------

"THTTPD" home is .a. http://www.acme.com/software/thttpd/ ./a. 

From the distro README:
.ul. thttpd is a simple, small, portable, fast, and secure HTTP server. 
.b. Simple: ./b. It handles only the minimum necessary to implement HTTP/1.1. Well, maybe a 
little more than the minimum. 
.b. Small: ./b.  See the comparison chart. It also has a very small run-time size, since it 
does not fork and is very careful about memory allocation. 
.b. Portable: ./b.  It compiles cleanly on most any Unix-like OS, specifically including FreeBSD, 
SunOS 4, Solaris 2, BSD/OS, Linux, OSF. 
.b. Fast:  ./b.  In typical use it's about as fast as the best full-featured servers (Apache, 
NCSA, Netscape). Under extreme load it's much faster. 
.b. Secure: ./b.  It goes to great lengths to protect the web server machine against attacks and 
breakins from other sites. 

It also has one extremely useful feature (URL-traffic-based throttling) that no other server 
currently has. Plus, it supports IPv6 out of the box, no patching required.
./ul.

Its compilation is very quick. Its configuration is simpler than that of Apache. 

Supposing I'd like to point my browser at "http://localhost:8000/path/to/MyProject.weave"
Then the minimal configuration will look something like this:


<<minimal thttpd configuration>>=
#
host=localhost
port=8000
dir=/my/home/00trash/tmp/literate.perl/WORKDIR
data_dir=/my/home/00trash/tmp/literate.perl/WORKDIR
#chroot

# extensions understood as valid "cgi" scripts:
#cgipat=/**.cgi|/**.weave|/**.pl
#cgipat=/**.tangle
cgipat=/**.weave

#logfile=/my/home/00trash/tmp/literate.perl/WWW.server/thttpd.log
logfile=/dev/null

#pidfile=./thttpd.pid
pidfile=/my/home/00trash/tmp/literate.perl/WWW.server/thttpd.pid

# uncomment this to be able to link from anywhere to your web server document dir 
# (unsecure, for local development only)
nosymlink
#
@
.b. NOTE ./b. that "nosymlink" schizophrenically .i. allows ./i. symbolic links in spite of what
thttpd documentation tells you

The script to start thttpd (it will detach and run as a daemon) is something like this:

<<run.thttpd.sh>>=
#!/bin/sh
PROJECT_ADMIN_DIR="/my/home/00trash/tmp/literate.perl/WWW.server";
#./thttpd -p 8000 -h localhost -C ./thttpd.config
#$PROJECT_ADMIN_DIR/thttpd -nos -C $PROJECT_ADMIN_DIR/thttpd.config
$PROJECT_ADMIN_DIR/thttpd -C $PROJECT_ADMIN_DIR/thttpd.config
@



#---------------------------------------------------
.h4. Markup of code and documentation sections ./h4.
#----------------------------------------------------

.b. First, ./b.
    .ul. .li.  code sections begin with "< <name of the section> >="
    in the first position in the line and end with "@"  in the first position
    .i. no spaces between the double angle brackets - I used spaces as escapes here ./i.
    
    ./li. .li. References to other code sections are done with "< <name of the ref sect> >"
    inside the code sections

    ./li. .li. The rest is "documentation chunks" which must be marked up with some formatting
    tags. (The default is html).

     I do not insert examples here, as the file you are reading provides them ad nauseam.

    THIS IS THE BASIC MARKUP OF THE LITERATE SOURCE FILE, as used by the 
    "noweb" tools.

    .i. .b. NOTE ./b. ./i. While Molly uses the same markup. .b. "notangle" allows to skip 
    "@" signs./b. in the first position on the line if the next chunk is also code.
    .b. Molly insists on full markup. ./b. Molly tangled examples from 'noweb' distribution 
    correctly when those were canonized to use full notation.


    ./ul.

.b. Secondly,./b. how do we format the documentation sections?

    .ul. .li. In the simplest case, with straight HTML markup (default)

    ./li. .li. If a simple subsection is added to the MOLLY.pl script to process it, in any
    markup lingo of your choice (e.g. "wiki") 
    
    Currently I provide a "dotHTML" formatting subsection as an example. It implements
    a few of the most common HTML tags but with "dots" around them, in place of angle brackets 
    "<" and ">" (which is faster, as one does not need to switch register, so is less prone 
    to typos). See below for more detailed explanations
    ./li. ./ul.
    
    .b. .i. The only difference ./i. ./b. from the truly "raw" HTML is that except in "pre" sections the MOLLY.pl script will 
    add automatic <br> tags at the end of each line of your Literate Source file.


}}}3

#-----------------------------------------------------------------------
{{{3 .h3. CREATE a Literate Source file -- documentation chunks markup./h3.
#-------------------------------

#-----------------------
.h4. Doc sections markup ./h4. 
#-----------------------

.ol. .li. .b. The default is to use ordinary 'html' markup ./b. in the document sections
The only difference from "true raw" HTML is that MOLLY adds automatic 
<br> linebreaks unless lines are inside the <pre>..</pre> sections
The script also cuts out vim folding marks -- { { { number ... } } } number
-- without spaces  and "#------------" lines.


./li. .li. .b. Any "markdown" or "wiki"-style markup can be added ./b. to MOLLY. Another 
reasonable suggestion could be a simple translator from basic TeX markup etc.
Some of this functionality is unnecessary to reimplement, as "noweb" or third-party 
filters can be run on the document after MOLLY weaving or from inside MOLLY.

./li. .li. .b. dotHTML is an example of a simple markup ./b. which can be done with regular
expressions, which is several most basic HTML tags inside "dots" instead 
of angle brackets.

Dots do not require switching the keyboard register, and therefore I can speed up
typing and avoid annoying repeating typos. This is my own "markdown" of sorts.

Currently implemented tags are (write them inside "dots", not "<..>"):
.ul. <br> <p>  <s> - </s> <pre> - .. <b> - </b> <i> - .. <ul> - .. <ol> - ..
<li> - ..

Special marks are dot-<-dot and dot->-dot (no dashes, like ". < ." 
without spaces) which are needed if the tag includes some options or
is not among the tags "dotHTML" understands already.
These will be converted to single < and > respectively.
This is clumsy, but for quick sample and to speed up typing I found
it sufficient. So far.


And there are three special, non-HTML marks I introduced for convenience.
(a) dot-x-dot ... dot-/x-dot .i.on one line only ./i. will .x. format the phrase ./x. as spaced
(b) dot-a-dot http://some/URL dot-/a-dot will create a hyperlink .a. http://some/URL ./a. 
(c) dot-sp-dot - creates a space
./ul.

etc. I add them as I go, whenever I need them for my current document.

This formatting also adds <br> linebreaks unless lines are inside the 
<pre>..</pre> sections.
"dotHTML" also cuts out vim folding marks 
.pre. { { { number ... } } } number ./pre.
without spaces, and  comment lines with dashes
.pre. "#------------" ./pre.
./li. ./ol.


#-------------------------------------------------------------------------
.h4. Choking on bad strings - on Markup and Escaping /h4.
#-------------------------------------------------------------------------

Escaping in files marked in multiple ways is a big source of all kinds of errors.
Currently Molly should be able to take care of most cases.

Exceptions include:

1.  .b. double angle brackets ("< <" and "> >" without spaces in between) and the "at" sign ./b.
Those confuse the "notangle" tool from Ramsey's "noweb". While Molly treats
double angle brackets and "at" as a switch between documentation and a code chunk .i. only
if those are in the first position in the line ./i., "noweb" stumbles.

If you tangle through Molly (by running "my_project.tangle > source.code"), Molly will
filter these out. If you run "notangle" directly on the project file ("notangle -R'some 
root chunk' my_project.weave > source.code"), it will fail.

Therefore in this file I always .i. use extra white space btw angle brackets as an escape ./i.

2. If you enabled the use of ASCIIMathML.js library to display mathematical exressions in
you document, then .b. double backticks ./b. on both sides of any expression pass 
processing to the library. (It also processes everything between "a m a t h" and
"e n d a m a t h" tags - no spaces btw letters - in you file).
Make sure that your target programming language or any future "markdown" you are going to 
add to Molly does not use these signs at all -- or re-edit the javascript library to
substitute all inclusions of double backticks to sth else.

#----------------------------------------
.h4. Markup and Editor highlighting ./h4.
#----------------------------------------

Working with Literate Programming documents will throw off your editor highlighting
unless you configure a special 2-language mode: all text as "html" (or "plain text"
if you wish), and the target language(s) inside literate code sections.
One problem is your Literate Source file may include .i. many ./i. target languages:
Makefiles, shell scripts, one or more scripting or compile languages etc.

The Vim editor has a "noweb.vim" and a "nw.vim" sample highlighting configuration files.
They both derive from the same ancestor. "noweb.vim" is more sophisticated, and allows
a user to have many target languages which are switched according to a regex match on the
title of the chunk. I.e. one tactic would be to name your chunks "subtree XX - Makefile",
or "xxxxx - c source" etc.

The files are obtainable from .a. http://www.vim.org ./a.

It is also possible to have a number of one-language highlighters - "noweb-perl.vim",
"noweb-sh.vim" etc - which are trivially generated, and switch quickly while editing,
without coding the language in the names of the chunks.

It is possible to keep "noweb.vim" for full highlighting, and "nw.vim" just to recolor
all docs, say, greenish, and all literate code chunks blue, and switch whenever it is
convenient. Make sure to rename internal vars in (the simpler nw.vim), because otherwise
they will clobber each other.

For Molly also change the "default" (for the doc sections) from TeX to HTML.

Vim is also good in the sense that its folding and prompts for symbols, nor its completion
of HTML tags depend on teh highlighting mode - in simpler editors those depend on each
other and make life unpleasant.

}}}3

#------------------------------------------------------
{{{3 .h3. Tangling - extracting code from LSFile ./h3.
#------------------------------------------------------


#--------------------------------
.h4. Tangling from LSFile ./h4.
#--------------------------------

    .ol. .li.  .b. Tangling "default root", "*" ./b.
    Usually only one file needs to be tangled constantly, the file the developer
    is working on.  It's easiest to do by using the "default root", which is a code chunk
    called "*". When you run 
    .pre. MyProject.tangle > source.pl ./pre. 
    MOLLY will tangle it itself (or pass the command to "notangle" from Ramsey's "noweb"
    tools, as configured at the top of your project file, and then it will print code
    chunks starting from the one named "*". Therefore, I drag along the default root to
    wherever I am editing now and reassigning to it the chunk I need to tangle currently.
    I usually keep the pointer subsection at the top of the Literate Source file. See 
    "THE CHUNK TO TANGLE BY DEFAULT" in the file you are reading as an example of this
    technique.

    This is the task for which MOLLY "quick tangling" is designed.

    ./li. .li. If, however, you need .b. to tangle out non-"*" root ./b., you have two options:

    (a) run MOLLY.pl from command line:
   .pre. MOLLY.pl -R 'root chunk name' literate_project_file ./pre.
    If chunk name is not supplied, the script will tangle from "*", the default root. 

    (b) ..or you might run N.Ramsey's "notangle" utility directly on your Lit.Source file. 
    .pre. notangle -R 'root chunk name' literate_project_file ./pre.
    This is the most flexible way, as "noweave" allows for extra pre- and post-processing.

    ./li. .li. If you need .b. to tangle out many source files ./b., then probably the easiest way
    to arrange it is through many invocations of Molly.pl or "notangle" in a makefile.
    Include "Makefile" subsection into your Literate Source file and create a target that
    combines several "notangle -R ... " commands in it.
    Assing default root "*" to the Makefile, then run "MyProject.tangle > Makefile" to create
    it and next run "make targetname" to tangle (and compile, run etc) many files included
    into your Literate Source project file.
    ./li. .li.
        .b. Numbering lines in the tangle output ./b.
        Sometimes your scripts or programs fail giving the line number at which error occured.
        Then it is convenient to know where in the Literate Source this piece of code lives.
        There is an option both in "notangle" and in Molly to print LitSource line numbers as
        comments in the tangled output.
        
        From command line: 
        .pre.   MOLLY.pl [-R 'root chunk name'] -l 'comment symbol' Lit_Source_file ./pre.
        When using QuickTangling, set the corresponding option in the config section at the top.
        "Comment symbol" is prepended to the line number printout, set it for the language you 
        used in this particular tangle-out. 
        Default for QuickTangling mode is "# " (hash-space).
        
        ./li. ./ol.



    .i. .b. NOTE ./b. ./i.
    that Molly uses the same markup for document and code sections as "noweb".
    Nowever .b. "notangle" allows to skip "@" signs./b. in the first position on the line if
    the next chunk is also code.
    .b. Molly insists on full markup. ./b. Molly tangled examples from 'noweb' distribution correctly
    when those were canonized to use full notation.


#--------------------------------------
.h4. Scaling up: tangling from many LS files ./h4.
#--------------------------------------

Built-in Tangler in Molly makes 2 passes over LitSrc file(s) on disk, it does not process
them in buffer memory. Therefore it cannot take piped input. However this type of tangler
is easy to adjust for processing many LitSource files .i. (note: this is different from
extracting several .b.target, code./b. files from one or many Literate Source project files) ./i.

When you project scales up so that even folding features of Molly cannot overcome
inconvenience (e.g. when TOC and chunks index become unmanageable), you can .b. .i.
mechanically split ./i. ./b. your Literate Source file into parts. Remember to "mollify"
the pieces (prepend Molly template at the top) if you use Quick mode based on file
extension. If you use Molly as a traditional weaver and tangler from command line or
scripts, just cut LitSrc mechanically.

(It also follows from above that you can then "cat" the pieces together into one big whole,
if you wish. Just remove the "mollification" headers from all but the first one, if you
added them, or otherwise simply mechanically "cat" them together.)

The cut cannot tear your code line inside a code chunk, obviously. You'll most probably
do your cutting at HTML section headings. .b. Code chunk names must remain unique
over the whole project, ./b. so you might include file names into their namestrings for
a largish setup, something like this: 
.pre. < <code to proces XX - "backengine.section.weave"> > ./pre.
But that is left to your own imagination.

However your chunk chains can run anywhere over the parts of your project, and it
will still be tangled correctly. Just supply the names of all partial Literate Source
file names on command line to Molly:
.pre. MOLLY.pl -R'root chunk name' [-l 'comment symbol'] file1 file2 .... ./pre.
You do not need to give explicit name of the root to tangle from if it is "*", the default.
You might want line numbering for debugging, but otherwise not use it.
(It would be a mistake to set a default root "*" in several partial LitSrc files: these
will be appended together, as follows from the lit. programming format for incremental
definition of a chunk done with splinters under the same chunk name).

.b. Partial definitions of one chunk, e.g. "chunk A"can be scattered ./b. in various
LS files now.
According to Knuth and descending tradition, the first mention of "chunk A" will
begin it, any other code under this name will append to the body of the chunk in
the order it is scanned down the file(s), and inserted in teh first location the chunk
was used.

Therefore if you have such split chunks and sequencing of code inside is important,
.b. supply filenames to MOLLY in proper order./b.  In other cases order is unimportant.

This is traditional treatment in Literate Programming and Molly just implements it.

.b. .i. NOTE 1./i. ./b. that multi-file tangling can be used from command line only.
Quick Tangle mode, Quick Weave and all other weaving are done on one file only.

.b. .i. NOTE 2./i. ./b. Do not adjust the HTML sections levels in the split LitSrc
subproject files. Molly won't care if the file to weave and turn into "folding HTML"
headings start not on level 0 or 1 and display all nicely aligned to the left.
This once again means that you can mechanically cut your LitSrc project into pieces
and reassemble them at your  will.

}}}3

#------------------------------------------------------------------------------
{{{3 .h3. Mathematical Formulae - MathML inside Mollified Literate source files ./h3.
#------------------------------------------------------------------------------

.i. The most common reason users put forward for the continued use of (La)TeX in this day 
is its facility in dealing with mathematical notation. However today much .b. simpler HTML ./b. 
(which is often further reduced to "markdown" level) if .b. used with MathML ./b. can cover 
most of such needs./i.

.b. 1 ./b. There are two libraries in JavaScript, ASCIIMathML.js and LaTeXMathML.js (derived 
from the former) which do translation "on the fly" in the reader's browser. Those provide 
almost a drop-in  functionality and can be used with Molly.

The latest version of the ASCIIMathML.js library (2.0.1) includes .b. .i. both ./i. ./b. 
ASCII and LaTex processing. 

.b. 2 ./b. There is a very nice "mimeTeX" application which is a C program that can be run
as CGI (i.e. under the same web browser you use for Molly), which produces GIF renderings
of LaTeX formulas on the fly. It is more complete that ASCIIMathML.js.
I will include an option to use it with Molly if it is dropped into your work directory in
a next version.

#----------------------------------------
{{{4 .h4. How to use ASCIIMathML.js in Molly ./h4.
#----------------------------------------

.b. 1 ./b.. To enable MathML one needs to use a compliant browser. Currently MathML is built in
Firefox, or it can be enabled in Internet Explorer with a (free) plugin.

For Opera the support is also built-in, however the library actually checks for the browser type
and then it seems disallows Opera. The Opera browser, however, has another feature to bypass
exactly that and lie about its identity. If "identify as Firefox" is set in your Opera browser,
the MathML output is caught and rendered.
I did not see any difference between Firefox and Opera_pretending_to_be_Firefox in rendering
output of the library.

.b. 2 ./b.. To enable MathML while dynamically creating the folding-html file, I use an LGPLed
Javascript library called "ASCIIMathML.js" from 
.a. http://www1.chapman.edu/~jipsen/mathml/asciimath.html ./a.
(the home page), while only the library itself can be copied from
.a. http://www1.chapman.edu/~jipsen/mathml/ASCIIMathML.js ./a.

.b. 3 ./b.. The use of MathML must be .b. enabled in the "mollifying" template ./b. at the top of your
Literate Project file, and the path to the ASCIIMathML.js added. See the config. 
template subsection above

.b. 4 ./b.. I had to .b. change escape symbols ./b. in the library not to conflict with 
.i. programming symbols ./i. - now it's double backticks instead of single ones -- and 
change config option enabling "preservation of $ and $$"

Therefore a copy of modified library is distributed with the Molly script.
The default location for Molly to work with is in the same directory as the Molly.pl.
Adjust configuration as needed at the top of your script.

If the rest of your file does not conflict with default ASCIIMathML.js escapes, i.e. does not use
$ signs and single backtics, you can point to the original unmodified library and distribute that
one with your work (see detailed explanations below).

.b. 5 ./b.. This javascript library allows a user .b. to type math formulas in simple calculator-
like ASCII notation with LaTeX constants ./b. if needed.

The interpreted parts of the text must be either:

(a) .b. .i. between "a m a t h" ...... "e n d a m a t h" tags (no brackets). ./i. ./b. The script
then tries to differentiate between ordinary text and math expressions and renders the latter
dynamically as MathML - which is then displayed by your browser.

It might get confused etc. (e.g. because you used underscores somewhere); then your math
could be

(b) .i. .b. escaped (the math expressions only) with DOUBLE BACKTICKS around them. ./b. ./i.

}}}4

#----------------------------------
{{{4 .h4. 6 . Some examples and LaTexMathML.js library versions ./h4. 
#----------------------------------


..are quite intuitive:

Let's try some interesting formulas: 
E=m c^2 ---> ``E=m c^2`` and e^(i pi)=-1 ---> ``e^(i pi)=-1``

and AA x in CC (sin^2x+cos^2x=1) ---> ``AA x in CC (sin^2x+cos^2x=1)``
and one more: sum_(i=1)^n i^3=((n(n+1))/2)^2 ---> ``sum_(i=1)^n i^3=((n(n+1))/2)^2``

..and here are .b. more realistic math expressions ./b. taken from a depository of some math 
    student and written in TeX notation. I drop $$ .. $$ wrapping the in-lined expressions 
    though and delimit it with double backticks instead. Here's the test:

    Hover your mouse over the expression to see the ASCII/TeX coding:

Freudental Formula
``mult(\xi)=\frac{2}{(\mu+\rho|\mu+\rho)-(\xi+\rho|\xi+\rho)}\sum_{\alpha\in\Delta^{+}} mult(\alpha) \sum_{k=1}^{\infty}mult(\xi+k\alpha)(\xi+k\alpha|\alpha)``
It includes roots ``\Delta=\left\{k\delta+\alpha|k\in Z,\; \alpha\in \Delta_0\right\}``
positive roots ``\Delta^{+}=\{k\delta+\alpha|k\geq 0,\; \alpha\in \Delta_0^{+}\}\cup \{k\delta+\alpha|k\geq 1,\; \alpha\in \Delta_0\setminus \Delta_0^{+}\}``

.i. NOTE ./i. If you hover with your mouse over some interpreted expression, a baloon
will appear showing the original ASCII markup for the expression. Very nice.

.i. NOTE 2 ./i. the "pre" (preformat) "/pre" HTML tags prevent the library from
rendering the expressions and might even be used as a quick escape tool when you are writing
your page.



.b. Some TeX operators are not understood by ASCIIMathML.js ./b.
(the failing test is in the ADD-ON MAthML subsection of the Source Code
 section in this document). The reason is most likely that the lib was 
never told about those 1 or 2 constants and operators on which it fails

.b. To solve ./b. the problem one might explore another version of the library, "LaTexMathML.js"
(of which there are 2 or 3 versions) changed to  interpret  the actual LaTex code:
.a. http://www.maths.nottingham.ac.uk/personal/drw/lm.html ./a.

One more version is here:
.a. http://math.etsu.edu/LaTeXMathML/ ./a.

One more version (related) of LaTexMathML.js is here:
.a. http://pillars.che.pitt.edu/LaTeXMathML/ ./a.
.a. http://pillars.che.pitt.edu/LaTeXMathML/latexmathmlguide.xhtml ./a.

.. and there is even a perl port for that
.a. http://pillars.che.pitt.edu/LaTeXMathML/ ./a.
(which also can potentially be used from MOLLY)


I have not tested those in full, but the use of JavaScript libraries from MOLLY should be
identical (and so it says in the documentation inside the libraries) to that of ASCIIMathML.js
They are directly interchangeable in a standalone HTML-based editor. Just point to a copy of the 
needed lib in the configuration section of your Literate Source file.

.b. Secondly ./b. the home page for the library mentions (at the very bottom) that the library
encoding in its JavaScript source file is straightforward and more translations can be added
as needed.
Here is the URL: .a. http://www1.chapman.edu/~jipsen/mathml/asciimathextend.html ./a.

I will provide a section in the Literater Source of Molly (in the ADD-ON section for MathML) 
to add all necessary definitions. It .b. .i. is ./i. ./b. very easy ;))

.b. NOTE ./b. 
.ul. .li. I had to adjust the lib (double backticks and preservation of $) because I was concerned
with programming languages. The source code must stay untouched by the math library 
(mis)translations. That is why I provide a copy of the lib with MOLLY.

./li. .li. If one wishes to use MOLLY for writing mathematical notation without conflicting computer\
language code sections, one can always .b. link to an unmodified copy of the same lib ./b.
and distribute that one with your work ( in *.mht files, see below) or even link to an 
address on the web.

./li. .li.The same applies to a version of the lib to interpret TeX rather than ASCII math notation: 
MOLLY inclusion of the library is generic, just point to the correct javascript library.
./li. ./ul.

}}}4

#--------------------------------------------------
{{{4 .h4. 7 .  ASCIIMathML Markup - reference ./h4.
#--------------------------------------------------

The full set of ASCII notation conventions for the library can be found on its
home page.
.a. http://www1.chapman.edu/~jipsen/mathml/asciimathsyntax.html ./a.

There are 2 pages there, the first describes the syntax, and the second one shows allowed 
LaTex escapes.

The easiest way to see the ascii code and to use any interpreted page as a reference is to
hover your mouse over an interpreted math symbol or expression. In about a second a baloon 
appears  with the ascii source in it.

There is an on-line tutorial in the use of ASCII math notation  for the library at
.a. http://www.wjagray.co.uk/maths/ASCIIMathTutorial.html ./a.
and .a. http://www.wjagray.co.uk/maths/ASCIIMathMLinfo.html ./a.

.b. One more URL ./b.
"an on-line ASCIIMathEditor" is here
.a. http://www1.chapman.edu/~jipsen/mathml/asciimatheditor/ ./a.

It is great to quickly test your expressions. 

Works in my case (firefox on Linux) if I save the page ("full page") and use it from
local files. Could immediately see what I am typing, next copy and paste expressions
into my target file.

Useful. Just delete the spy from google, "the urchin tracker" from the page ;)) .

}}}4

#----------------------------------------------------------------
{{{4 .h4. How to distribute your final folding HTML-formatted work ./h4.
#----------------------------------------------------------------

.b. 8 . To distribute your HTML-weaved folding files with MathML notation ./b.
afterwards you'll have to provide a copy of the JavaScript library with the weaved html 
file, as well as any images you might have used there.
This can be done by creating .b. *.mht archives ./b. from them.

(to explain: MHT is a base64-encoded concatenation of elements of a page
    and its images, scripts etc.;
    MAF is a Mozilla zip-comression of page or several with all images etc into one
    archive file).

Note that because I had to modify escapes in the library for it not to clobber single 
backticks and $ symbols, so common in programming, one has to provide a copy of the 
modified lib with your HTML work.  You cannot simply link to any copy ot ASCIIMathML.js 
found on the Net. (You could if you use an unmodified copy though).

Internet Explorer has built-in support for *.mht (as it is a MSoft standard).
For Firefox the problem is solved easily by installing either
"UnMHT" add-on: .a. http://www.unmht.org/unmht/en_index.html ./a.
or "MAF" add-on: .a. http://maf.mozdev.org/ ./a.

Then just open the weaved html file in your browser, and "Save As" *.MHT, it will 
include all images and the library with it.

So: (a) weave an HTML file from Molly
    (b) open it in IE or Firefox with MHT add-on, and 
    (c) save it as one file.

This will allow you to distribute your MathML marked files with included images, plots
etc. in a convenient manner.
If you are strictly Firefox-based, you can also use the open zip-based MAF format.


}}}4

}}}3

#-------------------------------------------------------------------------------
{{{3 .h3. READ and SEARCH inside a Folding Literate Source file ./h3.
#------------------------------------------------------



#----------------------------------------------------------------------
.h4. Opening many relevant subsections together, TOC highlighting ./h4.
#----------------------------------------------------------------------

One advantage of the folding format is that - in contrast to most other tools - it allows
    the programmer to open .i. many ./i. relevant sections from any parts of the file .i. at the 
    same time ./i.

    This can be done by "walking" the Literate Source file, as its sections and subsections are a 
    kind of TOC of themselves.
    This can be done from the TOC section as well. The open sections get a highlighted background,
    and highlighting appears irrespective of whether the section was opened from the TOC or in the
    body of the text.



#--------------------------------------------------------------
.h4. Keeping some sections of the LSFile permanently open ./h4.
#--------------------------------------------------------------

The behaviour of folding sections in the mollified LSFile is as follows:
    .ul. .li. If you click on the .b. "expand all" ./b. link anywhere in the document, all folding 
    sections in the body (not TOC/Index) will open. The limitation is 10000 objects per document,
    hardcoded.
    ./li. .li. If you click on .b. "collapse all" ./b., all folding sections in the body will fold. 
    The limit is 10k objects per document
    ./li. .li. If you .b. reload the document ./b. in your browser ("Ctrl-R" or an icon click), the
    document will be shown with sections open or closed .b. as marked ./b. in the document itself.
    ./li. ./ul.

    .b. The markup ./b. is simple: if you'd like a section to stay open upon load/reload,  
    add a "+" before "h" in the opening  heading tag:
    .pre. <+h1>, or <+h2>, ... ./pre.

    This is important when you work on a section or many sections of your LIterate Source file for
    a longer period and could not click them open in teh browser time and time again, after each 
    minor code change when you reload the "project.weave" in your browser to see the updates.

    Such changes - adding or deleting a "+" - can often be done with one command in a programming editor.
    For example, to open the whole subthread in vim, issue a range command, sth like
    .pre. 1024,1150s/<h/<+h/ ./pre. (i.e. substitute between lines 1024 and 1150 of your Literate Source file)

    Another suggestion is, when you, for example, work with several program files inside your LIterate
    Source (project) file, you might keep subthreads marked open, while changing only the topmost 
    heading to show/hide the whole of that file/subthread code and notes.

    Even when you handpick the sections to keep open, I found, this arrangement suits me well, at least
    for medium-sized LSFiles up to several thousand lines long.
    I also use folding inside "vim", and so line ranges are visible at one glance: "z-a" to collapse the
    vim fold, and then see the range and issue the range command.

    .i. Note ./i. 
        .ul. Use of folding inside your programming editor .x. is not a substitute ./x. for reading 
        a well-laid out formatted document in the browser. Vim folding is still "blind" and  navigation
        inside such a file still has to rely on your memory of the full layout. 

        MOLLY numbers lines in the code sections by default, making jumps from reading in your browser
        to editing a particular line in your editor painless. I could also put it like this: .b. I view
        the file globally ./b. in my browser as a folding document when I am thinking, .b. while editing 
        "locally" ./b. in a programming editor, without thinking about the overall layout of the file.

        ./ul.

    MOLLY.pl is written to filter out vim folding markups ( {{{number ... }}}number ) and those 
    will not show on your web page.




#------------------------------------
.h4. TOC and Code Chunks Index ./h4.
#------------------------------------

Traditional literate programming tools create extensive indeces of variables and code chunks.

    The approach in this script is as follows:

    Index of all code chunks in your LSFile exists (unless a configuration opton in the 
    template tells MOLLY to skip it) right under TOC. Click on teh line to expand it, and then use
    TOC section numbers to click open the sections which contain or define the chunks you need.

    Remember that .i. all parent sections ./i. above the needed one must be open for you to see 
    the innermost subsection in your browser. This is easy to control through TOC highlighting, and
    as an additional indicator, the slider on your web browser will visibly jerk and shorten when 
    new sections will become visible in the browser window.

    It's also convenient to keep some inner subsections open, but click closed the topmost section to
    quickly hide and unhide larger pieces of code (e.g.complete files which live inside the Literate
    Source File when it is used as a common Project file for many source files, test files, makefile(s)
    etc.



#------------------------------------------------
.h4. Searching for symbols, chunks, phrases ./h4.
#------------------------------------------------

Web browsers "search" functions  will work on open sections only, at least that's the behaviour of
    Opera and Mozilla browsers.
    This means that instead of creating exhaustive indeces of symbols, which clutter program text, 
    especially when they display as HTML links in the output of Weaver tools, one can ensure only the
    interesting subsections are open and then search using the built-in browser function. It usually
    highlights the results and iterates over the found items, which is a nice behaviour.

    To search in the whole of the LSFile, simply click on the "expand all" under any of the subsections,
    and then again use the browser search function.



#--------------------------------------------
.h4. TOC/Index unfolding and collapsing ./h4.
#--------------------------------------------

    The TOC and Index at the top of the document can be kept folded or expanded upon initial load
    depending on the configuration variables in the "mollifying" template you add at the top of 
    your Literate Project file. Please see the subsection titled '"Mollification" - full template' above
    Of course, those subsections can always be opened/closed manually at any time

#--------------------------
.h4. Use for printing ./h4.
#--------------------------

Unfolding only the needed sections also seems to work well with printing from the browser window.
    You can therefore avoid picking certain pages manually from "print preview". You'll print only
    the visible part(s) - i.e. the open sections and the topmost folded section names, which do not
    take place if your document is marked in a sane way, and rather help the reader to orient himself
    while looking at the printout.

    Just remember to check the font size in the "print preview" when preparing to print for the first 
    time.



#--------------------------------------
.h4. Text browsers and JavaScript ./h4.
#--------------------------------------

Web page folding seems to work well with text-only www browsers, such as  "lynx" or 'w3m': they do not 
understand javascript folding, and so display the whole document.

Here's an example invocation with w3m:
.pre.  ./MOLLY.weave | w3m -T text/html  ./pre.
.i. Note ./i.MOLLY does not issue HTTP headers, and this may affect browser behaviour. The web server
 and/or browser may add them or not, or ignore them or not. I might need to correct that.

If JavaScript in your GUI browser is turned off, you'll see only the sections marked in the body
of the Literate Source file as open.


}}}3

#------------------------------------------------------------------------------------------
{{{3 .h3. SUGGESTED USES: Work with Folding LSFiles./h3.
#-----------------------------------------------------------
.i. .b.  section unfinished; an outline of major points for now ./b. ./i.

I do not show examples below, as the document you are reading itself is an example of the
techniques (and that may be the reason it is too bloated and somewhat unorderly ;)) ).

.ol. .li. Literate Programming proper.

    Emphasis on note-taking rather than producing a polished "essay" or "documentation"
        
    Traditional Literate Programming texts place much emphasis on the fact that it is a means of 
    providing .i. documentation ./i. (albeit with source code, and reflecting the thinking rather
    than machine-imposed order).
    Knuth first thought of LP as a documentation tool, then upgraded it to a "programming paradigm",
    but insisted on writing programs "like literary essays".

    However I would like to suggest placing emphasis on NOT documenting and NOT producing 
    polished code or an essay in thinking. The first use of this programming technique is 
    to  .i. keep a full log of your thinking ./i. irrespective of how polished or stupid it is.

    This is most precious. When you first attack a problem, several directions might come to mind,
    but once you began to work out one of then in detail, everything else is lost. This is the
    way human mind works. Keeping even briefest notes is more valuable than attempting to produce
    polished exposition.

    Psychological restrain people feel when they are compelled to prepare something for other people
    is probably the third major hurdle to wide spreading of L.P. (the first two were flat file 
    structure and formatting languages which doubled the effort of programming and debugging).

    My MOLLY Literate Source file is a log of my thinking, trials and experiments. I might later offer
    a cleaned version to other people, but I do not give a damn about them in the beginning.

    Non-folding L.P. source files could not accomodate inclusion of old versions, test files, bad
    versions and skeletons without polutting the file to the degree of becoming unusable.

    Folding version of L.P. source allows one to relegate bad trials into subsections  nicely folded
    out of the way by merely adding a single header line.

    When creating documentation, I could double the content of some sections, by copy-pasting the
    "cleaned" versions, and then save a version with the original snippets bypassed, which is
    easy and mechanical with tools like "gema", "awk" or "perl", just put a code word like
    "scrap" in the beginning and end of those sections you'd like to avoid.

    This ability to bypass marked sections when weaving, by the way, might be incorporated as an
    option into a future version of MOLLY, to produce both 'clean' and 'dirty' versions of the
   formatted Literate Source doc from one file.

./li. .li. Use as a combined Project file
    .ul. .li. to include tests and snippets that implement skeleton functionality to
    be embedded into the main program
    ./li. .li. to include versioning and changes in the way  that does not break what
    has been working so far (there was even a special term for that as a programming 
    principle). Keep the old version, put it into a subsection, and rename the chunk
    to bypass it. The newer version of the piece of code that is being reworked will
    get the old chunk name to be included into the chain to tangle out.
    ./li. .li. To keep a makefile (split, to which I can add lines from many places in
    my Literate Source as I grow it) together with source file(s)
    ./li. .li. To keep several source files related logically in teh same Literate Source file.
    ./li. ./ul.

    .i. Folding meta-files ./i. could be used for larger projects, in which 
    the meta-file, a folding document, would describe teh topmost level layout of the 
    project, and contain ordinary HTML links to partial Folding Literate Sourcefiles, which
    will keep all the junk as described above.

    In any case, the use of Folding Literate Source files would considerably simplify tracking
    of the project files in a version control system.

./li. .li. Use for system administration etc. - as a single file to keep many sysadmin scripts, 
    descriptions of conventions and procedures, bugs and changes in the system .b. .i. in one place ./i. ./b.
    from which any of the included source (or texts, if they are kept in the "source chunks") 
    can be obtained easily with a single command.
    ALSO: combine LIterate Source project file with a makefile as a simple means to keep 
    one-command tangling of multiple targets.

    This Literate Source can live in a central place (e.g. root home) and be under version control.
    The tangled pieces, config files or scripts could be copied to files on one or many machines.

./li. .li. Use for non-programming tasks:
    .ul. .li. as a note-keeper. E.g. I kept job listings and followi-up info in a folding
   MOLLY file and found it quite convenient. 
    ./li. .li. as an outliner for general purpose texts.
    ./li. .li. for keeping documentation, books etc. I keep documentation for a number of
    software tools (e.g. "awk", "newlisp", "monotone") in folding format. It .i. is ./i. helpful.
    ./li. .li. for editorial work on larger texts. The text itself will become "code chunks", 
    and editorial remarks and meta-thinking will remain as "documentation" in a Literate 
    Source file.
    ./li. ./ul.

./li. ./ol.


.h4. scrap ./h4.
(a) //CGI and CL invocations - compared to "noweb"
    // dotHTML tagging; alternative tagging, translations
    // alternative tanglers and pass-through tangler; makefile problem

(b) ideas - how to incorporate tests; how to keep a single project file;
how to set up dummy plugs and alternatives (i.e. to develop without destroying
the old working version)
Maybe: for distribution of a lot of small scripts (admin etc), the  way I used
Makefiles?

(c) It's possible even to do development in smallest chunks -- and later (e.g. for
outside clients or for higher-level documentation, etc.) to consolidate the tiny fractured 
bits and pieces, with plenty of alternatives, dummies, plugs and trials etc. etc.
How? - by tangling those from some certain level and copy-pasting the result into the
Literate Project File.
One could even keep the actual development fractions in a subsection, while retaining
higher-level and cleaner view in higher level sections. I do not care about doubling the
text at all.

.b.suggestion from a Slashdot 2002 discussion ./b.(see ref above) about style or approach:
do not document what the code does (which is redundant, as can be seen from the code itself),
document rather WHY it does it (what's the purpose or the idea).

.b. Also ./b. use this file itself as an example of all illustrated techniques.

Use .b."mollify"./b. as  a technical term (insert MOLLY.pl invocation into the LitProject file)

}}}3

}}}2

.h2. ./h2.

#------------------------------------------------------------------
{{{2 .h2. PART III ------------ LITERATE SOURCE OF THE SCRIPT ------------ ./h2.
#-------------------------------------------------------


#----------------------------------------
{{{3 .h3. Local Reference ./h3.
#--------------------------------

.b. VIM regexp to highlight literate code sections only: ./b.
.i. HAD TO spoil it because "notangle" complained about double diamonds 
"in documentation chunk", bastard ./i. .b. delete spaces btw angl brckrs ./b.
.pre. ^< <.*> >=\_.\{-1,}\n^@/e ./pre.
.. and vice versa, highlight text, leave only code unhighlighted:
.pre. ^@\_.\{-1,}\n^< <.*> >=/e ./pre.


.b. In SciTE, and works in geany, too. Poss. built into Scincilla ./b.
the //{ and //} allow explicit folding marks in "languages like C etc"
Seems to work in Perl. At least, I could correct/improve geany's folding
by adding leading // to my vim closing fold marks, what does not afect vim.

Should add that to the filtering expression in MOLLY



}}}3



#----------------------------------------------
{{{3 .h3. Bugs, status, changes ./h3.
#----------------------------------------------

#-------------------------------
{{{4 .h4. Bugs ./h4.
#-------------------------------

.b.Mistreatment of tabs in _some_ cases. ./b.
.s.       (*) Molly tangler will treat tabs OK .i. unless ./i. they were met
        in the "while over regexp" where Molly just calculates the length
        of the left margin, which is filled with spaces in "pass 2 - print"

I.e. chunks referenced from within Makefiles .i. have to ./i. be in pos 1
on the line and all tabs used explicitly in their bodies.

This can be fixed, but not now. ./s.

I made sure to "et!" (=expand tabs)  and "retab" in vim to make sure
all indentation I type in code is preserved in folding html 
/which may be a good idea generally/

}}}4

#-------------------------------
{{{4 .h4. Ideas - versions - changes./h4.
#-------------------------------

Current version:
.ul.
    .li. creates folding documents on the fly..
    ./li..li. ..based on rawHTML or dotHTML markup 
    (i.e. either creates new docs or displays some existing HTML in folding format);
    ./li..li. and generates dynamically TOC and code chunks index;
    ./li..li. ties collapsing/expandind of sections with TOC highlighting;
    ./li..li. ..provides pass-through tangling with "noweb".
    ./li..li. provides built-in fully-fledged tangler which does not clobber tabs
    ./li..li. provides a tie to ASCIIMathML.js library for inclusion of 
            mathematical expressions in your documents.
    ./li..li. can tangle and weave from command line like traditiona lit prog tools
    ./li..li. allows for the project to be split into several files or reassembled
            at will: the tangler can process many LitSrc arguments on command line
    ./li../ul.

    This script is a test of the concept (LitProg+folding+dynamic web formatting)


#--------------------------------
{{{5 .h5. first ideas ./h5.
#--------------------------------
    

    Options for future versions --  3 ideas for now:
    .ul.
    .li. .s. add a built-in tangler (while retaining the possibility to tangle
    through an external tool, such as "notangle" with all its options and filters). ./s.
    -- DONE

    ./li..li. .x.create a fully self-contained./x. folding web-based .x.weaver./x.
    ("Lilit") (plus possibly a built-in tangler).
    This can be done in newlisp (www.newlisp.org), which has a built-in httpd
    and allows creation of 250k+(size of script) standalone executables on all
    major platforms (unix/linux, windows mac).

    ./li..li. maybe - .x.add "views" (a la Leo views)./x. and maybe editing of single
    subsections (in a pop-up term window with a running editor).
    This - as I see it now - may/will involve storing each subsection in its 
    own little file and dynamically concatenating them both for dynamic weaving
    and for tangling ( keep a third name, some "scriptname.src" for dumping the
    full file to STDOUT?? )
    This can be done dynamically: a huge project file will be displayed fully
    collapsed, and then only the sections marked open will be dumped to the web
    browser dynamically.
    The whole thingy can be kept either as a bunch of section-files in a subdir,
    or (and/or) in a tar file (gzipped or not, indexed or not).

    .b. One possible implementation ./b. is with .a. http://www.wikiwyg.net ./a.
    - the "wikiwyg" library, which can turn any "div" into an editable section.

    The lib as it is distribted does not save (no sub that saves, actually), but
    one can "save in the browser" and next save the page from the browser to a 
    local file

    For a local programming setup ( thttpd on localhost:8000 for example) this
    will do. However one still needs to figure out if there is a possible post-
    processing of the dynamically created (and then saved from the browser) page
    with some script that would strip back the "fieldset" decorations and the 
    invocations of folding plus the html head and TOC/chunks index sections.

    I might say, I need the "reverse weaver" (although not as thorough ?)

    .. and, secondly, after post-processing, if there is an invocation of "patch"
    which would apply the diffs back to the Literate SOurce file.

    .ul. .li. Will a "context patch" do? Can I postprocess
    ./li. .li. Can I postprocess the dynamically generated HTML to mimic some
    sort of diff markup (unified? Context? ...)
    ./li. .li. Third, the exact invocation to apply it to an LSF.
    Is manual restoration of this kind possible? Is it poss to run through a GUI
    merge tool? An "apply all left to the right one" automatic tool?
    ./li. .li. This can be presented as buttons on the page (when the wikiwyg
    option is enabled) etc.
    The script then would take another option (your_filename?apply_changes) to
    run this, and fail with a request to do a manual merge if not possible.
    ./li. ./ul.

    Then the question   remains, how practically useful/convenient that is.
    

    .i.How to do it./i.
    /* Basically, to manage sectionfiles I'd have to code as if those are comments
    to a blog entry (and dynamic gathering of them is the same, too. Should be
    rather obvious and possibly quite easy*/
    This 'views_enabled' mode will switch to sect-files in a ./.molly/ and will
    add dynamic links to each section: "edit section" (in an external editor);
    "update Literate Source Project file". And the script will start to check the
    subdir and dynamically reconstruct its HTML output in this mode.
    Same must be true for the "tangler" pre-filter, too - it will reconstruct
    the whole before passing it to the "notangle" utility.

    ./li..li. Is it possible to do .x."promoting/demoting"  of branches??./x.
    Is it possible in the view-enabled weaver?

    ./li..li. .x.add "web tangle mode"./x., in which MOLLY will dump coloured diff
    of the subsections (or the whole file if not too large), of the current version,
    tangled via the browser form versus a file on disk.
    (I do it all the time firing fldiff after each tangle - would be convenient to
    have the functionality before tangling to disk (with file renaming etc), just
    by reloading a page in a browser.
    Will need to allow saving files (or some branch of chunks) from the browser, then.

    Do I underuse .x. Vim ./x. ? - it does have .b. some "diff" mode ./b. for side-to-side
    display. Just open 2 files, current and prev. vers. and toggle btw one-file
    view and edit or side-by-side coloured diff ???

    .i.How to do it./i.
    .. by first (poss) checking that the files are not identical, and then by
    filtering into "diff":
    .pre. MOLLY.tangle|diff -y -w --left-column xxz - |less ./pre.
    Just wrap it into html-body-pre tags and colour if there's something in the 
    right half (not necessary, really)

./li. .li. .b. context diffs and sections editing ./b.   
./li. ./ul.

}}}5



#-----------------------------------
{{{5 .h5. more ideas ./h5.
#-----------------------------------

.b. More ideas: ./b.

.b. 1. Maybe ./b.  have the .x. "composite document" ./x. (or "fragmented document") 
    option for .x. the second-level ./x. meta-management .x. of project files ./x.
    I.e. one project could be split into many file-sections, and MOLLY weaver will 
    assemble them into the doc.
    OR: I maintain logically consistent pieces in medium-size project files, and
    keep a meta-project file with the split option enabled.
    The "edit" links from it will pop up gvim or another editor of choice with
    the full piece in it.
    While viewing will present it all as if one LIterate Source project file.
    I'll need only to mark the pieces in the TOC, possibly, as a separate doc,
    and -- .b. how will I treat TOC compilation then ? ./b.

.b. 2. Maybe ./b. - if I find an ASCII script (ascii art from commands and 
    descriptions), I could automatically generate .x. ascii-art "maps" ./x. 
    of the chunks ??


.b. 3. introduce horizontal/vertical layouts? ./b. 
    I.e. TOC and Chunks Index either on top of the file or on the left of it for
    "wide screens", like those on notebooks ??
    Changes are in  "Print out the resulting page" (currently line 3359)
     - plus options, of course

    Note: will require gluing TOC or the body to stick at the top, not simple table
    cells, which will center and resize depending on relative cell content sizes.
    May require separate scrolling for the two panes?

OR: .b. TOC/Index in a floating window? ./b. -- should be easy, just  name the
    output windows.

.b. 4. Maybe ./b. add ancors to folding section names to be able to 'hyperlink' 
    and refer to them from other parts of the document??
    // no jumps from the TOC? or with a separate 'jump' icon? //
    // no jumps to closed sections will work anyway, then what to do? //

}}}5




#-------------------------------------------
{{{5 .h5.  Contract programming  and Molly?? ./h5.
#-------------------------------------------

possibly add conditional tangling to produce a contract-assertion-littereted prog during
development and debugging --
versus a "clean" copy -- ???

.b. OR ./b. just keep an "assertion-full" skeleton  by using a simple "rename" or a 
meta-chunk which is a dispatcher??
.pre. [chunk_to_do_sth]=
 code code code code
 [another_chunk_ref]
 more code more code
 @
./pre.
...NO, I cannot do that without conditional tangling
...or - do it in the target language using a "lazy and" for conditionals:
(and (debug_assert_flag) (block .....with some code.....))

.b. Well, this is what I can do: ./b.
.pre.
< < main code chunk > > =
if ($DEBUG) {
< < pre-chunk assertions and if-checks > >
} # fi DEBUG

# main chunk code goes here

if ($DEBUG) {
< < post-chunk assertions and if-checks >>
} # fi DEBUG
#end of chunk here @
./pre.

Then I'll be able to produce debug code by tangling:
.pre. notangle -R"some root" project.weave ./pre.
and the "clean code" without assertion checks I'll get from it
by simply filtering with
.pre. perl -ne 'print unless m!^if ($DEBUG) {! .. m!^\} # fi DEBUG!' tangled.code \
> clean.distribution.code ./pre.

Should be easy enough ;))

}}}5

#-------------------------------------------
{{{5 .h5. Editor highlighting, Perl POD and Literate Markup ./h5.
#-------------------------------------------

The editor highlighting and literate marks  - is .i. bullshit ./i.
The way I danced around it in Molly is half-insane.

.b. SO: the solution might be ./b. to configure POD marks as 
alternative literate markup boundaries, and/or filter the shit
substituting for the real ones in the "pass-through tangler".
Will be Ok at least for quick development, won't it?

The namess of the POD sections are the names of literate chunks, then.

I coould try "=pod Name of the Chunk"
as simple PODs stand for nothing

        (a) need empty lines around all POD markers
        (b) =pod is ignored unless there is a =meaningful_mark
        before
        (c) =pod SOME TEXT -- text is ignored

So, I could use it, it seems? OR: is there any sense in that??

}}}5

}}}4


#----------------
{{{4 .h4.Done ./h4.
#----------------


#-------------------------------------------
{{{5 .h5. done for Tangling ./h5.
#-------------------------------------------

1. command-line options (for weaver, too?) to imitate "noweb" (?)
Options: 
    .ul. -to show all roots??
    - .s. mark refline nums and offsets of the Lit Source in the tangled code ./s.
    - .s. root to tangle ./s.
    - keep docs in comments (and/or Pod sections?? ./ul.

2. ability   to tangle over several file. I.e.
    .ul. .s. I grow my LitSource for a while, then split it almost
    mechanically, not making splinters self-contained
    I then supply all splinter file names to "MOLLY.pl my.project-*.weave"
    and it tangles them out, however my prog is going through the splinters. ./s.
    ./ul.

3. 

}}}5

}}}4


#----------------
{{{4 .h4.Todo -  current./h4.
#----------------

.b.1. Problem of double diamonds in doc chunks ./b.
.ul.
    .s. change the pass-through tangler to isolate the "notangle" from docs sections???
    (will squash the prob of double angle brackets in documentation chunks)

    OR - introduce a sort of escape?? Because sol 1 would clobber correct line numbering??
    But I do not number lines in the default pass-through invocation..
    The standalone will be affected, anyway.

    I could add regex conversion to hexadecimals in the pass-through tangler.
    However it would still break the "notangle" when run standalone. ./s. -- DONE

    I could collect command-line options in case of "scriptname.tangle" invocation
    and pass them to "notangle" - that could be a sort of solution.
./ul.

2. .s. Change logic of "weave-tangle" ("main despatcher"), as it is stupidly convoluted 
and ugly now ./s. -- DONE

3 and 8. (++) .s. rawHTML mode ./s. -- done

4. (++-) see below in "done"

5. .s. .b. BUG of sorts ./b.
the "lt" and "gt" escapes in the code chunks right above ("tangle me with filtering") are not 
displayed correctly  by the web weaver: the browser displays them as real angle brackets. ./s. 
-- DONE

6. .s.HTMLize the error message in the main despatcher (if "I am a module", print it as
an html file). Otherwise it won't look good in the browser. ./s. 
.b. NOT NEEDED ./b. as "tangling" is done from CL only

9 (++). DATA or ---start of script-- to filter?? - done

10. Add "expand subthread" ?? - to open only the current branch of the L.T.File?

11. .s. Add auto-href creation (i.e. posting a URL with minimal markup must create a valid
    HTML link with URL as the name of it. ./s. -- done

12 (+--). and checking 
.s. qwer qwer .x.spacing./x. of one word and .i..x.of  many  on  the  same line./x../i. asdf asd  
SO: to do word boundaries properly. ./s.
DONE although not very cleanly. The tag will break across several lines

13. Add a full set of HTML escapes to "dotHTML" and "rawHTML" ?




}}}4

}}}3



#-------------------------------------------------------------------
{{{3 .h3. PRELIMINARIES: The script layout and idea ./h3.
#-----------------------------------------

.. are very simple.

.b.1. ./b. It is possible to include an invocation of a  perl script with 
.pre. "do script.pl" or "use script.pl"  ./pre.
as the first lines in a Literate Source file.  This turns the source into a standalone perl script. 
If we write the script to process the L.Source file itself ($0 in perl parlance) then the script can do 
weaving and tangling of it. The output, which is formatted documentation (which was "weaved" from L.P.) 
or the sources usable by a computer ("tangled") can be  saved in a file, as with traditional literate 
programming tools. But which is more convenient, the formatted documentation on STDOUT can 
be  picked up by a web  server.  We'll see formatted representation of work in progress immediately
upon reload of the page in a web browser. 

When I think, I prefer to look at the formatted document, and when I edit, I do it "locally" in a good
programming editor (such as "vim"). Even with vim folding the difference in perception is enormous.

.b. 2. ./b.I see three advantages to such an arrangement:  
.ul..li.first, web markup is ubiquitous today, and it is quite sufficient for most publishing needs. 
So Literate Programming with HTML will free us from a tie to TeX which so negatively affected 
perception of L.P. by the masses of programmers. The markup becomes very, very simple. 

This eliminates a source of one major complaint about the traditional L.P. - that the programmer
has to maintain not one, but two "programming languages" while working with LP, the target one
and the formatting one. This - the complaint goes - becomes a source of errors from an additional
layer of programming in addition to writing the code itself. It taxes the programmer's mind rather
than relieving it.

In fact, simple HTML is almost self-correcting, as wrong markup becomes immediately obvious
in the browser. 

./li. .li. secondly, this eliminates "weaving" as a separate step during development, but more
importantly, such an arrangement allows a programmer to think about his program while looking
at a .i.folding./i. document, in which only relevant sections are open.

I perceive .i. folding ./i. as a major advantage

With .i.folding./i. the Literate Source file becomes manageable even for really large texts. 
This in turn allows the programmer to keep .i. many files ./i. inside the LPSource, including
test snippets, the makefile, preserve old versions of parts of the source etc. etc.
Literate Source file is painlessely turned into a Project File, one file, from which anything
can be easily generated.
It is this one file that my version control system must track now. For really large projects,
we can keep a meta-file (also in the folding format) with largish partial files as links,
corresponding to logical pieces of the project, which will open in the browser as automatically
weaved Literate Source Files.

I attempted to explain this at length in introductory sections above.
./li../ul.

.b.3. ./b. How the script works overall

After figuring out how it was invoked, the script:
.ul..li. .b.when it is supposed to weave./b., scans into memory the Literate Source 
file, marked up in "noweb" notation (and its document sections marked with raw HTML 
or in "dotHTML") doing necessary substitutions and conversions along the way.
Then the buffered string thus formed and several auxilliary buffers (for document TOC 
and Index sections) are printed out to STDOUT. They can be redirected into files, if 
LPSource, now also an executable script itself, is run from command line, or they can 
be picked up by a local personal web server that is used by the developer, which  should 
recognize the LPSource as a valid CGI script

After any changes in the script, the user can reload the page in his web browser and 
immediately see the changes.

./li..li. .b.If invoked to tangle./b. the source of the program, this version of the 
script will either use the built-in tangler, or refer the actual tangling to the 
"notangle" utility from "noweb" suite of  tools by Ramsey, depending on configuration 
at the top of your litsource file

The built-in tangler does not scan the file into memory. Rather, it does a first pass over
the file noting the offsets where code chunks start, end or get referred to.
The second pass then recursively prints them to STDOUT.
./li../ul.
.b.5. ./b. .x. A future version ./x. of the script may be made completely self-contained, i.e. 
include a mini-web server as well as a folding weaver and a tangler.

}}}3

#------------------------------------------------------
{{{3 .h3. Makefile./h3.
#------------------------------------------------------


<<Makefile for split MOLLY-top and MOLLY-bottom>>=

all: help
#-

help:
        @echo;
        @echo " ---TARGETS---";
        @echo " new - tangle out MOLLYs-new.pl";
        <<other Makefile help commands>>
        @echo;
#-

new:
        perl ./MOLLY.pl -R'MOLLY.pl module' MOLLYs-bottom.weave >MOLLYs-new.pl;
#-

<<other Makefile commands>>

@

Checking my Makefile insertions ;)). This is going to be a test of M's tangling
as well, actually

The help insertions have already been indented with  a tab

<<other Makefile help commands>>=
@echo " td  - run a tanglediff against MOLLY.pl";
@echo " fld - tangle and run a GUI diff tool, fldiff";
@

<<other Makefile commands>>=

td:
        MOLLYs-bottom.tangle | diff -y -w --left-column MOLLY.pl - |less;
#-

fld:
        perl ./MOLLY.pl -R'MOLLY.pl module' MOLLYs-bottom.weave >MOLLYs-new.pl;
        fldiff MOLLY.pl MOLLYs-new.pl;
#-

@

}}}3

#------------------------------------------------------
{{{3 .h3. Tests to run on a newly tangled script./h3.
#------------------------------------------------------

.. will append to the Makefile, mostly.

FIRST, tests go into ./tests.and.examples subdir

TANGLING
1. Check it tangles itself out
    "make fd" "make fld" already do that
    maybe: check sizes and/or MD5s

2. Check tangling of the "substitution torture test"(s)

3. Check tangling of split script(s)

4. I have 2 modes: command line and Quick Mode
Testing CL involves:

    Usage printout - no ARGV[0], wrong options (several poss)
    weaving from CL
    tangling from CL
    weaving, tangling from Quick Mode; compare output?

Running against "notangle" in several tests above.

ALSO: tangling, split or not: test for DAG loops
/toposort not added yet/


}}}3

.h3. ./h3.

#------------------------------------------------------
{{{3 .h3. MOLLY.pl - top of the script ./h3.
#------------------------------------------------------

<<MOLLY.pl module>>=
#!/usr/bin/perl

#-----------------------------------------------------------------------------------------
# MOLLY - A MO-DULE FOR LI-TERATE PROGRAMMING
#-----------------------------------------------------------------------------------------
# ............................................
# .......licensed under GPL version 3.........
# ............................................
# ....... Author: unixtechie; email: .........
# .. /same/ ..  at .. yahoo .. dot .. com ....
# ............................................
# ..... Git depos. - docs and download: ......
# http://github.com/unixtechie/Literate-Molly/
# ............................................
#
#-----------------------------------------------------------------------------------------
#---|-4-|-8-|-12----|-20------|-30------|-40------|-50------|-60------|-70------|-80------|


<<general settings>>
<<main despatcher>>
<<tangle me>>
<<weave me>>

# just in case:
exit;

# END OF SCRIPT

@


#-------------------------------
{{{4 .h4. general settings ./h4.
#-------------------------------


Some internal script parameters are set here.
E.g. "noweb" tangler gets confused if it finds double angle brackets
anywhere, so sometimes it's possible to fool it by setting a var in the
perl script like this:

<<general settings>>=
  # need to fool the noweb "notangle" utility, switch markup modes etc.
  $lt = "<";
  $gt = ">";
  $lt_esc = "&lt;";
  $gt_esc = "&gt;";
  $dash = "-";
  $dot = "\.";
@

But more importantly:
There are a number of parameters that can be set from the literate source file
at the very top before loading the module with "do MOLLY.pl" (or "use MOLLY.pl").
These are assigned defaults here. You can refer to this portion of the script
to find out what parameters are settable in your document template and expand the
tunings if you modify MOLLY.pl

<<general settings>>=

  # ----- GENERAL settings -----
  
        # print toc? 1:0
        $print_toc = 1 unless defined $print_toc;
        
        # keep TOC expanded on initial load? "block":"none"
        $toc_expanded = $toc_expanded || "block";
        
        # keep TOC expanded in initial load? "block":"none"
        $ind_expanded = $ind_expanded || "none";

@

The script will behave differently depending on its own name. One "major" name
can be used with others existing as soft links (although an "thttpd" lightweight
web server insisted on a hard link in one case).
The modes are detected by file extensions, which are set here.
'Weaving' = creating a formatted HTML documentation file on STDOUT
'Tangling' = creating the program/script source on STDOUT

<<general settings>>=
        # what is the file extention to weave it? (perms must allow execution!)
        # e.g. "scriptname.weave" or "scriptname.cgi" etc.
        $weave_extension = $weave_extension || "weave"; # default is "weave"

        # what is the file extention to tangle it? (perms must allow execution!)
        # e.g. "scriptname.tangle",  "scriptname.pl" etc.
        $tangle_extension = $tangle_extension || "tangle";      # default is "tangle"
        
        
        #When tangling, should I use the built-in tangler? 0:1
        # (if 0, the "pass-through" tangling will call "notangle"
        # from Ramsey's "noweb" tools, must be installed and in your path)
        # use_builtin_tangler = 0; # default for now is to use external "notangle"
        $use_builtin_tangler = $use_builtin_tangler || 0; 
        
        # Actually, let's always do it and disallow unsetting
        # number lines ? 1 : else
        $line_numbering = 1;

        # Print LitSource's line nums as a reference in the tangled output? deflt is 0.
        $print_ref_linenos_when_tangling = $print_ref_linenos_when_tangling || 0; 
        $code_sections_comment_symbol = $code_sections_comment_symbol || "# ";


        # find and print root chunks in the LitSrc (i.e. instead of tangling when it is run
        # as "./LitSrc.tangle" from command line) ? 0:1
        $show_all_roots = $show_all_roots || 0; # default is not (i.e. to tangle)


@

The document sections ("chunks") can be marked with actual HTML tags ("rawHTML") or
with a smaller number of the same tags in "dots" in place of "angle brackets", i.e.
dot-br-dot, not open.angle-br-closing.angle
This is selected here:

<<general settings>>=
        # how are doc sections marked? "dotHTML":"rawHTML"
        $weave_markup = $weave_markup || "rawHTML"; # default is "rawHTML"
        
        if ($weave_markup eq "dotHTML") {
        $tag_open_symbol = $dot;        # this will take care of default
        $tag_close_symbol = $dot;       # when no var is set in the Lit Src file
        }
        elsif ($weave_markup eq "rawHTML") { 
        $tag_open_symbol = $lt;
        $tag_close_symbol = $gt;
        } #fi

@


Do we need to enable MathML functionality? If "yes", set the path to the js library
"ASCIIMathML.js" and enable the switch.
This slows down interpretation of the file on reload drastically (from nothing to
3 seconds with 4-5 example formulas), so for sfw development usually this should be off.

<<general settings>>=

        # enable MathML interpretation? 1 : 0
        $enable_ASCIIMathML = $enable_ASCIIMathML || 0;
        # If enabled, set the path; default is local in current dir
        $path_to_ASCIIMathML = $path_to_ASCIIMathML || "ASCIIMathML_with_modified_escapes.js";

@


It would make sense to add a few more markups to the program, i.e. some "wiki" many use,
and some TeX basic tags for automatic translation, if the doc is tagged as a TeX file.

In more detail dotHTML is explained in its own section

}}}4



#-------------------------------
{{{4 .h4. main despatcher ./h4.
#-------------------------------

The script figures out how it was invoked and then starts either a tangler
or a weaver, or fails with a usage line.

If the script name ends in "weave" (e.g. script.weave), it will be weaved to STDOUT.
If the name ends in "tangle", the source starting from the default chunk < <*> >
(without spaces) will be dumped to STDOUT:

<<main despatcher>>=
<<main despatcher with CL>>
@

There later may be other versions of despatchers to take care of command-line invocations
of Molly etc.

#-------------------------------------------------------------------------
{{{5 .h5. "Main despatcher with CL" ./h5.
#-------------------------------------------------------------------------

Adding command-line processing.
For now the logic is as described below in sub "usage".

More options are typed below, but they are disabled for now.
This version allows using MOLLY.pl from command line to tangle from any root
in addition to the standard "mollified" file weaving and tangling as described
in the docs.

<<main despatcher with CL>>=
  # -- MAIN DESPATCHER WITH CL----
  use Getopt::Std;

<<sub print usage>>

my @LITSOURCE_list = ();
my %LITSOURCE_hash = ();

  # -1- shortcut invocations for "mollified" LitSrc file depending on its extension ---

  if ( $0 =~ m!\w+\.$weave_extension$! ) { 

        open LITSOURCE, "< $0" or die "\n\tcould not open the target file\n\n";
        goto WEAVE_ME;
    }
    elsif ( $0 =~ m!\w+\.$tangle_extension$! )  {

        open LITSOURCE0, "< $0" or die "\n\tcould not open the target file for tangling\n\n";
        push @LITSOURCE_list, 'LITSOURCE0';
        $LITSOURCE_hash{'LITSOURCE0'} = $0;
        goto TANGLE_ME;
  }

  # -2- several cases for application of Molly to an external target file. --- 

  elsif (-t STDIN) { 

    <<running on interactive TTY>>

    }

  # -3- MOLLY.pl as a standalone script is called from CGI, nothing in here yet ---

  elsif (defined $ENV{'REQUEST_METHOD'}) {

<<write 'nothing here' html message>>

  }

  # -4- other cases ---

  else {

        die "MOLLY.pl: I do not know how I was called, exiting anyway\n";

  } # esle, fi - end of despatcher

exit;  # just in case
  
@


#------------------------------
{{{6 .h6. running on interactive TTY ./h6.
#------------------------------


<<running on interactive TTY>>=

    #print STDERR "$0 was called from command line..\n";

    #getopts("hwu:l:d:R:", \%cl_args);
        getopts("hwl:R:i", \%cl_args);

    # -- print USAGE if not evoked correctly
        if( (! defined $ARGV[0] ) or  ( $cl_args{h} ) ) { usage(); exit };

    # -- does target file exist?
        if ( -f $ARGV[0] ) {
            ; # nop, a debug printout
            #print STDERR "target file to operate on is $ARGV[0]\n";
        }
        else { 
            die "\n\tERROR: No target file $ARGV[0] seem to exist\n\n";
        };


    # -- Final CL despatch, do it: --
        

        if($cl_args{w}) { # this is weaving

            if (@ARGV > 1) {
            print STDERR "\n\n\tDo not know how to weave several files\n";
            print STDERR "\tweaving the first one: $cl_args{w}\n\n"; 
            }

            open LITSOURCE, "< $ARGV[0]" || die "could not open the target file\n";
            goto WEAVE_ME;

        }

        else { # this is tangling, default action, no opt

            <<process args for tangling an external file from CL>>

            for (my $countem=0; $countem < @ARGV; $countem++) {

             $LITSOURCE_multi = 'LITSOURCE' . $countem;
             open $LITSOURCE_multi, "< $ARGV[$countem]"
                 or die "\n\tCould not open target file $ARGV[$countem]\n\n";

             push @LITSOURCE_list, $LITSOURCE_multi;
             $LITSOURCE_hash{$LITSOURCE_multi} = $ARGV[$countem];

            } # rof

            goto SEEK_PEEK_TANGLER;

        } # fi - CL final despatch

    exit; #redundant and unused

@


<<process args for tangling an external file from CL>>=

        if($cl_args{d}) { 
            #print STDERR "doc sections in coments; comment char is $cl_args{d}\n" 
            };

        if($cl_args{u}) { 
            #print STDERR "applying UN-tangling with script char is $cl_args{u}\n" 
            };

        if($cl_args{i}) { 
            #print STDERR "printing information on roots, discovered chunks\n" 
            $show_all_roots = 1;
            
            };

        if($cl_args{l}) { 
            #print STDERR "will add reflines; comment char is $cl_args{l}\n" 
            $print_ref_linenos_when_tangling = 1;
            $code_sections_comment_symbol = $cl_args{l};
            };

        # -- getting the root chunk for tangling --
        if($cl_args{R}) { 
            $root_chunk = $cl_args{R};
            print STDERR "tangling root chunk '$root_chunk'\n"
            };

@



}}}6


#------------------------------
{{{6 .h6. write 'nothing here' html message ./h6.
#------------------------------

<<write 'nothing here' html message>>=

    print "Content-Type: text/html; charset=utf-8\n\n";
        print <<_XXX_;
        <html><body>
        <p>
        <b>I was caled as CGI, but this invocation seems to be meaningless.</b><br>
        Maybe you meant to "weave", but set a wrong file extension.<br>
        Goodbye.<br>
        <i>-- MOLLY.pl --</i>
        <p>
        </body></html>
_XXX_

  exit;

@

}}}6


#------------------------------
{{{6 .h6. Sub Print Usage ./h6.
#------------------------------


<<sub print usage>>=

sub usage {

    print STDERR<<'end_of_usage';

    USAGE: MOLLY.pl [options] [--] filename

        -h, or no filename
                get this help message


        TANGLING Options:
                Tangling mode is default, no special option to force needed.

        -R "root_chunk_name", 
                tangle starting from this chunk. if omitted, "*" is default.

        -l 'comment symbol for your language',
                add comments with coresponding line numbers in Lit Src

        -i  information on root chunks


        WEAVING options:

        -w,  weave from the external target file
            This option presupposes html-marked document sections,


    FOR INTERNALLY MOLLIFIED FILES:
        shortcut invocation - depending on file extension. If run as script

        "lit_prog_file.tangle" 
                tangles to STDOUT, from default root "*" only. 
        "lit_prog_file.weave" 
                weaves to STDOUT as folding HTML; usable under CGI
        
        / OR: set your own extensions in your lit.source configuraton section /

end_of_usage

 exit;
}

@


}}}6


}}}5

#-------------------------------------------------------------------------
{{{5 .h5. "Main despatcher simple" ./h5.
#-------------------------------------------------------------------------


This one is for "mollified" file processing only; no use of MOLLY.pl as
a standalone script on external files will be possible.
For that enable the "main despatcher with CL" chunk.

.b. Note ./b. Now that I'm redoing target file opening there, this despatcher
needs to be adjusted too, or it will break (open $0 as LITSOURCE here)

<<main despatcher simple>>=
  # -- MAIN DESPATCHER ----

  # check if $i_am_module and set filenames to $0 - or 
  # process CL options otherwise

  # (temp) - 2 options, to weave and to tangle in module mode:
  #
  if ( $0 =~ m!\w+\.$weave_extension$! ) { goto WEAVE_ME }
  elsif ( $0 =~ m!\w+\.$tangle_extension$! )  {goto TANGLE_ME}
  else {
  print <<end_of_print;

        USAGE:
        Not set to tangle (wrong file extension).
        Set config variables at the top of the script.  

end_of_print

exit;
  }

@

}}}5

}}}4

}}}3


.h3. ./h3.

#---------------------------------
{{{3 .h3. TANGLER ./h3.
#---------------------------------

#---------------------------------
{{{4 .h4. Notes on; difference btw built-in and "notangle" ./h4.
#---------------------------------



    This version of the script includes a built-in tangler. However if you set
    "$use_builtin_tangler" to 0 in the settings, the script will pass ithe request 
    through to the "notangle" utility of the "noweb" Literate Programming tool 
    to tangle starting with .i.the default chunk < <*> >./i.   and without other 
    options passed. I only set "notangle" below to preserve the tabs in case you 
    are tangling Makefiles.

    If you set $use_builtin_tangler to 1, the "seek-peek" tangler will be used.
    It does not clobber tabs at all, so tangling out Makefiles is possible.
    It does not choke on double < < in text chunks, either.

    It is:
   .ul. .li. not so much tested (but seem to handle example from "noweb" distribution
    after those have been brought to "canonical form", i.e. .b. all code chunks
    end with "@", no exceptions ./b. 
    ./li. .li. can handle multiple refs on the same line surrounded by other text,
        ./li. .li. can tangle starting from arbitrary chunks (option "-R 'chunkname'" on CL)
    ./li. ./ul.
    You might want to use the Ramsey's tool sometimes  to check the Molly's output.
    Norman Ramsey's "noweb" has been around for more than 10 years and been used
    on thousands of lines of other projects.

        .ul. .b.  1. Note./b. that for .b. "notangle" ./b. you have   to make sure your 
        double angle brackets are broken by spaces "< <"  or with actual escapes 
        "<\<" even in doc chunks (and escape them in your code, too. "Notangle" 
        reacts to them NOT only at the beginning of the line, as my tangler does, 
        but everywhere and chokes if finds them.

        .b. 2.  Note./b. that the built-in tangler ./b. uses "noweb" markup, but it is 
        .i. more restrictive ./i. in one sense: .b. all code chunks MUST end with "@"./b. 
            .pre.< <name of chunk> >=
            ......
            @ ./pre.
        Norman Ramsey's convention was: .i. you are allowed not to close the code chunk
        with "@" if the next code chunk is coming right after it ./i.
        ./ul.


    So, when the script is invoked as "script.tangle" (which can be a link or a soft
    link to something like "script.weave", it's "real" name), it will dump the source
    to STDOUT, very convenient for development.

    I usually move the root chunk < <*> > along inside the LIterate Source project
    file and assign it to the test script or the part I am working on. The Literate Source
    project file includes several actual source files usually. By the way, it's quite
    nice to be able to keep all partial tests that you do before including those into
    the target program all in one place, your common project file.

    To tangle with more specific options, run "notangle" directly on your "mollified"
    literate source file, e.g.
    .pre.notangle -R my_root_chunk -L script.weave ./pre.

    Later other modes of operation may be incorporated here.

}}}4


#---------------------------------------
{{{4 .h4. Tangler despatcher: run pass-through or built-in tangler ./h4.
#---------------------------------------

    Passes the request to "notangle" from the "noweb" literate tools suite -
    OR starts the built-in tangler chunks.

    1. Pre-filtering needs to be done. E.g. "notangle" gets confused by double angle 
    brackets even when those are inside the document chunks.
    Therefore I will simply cut out document chunks altogether and not pass them to the
    tangler.

    Test line (delete spaces to test): < < get it, choke on this, bastard > >
    < < or this> >=
        or even this
    @

    .b. ADD-ON: multi-file tangling ./b.
    LITSOURCE  opened in the main despatcher for Quick Mode
    and for weaving, both modes. That is always .i. one file only ./i.

    Many files must be opened here now for multi-file tangling, pass-through
    disabled ("notangle" cannot do it).
    
    ALSO: an error in logic to check for $use_builtin_tangler? Will be unset and false
    in Command-line mode anyway.


<<tangle me>>=

TANGLE_ME:


  if ( $use_builtin_tangler ) {

    <<seek-peek tangler>>

  } 
  else { # pass to "notangle" from "noweb" tools by Ramsey

    open TANGLE_PIPE, "| notangle -t4 -";

    while  (<LITSOURCE0>) {

            if ( m!^<\<(.*)>\>=! ... m!^@\s*$! ) { # -- CODE CHUNKS ONLY -- 
                print TANGLE_PIPE $_;
            }

    } # elihw

    close TANGLE_PIPE;
    close LITSOURCE0;

  } #; esle, pass-through clause end

 exit;

@

    .s. .b. Speculative: Immediate execution of the script ? Do I need it?  ./b.
    It did not work exactly, but then I did not try it for real -- if it is needed and if your
    language supports taking input from STDIN, it may be possible also to immediately run your
    script skipping the stage of "tangling-into-a-file" before running.
    To be added (possibly). ./s.

}}}4


#------------------------------------------------------
{{{4 .h4. seek-peek tangler ./h4.
#------------------------------------------------------

#------------------------------------------------------
{{{5 .h5. Advantages, limitations; escapes; the algorithm ./h5.
#------------------------------------------------------

.b. Advantages, limitations and escapes ./b.

(a) .s. The first version can only take care of one litprog chunk reference per line, and
this ref cannot be surrounded by other text. ./s.
Built-in tangler can handle multiple references per line surrounded by other text

(b) However it does not clobber tabs, so you can tangle out Makefiles with it.

(c) Built-in tangler insists on ending every code block with an explicit "@"
("notangle" allows to drop them if the next chunk is code, too)

(d) "notangle" by Ramsey is confused by < <  even inside documentation chunk, in any position.
My default "pass-through" tangler filters those out and treats as "real" only the dbl-angle
bracketed stuff .i. in the first position in its line ./i.

This 'seek-peek' tangler also filters out all of documentation chunks and looks only for 
.b. dbl angles and the @ sign in the first position of the line ./b.

(e) The used regexps ensure that if you have both opening and closing double brackets
somewhere .b. in your code ./b., the S-P tangler will get screwed.
This and the first-positioned symbols are the only escapes one needs to observe as
far as I understand.

(f) The S-P tangler only reads files on disk, two times actually. So for now at least
you cannot pipe into it. (It works roughly 3 times faster though and can potentially
work across several litsource files (not implemented yet))

.b.The idea/algorithm is to:./b.

(a) run a first pass over the file collecting a list of 
        chunk offsets, or "addresses"

(b) run a second pass recursively, already knowing where to read the
        bits and pieces from . That's all there is to "tangling".
        
(c) Filters could apply to some chunks for:
        /weaving/
        * feeding the text to a markdown lang
        * cutting sth out for a math processor 
                (e.g. mimeTeX, or to ASCIIMathML.js)
        /tangling/
        * for conditional tangling of certain chunks
        (e.g. those whose names start with DEBUG, negative
        or the opposint, CLEAN, for a positive filtering)
/none of the above are used now/        

This type of tangler will not be able to get input via pipes,
but - it should be fine working across .b.several literate source
files./b., the only requirement is that their chunk namespace should be
flat (unique names across your whole project). 
It can be managed by prepending the chunk names by their file IDs, for
example.
/* command line option to get several files and their processing by the 
script are not implemented yet */


#-----------------
{{{6 <h6>todo</h6>
#-----------------
.s. Now, the next step is to fix the trailing line with @

.i. HOW: ./i.
I'll "tell" on each line and use as my end-of-splinter address
the one from the previous line (i.e. just before the last line, 
which contains the trailing rubbish).  ./s. -- DONE

.s. NEXT is many chunk refs on teh same line surrounded by other text/code ./s.
--done--

.s. NEXT is command-line processing for tangling any given root ./s. --done

NEXT is adding comments with line nums and offsets in teh original Lit Source
into the tangled out code

NEXT is "un-tangling", i.e. reimporting code, changed during testing back into
LitSource (? possibly)

NEXT possibly - processing over several files, which are mechanical splinters
of the same Literate Project

}}}6

}}}5

#-----------------
{az{{5 <h5>The code</h5>
#-----------------

<<seek-peek tangler>>=

SEEK_PEEK_TANGLER:

        <<pass 1>>
        <<check for cycles, search for roots>>
        <<pass 2 and print>>
        <<main: call the sub>>

exit;

@

"pass 1" "goes through the file collecting addresses.

Then "pass 2" sets a recursive sub to "untangle" the chunks with
their included references.

The "main" sets the root chunk and calls the sub

I'll ' add command-line and CGI arguments processing later.

<<main: call the sub>>=

        $root_chunk = $root_chunk || "*";

# USAGE: print_chunk(name_of_chunk, left_margin, print_newline_flag)
        print_chunk($root_chunk, "", 1); 

        # and close all the opened files
        for (@LITSOURCE_list){close $_;}

@


#------------------
{{{6 <h6> PAss 1 </h6>
#-------------------

declare and initialize some vars

<<pass 1>>=

        <<declarations and initializations>>
        <<while-loop and peek file ofsets>>

@

<<declarations and initializations>>=

        my $chunk_beg_pattern = q(^<\<(.*)>\>=);
        my $chunk_end_pattern = q(^@\s.*$);
        my $chunk_ref_pattern = q(<\<(.*?)>\>[^=]); # can be used several times in a line
        
        my $current_chunk_name = "";
        my $current_chunk_start_foff = 0;; # "foff" is a "file offset"
        my $current_chunk_end_foff = 0;

        my %file_offsets_hash = ();
        my %file_lines_hash = ();
        my $parents_list = ();

        my $line_num = 0;
        my $previous_line_foff = 0; # "foff" is a "file offset"

@


.b.Main loop./b.
Need to collect chunk addresses (with "tell"), and keep .i. a hash of lists./i.
with addresses of all chunk continuations (if any)
    .pre. (beg_offset, end_offset) ./pre.

I'll need to push them in, and in pass 2 I'll iterate over them, 
and so ensure all of a split chunk is read.

.b.Anyway,./b. to deal with references to chunks from inside chunks I 
will insert ("ref", "referred_chunk_name", left_indentation) into my 
    .pre.%file_offsets_hash ./pre.
during the first pass. This pair is different from numeric beg-end offset pairs,
and I'll start a recursive printout of the chunk referred to when I detect it.

I'll need to keep "left margin", the indent for the reference as a next value
in the list/array, so it is
    .ul..li.pairs (beg_chunk_offset, end_chunk_offset) for regular chunks
    ./li..li. and a triple ("ref", name_of_referred_chnk, its_left_indent) for refs
    ./li../ul.

I might need to keep more information (e.g. "line numbers in the Literate Source),
then those become quadruples and quintil.. whatever ;)


#------------------
{{{7 <h7> Ref: perl operators </h7>
#-------------------

.b.---Perl operators----./b.
        
.b.pos./b.

.ul.
3253   while ( $string =~ m!\G(.*?)<\<(.*?)>\>!gs) {
3254     
3255     $off = length($2);
3256     $position = pos($string);
3257     #pos($string) = $position + $off;
3258     print "\n\t---title---\n$2 of length $off at $position \n\t----after----\n$1\n========\n";
3259   
3260   }
./ul.

.b.tell filehandle./b. (if omitted, the last file read)
                
.b.seek FILEHANDLE,POSITION,WHENCE ./b.
Sets FILEHANDLE's position, just like the fseek call of stdio . 
FILEHANDLE may be an expression whose value gives the name of the filehandle. 
The values for WHENCE are 0 to set the new position in bytes to POSITION,
        
.b.read FROM_FILEHANDLE, IN_SCALAR_BUFER_VAR_OR_STREM, LENGTH_IN_BYTES_OR_CHARS./b.
There .b.may be problems./b. if FH is opened in non-bytes mode, or if some pragma is used

-------------------

}}}7

}}}6

#------------------
{{{6 <h6> Pass 1: main while-loop and foffs</h6>
#-------------------


.i. A loop over the doc starts here ./i.
Process inside doc sections:

.b. ADD-ON: multi-file tangling ./b.
LITSOURCE  opened in the main despatcher for Quick Mode
and for weaving, both modes. That is always .i. one file only ./i.

Many files must be opened here now for multi-file tangling, pass-through
disabled ("notangle" cannot do it).
 
mm.. (a) all fh's must be opened at the same time for pass2; not for pass1.
Then: can I still call it "LITSOURCE" here, but rename down the line?
Is it kludgier than renaming the whole chain from despatcher?

.b. the problem is this ./b.
I .b. ./i. absolute have to ./i. ./b. process files in the correct order,
which is presupposed to be the one given on CL.
Why? - bcs splinters of a chunk spread across several files must start with
the correct first splinter, etc., get assembled in correct order.

Now, as I assigned names like "LITSOURCE0", 1,2,... to my opened filehandles,
if I sort hte hash keys, I'll get them right (I iterated over ARGV to open
them and therefore the numbering is good) -- IF I sort NUMERICALLY.

Question: will sort do 0,1,2,3..10.11.12... -- or 0,1,10,11,12...,2... ??
Check, or I'll get a hidden error here.


<<while-loop and peek file ofsets>>=

foreach $LITSOURCE_multi(@LITSOURCE_list) {


 while (<$LITSOURCE_multi>) {
    $line_num++;

        # --- CODE CHUNKS -- not inside documentation section
    if ( m!$chunk_beg_pattern! .. m!$chunk_end_pattern! ) {

        <<chunk beginning>>
        <<chunk ending>>
        <<chunk reference>>
        <<chunk body>>


    } #fi

        $previous_line_foff = tell $LITSOURCE_multi;

  } # eliwh

} # hcaerof iteration over all files collecting file offsets

@

.b.A later adjustement, actually to rid of the last line trash./b.
    "tell" gives addr .i. at the end of the matched line./i. So,
    I'll have to use .i. offset from the previous match./i. to
    rid of the current line rubbish - this is what the last
    var assignement in the loop is about.


.b.First, the chunk body./b. 
It closes the "if" opened in "chunk beginning", The rest of then are
"elsif" clauses. 

There's nothing to do here in pass 1; just here not to leave it
implicit and hidden

<<chunk body>>=

    else { # chunk body

                ; # nop; here just not to hide an implicit case
                #~ print "."; # debug: show dots for lines 

    }

@


.b.Next, match for a first line of a new chunk./b. (which can continue
some previous splinters of the same chunk in the LitSource file)

If a chunk begins (or continues) here, 
        - set the current chunk name
        -pick up its "peeked" offset in  the file 
        - and push it in the hash of f_offs under the current chunk name

Hmm.. The perl "tell" gives the position after the read line, I understand.
No adjustment is needed for the beginning chunk match, and it'll eliminate
the title from the later output.

.b. Multifile ./b. - not touching line numbering yet. First, do tangling.
Line numbers are analogous.

<<chunk beginning>>=

    if ( $_ =~ m!$chunk_beg_pattern! ) {
        $current_chunk_name = $1;
        $current_chunk_start_foff = $LITSOURCE_multi . "-" . (tell $LITSOURCE_multi);

        # -- collecting offset and line number, actually
        push @{$file_offsets_hash{$current_chunk_name}}, $current_chunk_start_foff;
        push @{$file_lines_hash{$current_chunk_name}}, $LITSOURCE_multi . "-" . $.;
        #~ print "----> chunk $1 line $. offset $current_chunk_start_foff\n";

    }

@


.b.If a chunk ends,./b. peek its end offset in the file ("f_off") Actually, not to pick
up the closing "at" signs, get the previous line's told offset.

.. and push it in the hash of lists
Plus - set $current_chunk_name=""; # just in case

<<chunk ending>>=

    elsif ( $_ =~ m!$chunk_end_pattern! )  {

        $current_chunk_end_foff = $LITSOURCE_multi . "-" . $previous_line_foff;
        # -- collecting offset and line number:
        push @{$file_offsets_hash{$current_chunk_name}}, $current_chunk_end_foff;
        push @{$file_lines_hash{$current_chunk_name}}, $LITSOURCE_multi . "-" . $.;
        #~ print "\tline $. offset $current_chunk_end_foff<------\n";

        $current_chunk_name = "";
    }

@

.b.Ok, so we put the foffs and line numbers  in the 
(beg offset,  end offset) order./b..


.b.Now, the chunk refs ./b.

If we get a reference, regex-split the line 
        - set the end offset of the current chunk
        - push it in the hash of lists
        - figure the offset after the chunk ref
        - push it into the hash of lists using the current chunkname
        - IMPORTANT COMPLICATION
                deal with the case when several chunk refs on the same
                line are present;
                do .i.not./i. do it for the first version of the script.

for many matches on a line - 
Here we'll have to match with continuation and repeat matching in a loop until the line 
with matches is exhausted.

<<chunk reference>>=
    <<chunk reference - multiref inside other text>>
@

}}}6

#------------------
{{{6 .h6. multiple refs on the same line./h6.
#-------------------


.b. Multiple references on the same line surrounded by other text./b.

First, I'll need to change the regexp pattern:
mm.. I probably needn't. I'll get to it on a normal regexp, and use the
multiref regexp only within
The "chunk_ref_pattern" is set to q(<\<(.*?)>\>[^=])

So, if I got (one, first) reference using a normal regex,
start iterating on this line using a multi-match regex.
'The rest of the line' can be got right after the while{} loop.
I'll need to subtract
.s."told offset" - "line length" + "pos-given offset"../s.
I have $previous_line_foff

.i. NOTE: "pos" does not work intuitively, so I do not use it below ./i.

I'll write out logic later, after I have tested and adjusted it enough

<<chunk reference - multiref inside other text>>=

 elsif ( $_ =~ m!$chunk_ref_pattern!g ) {

    my $line = $_;
    my $current_foff_pos =  $previous_line_foff;
    my $initial_margin = "";
    my $homegrown_pos = 0;

    while ($line =~ m!(.*?)<\<(.*?)>\>!g) {

        my $pre_ref_match = $1;
        my $ref_match = $2;
        my $len_pre_ref_match = length $pre_ref_match;
        my $len_ref_match = length $ref_match;

        # "end" of prev pair; collecting offset and line number
        push @{$file_offsets_hash{$current_chunk_name}}, 
            $LITSOURCE_multi . "-" . ($current_foff_pos + $len_pre_ref_match); 
        push @{$file_lines_hash{$current_chunk_name}}, $LITSOURCE_multi . "-" . $.;


        #-------deal with pushing ("ref", "chunkname") pair -----
        # special id string for refs
        push @{$file_offsets_hash{$current_chunk_name}}, "ref";
        # name of reference
        push @{$file_offsets_hash{$current_chunk_name}}, $ref_match; 
        # .. and form pairs for toposort (cycles check, search roots):
        push @parents_list, ($current_chunk_name, $ref_match);

        # -- next a special entry for refs: (left_margin)
        # I keep tabs and spaces and subst all else to spaces
        $pre_ref_match =~ s!\S! !g;
        $initial_margin .= $pre_ref_match;
        push @{$file_offsets_hash{$current_chunk_name}}, $initial_margin; 
        $initial_margin .= " " x ( $len_ref_match + 2*( length "<>") );      

        my $homegrown_pos = $len_pre_ref_match + $len_ref_match + 2*(length "<>");
        my $end_of_match_pos = $current_foff_pos + $homegrown_pos;

        # "start" a new pair.. - ok, let's not use "pos" at all, if it fails
        # .. and collect both offset and the line number
        push @{$file_offsets_hash{$current_chunk_name}}, 
            $LITSOURCE_multi . "-" . $end_of_match_pos;
        push @{$file_lines_hash{$current_chunk_name}}, $LITSOURCE_multi . "-" . $.;

        #  I'll need to reset current_foff_pos to the pos
        #   (or to the directly caclucalted offset, if I prefer that):
        $current_foff_pos = $end_of_match_pos; 

    } # elihw

    # This is where chunk refs get an extra newline ?

 } # fisle

@

 .b.This works../b.

  Handles Ramsey's test progs .b.when those are cleaned of an annoying
 irregularity in his notation./b., i.e. when .b.all./b. prog chunks start
 with title in dbl brackets and end with an "at" (exactly in the first
 position on the line)

.b. aligning printout./b.
Need to save left margin (here) and apply it in the sub print_chunk below


#-----------------------------
{{{7 <h7> first test: how to iterate over regex, using pos</h7>
#-----------------------------


This is a snippet showing how it can be treated:
.i. broken with whitespaces as escapes for the half-brain "notangle" not to
stumble. Delete spaces btw <_<  ./i.
.pre.

#~ Nope, they use it inside "while" 
#~ OK, this is how it will work in the script:
#~ /the while loop is over the line that contained at least 
#~ one ref; the loop body includes pushes into the hash/

        $line = "asdfas asd < <qewrqw>> opiopiopi < <2345>> asdasdf";
        $llen = length $line;
        print "the original line:: [$line] -- of length $llen\n";


        while ($line =~ m!(.*?)<\<(.*?)>>!g) {
                print "$1 -- $2\n";
                $ancor_off  = pos $line;

                print "ancor position is $ancor_off\n";
        }

        print "THE LAST PART IS  ", (substr $line, $ancor_off), "\n";

./pre.

And this is its output:

.pre.
the original line:: [asdfas asd < <qewrqw>> opiopiopi < <2345>> asdasdf] -- of length 48
asdfas asd  -- qewrqw
ancor position is 21
 opiopiopi  -- 2345
ancor position is 40
THE LAST PART IS   asdasdf
./pre.

}}}7

}}}6

#-------------------------------------------------------------
{{{6 .h6. Topological sort: check for cycles and find roots./h6.
#-------------------------------------------------------------

So, let's cut the root search (if an options is set) and cycles check (always,
unless an option is given to bypass, maybe), into the program flow.

<<check for cycles, search for roots>>=

<<toposort>>

# will abort if cycles detected. Does not detect all cycles? - need to check
my @chunks_in_chains = topological_sort($show_all_roots, @parents_list);

if ($show_all_roots){

    # non-standalone roots are already printed from inside "topological_sort"
    print "\n\t------single-chunk roots-------\n";
    my %lookup_hash = ();
    for (@chunks_in_chains){ $lookup_hash{$_} = 1;}

    for (keys %file_offsets_hash) { 
        print "\t$lt$lt$_$gt$gt\n"
            unless exists $lookup_hash{$_};
        }
    print "\t-------------------------------\n\n";
exit;
}

@

1. This sub is taken from .............. and is a perl reimplementation of an
algorithm taken from "Programming Pearls" by Bentley, who borrowed it from
Knuth.

2. I have modified it trivially to abort at cycles and print roots. 
Should I offer an option to force Molly to ignore this with a "-f" flag from
command line (?? and config var ??).

3. .. and added an option, both CL and from mollifying template to print roots.
    Now, the prob with printing roots is that standalone chunks, which can be
    tangled out, are not in my collected pairs list, and so not printed among
    possible roots.
    -> collect them as children of some 'dummy' and print out in a second
    invocation of topological_sort?? 


<<toposort>>=

sub topological_sort {

    my $flag_show_roots = shift;
    my %pairs;  # all pairs ($l, $r)
    my %npred;  # number of predecessors
    my %succ;   # list of successors
    my $opt_b = 0;

    my @topo_list_out = '';

    while ( @_ ) {
        my $l = shift @_;
        my $r = shift @_;
        my @l = ($l, $r);
        #my ($l, $r) = my @l = split;
        next unless @l == 2;
        next if defined $pairs{$l}{$r};
        $pairs{$l}{$r}++;
        $npred {$l} += 0;
        ++$npred{$r};
        push @{$succ{$l}}, $r;
    }

    # create a list of nodes without predecessors
    my @list = grep {!$npred{$_}} keys %npred;

    #--print discovered roots, if asked--
    if ($flag_show_roots) {
    print "\n\t--roots of multi-chunk chains--\n";
    for (@list) { print "\t$lt$lt$_$gt$gt\n";}
    print "\t-------------------------------\n";
    }

    while (@list) {
        $_ = pop @list;
        unshift @topo_list_out, $_;
        #print "$_\n";
        foreach my $child (@{$succ{$_}}) {
            if ($opt_b) {       # breadth-first
                unshift @list, $child unless --$npred{$child};
            } else {    # depth-first (default)
                push @list, $child unless --$npred{$child};
            }

        }
    }

    # mm.. better if I warn, print out the list so far, and then exit
    #   the user will have the place where the problem occured 
    #~ warn "cycle detected\n" if grep {$npred{$_}} keys %npred;
    if ( grep {$npred{$_}} keys %npred ){
        warn  "\n\tERROR: cycle detected - aborting execution!\n"; 
        print "\n---chunks discovered before the 'cycle' ERROR - sorted topologically ---\n";
        for (@topo_list_out) {
            print "$_\n";
        } # rof

    exit;
    } # fi check for cycles

# -DEBUG-
#    if ($flag_show_roots){
#    print "\t--all chunks discovered--\n";
#    for (@topo_list_out) {
#        print $_, "\n";
#    }
#    exit;
#    }

return @topo_list_out;
#return 1;
} # tros_lacigolopot

@


}}}6

#---------------------
{{{6 <h6> pass 2 and print </h6>
#---------------------

OK, now we have a hash of pairs (beg_chunk_offset, end_chunk_offset) with triples 
("ref", "ref_chunkname", left_indent) interspersed where there were references included.
    - start from the given root
    - iterate over subchunks of a given chunk - beg_f_off, end_f_off
    - print out the file verbatim
    ...
      IF there WAS a ref, RECURSE on the new chunk name

That should be it

<<pass 2 and print>>=
        <<sub: tangle recursively - nondestructive>>
@


Now - the RECURSIVE PRINTOUT untangling from the default/given root.
--seek, then "read" into some $buf, which then is printed--

    (a) if the pair got from the hash of lists is digits, print out 
    (b) if the first is a specia string "ref", get the ref name from
    the second item in the pair and go there recursively


<<sub: tangle recursively - nondestructive>>=

# USAGE: print_chunk(name_of_chunk, left_margin, print_newline_flag)
sub print_chunk {

 (my $chunk_being_printed,
    my $snippet_left_margin, 
          my $snippet_print_newline_flag, @rest) = @_; 


 #~ print "\n---- printing chunk $chunk_being_printed --------\n";
 #~ print "DEBUG: got left.m. $snippet_left_margin nl. flag $snippet_print_newline_flag \n";

 # -- error mess. not to fail silently --
 unless ( defined $file_offsets_hash{$chunk_being_printed} ) {
    die "\n\tERROR: chunk $chunk_being_printed not found in file $ARGV[0]\n\n";
 }


 # iterate over splinters of a chunk, which are foff pairs
 my $iterate_lines = 0;
 for ( my $iterate_foffs = 0;
            exists $file_offsets_hash{$chunk_being_printed}[$iterate_foffs];)
 {

    my $snippet_position =  $file_offsets_hash{$chunk_being_printed}[$iterate_foffs++];
    my $snippet_end = $file_offsets_hash{$chunk_being_printed}[$iterate_foffs++];

    #~ print "debug got: beg $snippet_position -- end $snippet_end\n";

    <<if the record is a ref, call it recursively>>
    <<else print out the chunk>>

 } # rof non-destructive
        
     return 1;
} # bus -- ends the recursive sub

@

ok, if we got a chunk ref in our array of pairs, then the next entry after it is 
        (left_margin) 
(that is my nano-protocol here ;)  ). I use these numbers to keep the 
"printer state" before recursively calling this function for the referred 
chunk.

<<if the record is a ref, call it recursively>>=

    if ($snippet_position eq "ref") {

        my $snippet_left_margin_ref = 
            $file_offsets_hash{$chunk_being_printed}[$iterate_foffs++];

        # any call to a ref uncurs "print newline" flag of 0
        print_chunk($snippet_end, $snippet_left_margin_ref, 0);
    
    }
@

if the next data you got from your list is pairs of offsets (beg, end), 
print them out, observing (a) left margine this chunk is at, and
(b) skipping newlines after a chunk (could be an in-lined ref) unless 
this chunk is last in the referring line.

For that I'll have to keep "a printer state" wth two flags for (a) and (b).

.b. Multifile ./b.
Now $snippet_position and $snippet_end are not just offsets, but strings like
"LITSOURCE0-2365"
I'll need to split them and supply the two pieces to "seek" and "read".

<<else print out the chunk>>=
    else { # .. print it here

        (my $litsrc_fhname_beg, my $litsrc_line_beg) =
            split '-', $file_lines_hash{$chunk_being_printed}[$iterate_lines++];
        (my $litsrc_fhname_end, my $litsrc_line_end) =
            split '-', $file_lines_hash{$chunk_being_printed}[$iterate_lines++];

        (my $litsrc_fhname_beg, my $litsrc_foff_beg) = split '-', $snippet_position;
        (my $litsrc_fhname_end, my $litsrc_foff_end) = split '-', $snippet_end;
        # DEBUG prints:
        #print "in file_beg $litsrc_fhname_beg $litsrc_foff_beg\n";
        #print "in file_end $litsrc_fhname_end $litsrc_foff_end\n------\n"; 


        # the file is always the same, though. You cannot have one chunk splinter
        # start in one file and end in another one. (2 splinters can live in 2 files)
        seek $litsrc_fhname_beg, $litsrc_foff_beg,  0;
        read $litsrc_fhname_end, $buffer_out, ($litsrc_foff_end - $litsrc_foff_beg);

        #----Newlines at the end of chunks and refs. 
        # only for the last splinter of a chunk, do newline control:
        if ( ((scalar @{$file_offsets_hash{$chunk_being_printed}}) - $iterate_foffs ) == 0 )
        { 
                # works, but is suspicious logically:
                # maybe I just have not invented a counterexample yet, and
                # it's a trap waiting for its quarry. But it works
                $buffer_out =~ s!\n([\s]*)$!$1! unless ($snippet_print_newline_flag);
        }

        #----Left indent/margin. Seems OK

        $chunk_left_margin =  $snippet_left_margin;
        $buffer_out =~ s!(\n)!$1$chunk_left_margin!sg;


        #----And Print Out:

        if ($print_ref_linenos_when_tangling){
        print $code_sections_comment_symbol, 
            "[line ", $litsrc_line_beg," in file ",
            $LITSOURCE_hash{$litsrc_fhname_beg}, "]__start" ;
        print "_________________________________\n";
        }
        
        print $buffer_out;

        if ($print_ref_linenos_when_tangling) {
        print $code_sections_comment_symbol,
            "[line ", $litsrc_line_end," in file ",
            $LITSOURCE_hash{$litsrc_fhname_end}, "]__end" ;
        print "___________________________________\n";
        }

    } #esle for printout

@


#---------------------
{{{7 <h7> two debug chunks</h7>
#---------------------


<<DEBUG print out full hash>>=

        for $k (keys %file_offsets_hash) {
                #~ print @{$file_offsets_hash{$k}}, "\n";

         while (scalar @{$file_offsets_hash{$k}}){
                print "beg $k foff - ", shift @{$file_offsets_hash{$k}};
                print " -- end foff - ", shift @{$file_offsets_hash{$k}}, "\n";
                }
        }
@

<<DEBUG check hash keys>>=

 #~ for $k (keys  %file_offsets_hash){
        #~ print $k, "\n";
        #~ }

@

}}}7


}}}6

/* end of tangler part of script in the litsource file */

}}}5

}}}4


#-------------------------------------------------------------
{{{4 .h4. (VIRTUAL VIEW): adding multi-file tangling ./h4.
#-------------------------------------------------------------

.i. This is (VIRTUAL VIEW) - a Virtual View on the code with LINKED_CHUNKs./i.

The tangler was developed incrementally, and ealier variation could tangle from one
file only, and only in Quick Mode (i.e. by running mollified "Lit.Src.tangle" as
script file to dump the code to STDOUT)

Later command-line invocation was added, and ability to tangle from many files, 
supplied on command line. ("Quick Mode" still tangles from the only mollified
file, and uses whatever is assigned as the current default root chunk, "*")

This change affected a number of code chunks in various places.

First, the "despatcher" at the top of Molly script was modified.

[[LINKED_CHUNK main despatcher]]

while the previous simple despatcher was renamed and put away.

There are several comments I could make on the newer one..........

[[LINKED_CHUNK main despatcher with CL]]

Then, because I still wanted to preserve pass-through Quick Tangling, which 
hands the job to "notangle" from "noweb" tools by N. Ramsey, I had to split
dispatching and keep a second part in

[[LINKED_CHUNK tangle me]]

Actually, the first, main despatcher uses two labels, "TANGLE_ME" if there is
only one file to tangle, to enable pass-through or built-in tangler use, and
"SEEK_PEEK_TANGLER", to send execution for multi-file tangling with the built-in
tangler.
.b.Note./b. that "notangle" by Ramsey also can tangle from multiple Literate Source files
when supplied those on its command line.
.b.Note./b. also that pass-through tanlging filters out doc chunks - something practical
because Ramsey's tool dies on finding unescaped <<bla-bla>> anywhere in the file.
I switch "doc section - code section" only on <<bla-bla>>= in the .b. first position ./b.
on the line, and do not react to <<bla-blas>> anywhere, but inside code chunks.

Now, the tangling has been wrapped inside a "foreach" loop, going over a list of 
opened files. I had to keep their names standard, "LITSOURCE0", "LITSOURCE1", ...
for the code to work in all cases, and LITSOURCE0 is given as a handle even  in case of
one file only. This and the wish to preserve pass-through tangling was the reason for a
certain clumsiness in the despatcher.

[[LINKED_CHUNK while-loop and peek file ofsets]]


Now, the "seek-peek" tangler algorithm is this: 
(1) make pass 1 over all LitSource files collecting offsets for code chunks and for
references to other code chunks embedded in them. The information is collected in
hashes.
(2) Then make another pass, starting from the given root chunk and "seek" jumping
between files and known offsets, then printing out. No building of a tree with 
subsequent topological sort in memory.

To turn this algoritm into a multi-file app, it was sufficient to prepend filehandle
names to collected offsets, and on the second pass to split the strings and proceed
to the known filename and offset.
Note the use of $LITSOURCE-multi in the first pass chunks:

[[LINKED_CHUNK  chunk beginning]]

[[LINKED_CHUNK  chunk ending]]

[[LINKED_CHUNK chunk reference - multiref inside other text]]

............................
............................
............................

One more later addition was "topological sort" subroutine. It is not needed for tangling
itself and provides (a) some check for reference loops and (b) allows one to print out
all possible root chunks in a Literate Source file.


.b. .i. ....unfinished.... ./i. ./b.

{{{5 .h5. Scrap ./h5.
.b. Brief descrtiption ./b.

1. Files opened with universal filehandles LITSOURCE0, 1, ...
Both for 1-file case (both for QuickTangle and CL), then
despatched correctly (one file, pass-through, multifile-CL) - 
multifile can happen only on CL only for external files.

File opening, the whole chain not changed for weaving.

2. Order of files on CL is .i. important ./i., as there are
cases of one chunk living in its additive splinters in many
files. According to Knuth, Ramsey such a chunk is added to
in the order or occurrence. This behavious is implemented
with a LITSOURCE_list. /fh-fhame/ correspondence for line ##
printout is kept in a hash.



.b. Tests for ./b. /to add to the Makefile/

Tests live in their own directory, ./tmp-tests/assembly.order*
.pre.
 perl ../../MOLLY.pl -l# -R 'chunk A' nl-0 nl-1 nl-2 nl-3 nl-4 nl-10 nl-11
./pre.

.b. .i. not finished ./i. ./b.


}}}5

}}}4


}}}3


.h3. ./h3.

#--------------------------------
{{{3 .h3. WEAVER ./h3.
#--------------------------------


The idea of the weaver is simple, again.

<<weave me>>=


WEAVE_ME:

#1. Set formatting strings for weaver
<<set formatting strings for weaver>>

#2. accumulate result in a buffer
<<accumulate result in a buffer>>

#3. print out the TOC, the Chunks Index, the output buffer and close the page.
<<print out>>

exit;

@


#---------------------------------------------------------------
{{{4 .h4. How formatting is done in the weaver ./h4.
#-------------------------------------------------------------

.b.1 The first formatting element ./b.  that is  part of the .i. function ./i. of the script
rather than formatting decorations is the
.pre. fieldset - legend - /legend - /fieldset ./pre.
which create the distinctive look of the documents generated by MOLLY. I do not wish to
change them (although strictly speaking they are not obligatory).

What can be changed is the Stylesheet (a part of the script, it has its own subsection), so
other people could play with fonts and colors and margins and such by throwing in a simple
switch and copying their alternative stylesheet into the script.

I do not wish to use external files for any configuration, as the whole idea of MOLLY is to
be a single self-contained script, which does not require any "installation" of many files to
many places.

.b.2  Secondly,./b. the "actual working test.html" subsection below probably explains best how
.b. text folding and background highlighting ./b. which is tied to it (i.e. a TOC click will open the 
section + highlight; and click on the section in the body of the doc will toggle it + highlight 
the TOC line ) work.

Actual implementation is spread between several subsections - they are
coded in in the hard way, as I do not expect them to change ever, they are a functional element
not a changeable "skin" or decoration.
.ul..li. vars, JS and styles are set in the weaver "set formatting strings for weaver" subsections;
./li..li. invocations can be seen in "accumulate the result", esp in the "section headings"
./li..li. and more are applied in the "print out the result"
./li../ul.

Some of the vars are set to pieces of formatting strings in the beginning, and later variables
in them are interpolated with "executable" regexs ( the "e" flag at the end of the regex) - see 
"process section headings"


#---------------------------------------
{{{5 .h5. actual working test.html ./h5. 
#---------------------------------------

Here's the actual script that was used to test the JS folding and
tying of folding to the TOC section highlighting:


<<test.html>>=

<html>
<head>


<style type="text/css">


.unhilited {background-color:white}
.hilited {background-color:yellow; text-decoration:underline}
</style>


<script type="text/javascript">

    function setHilite(evt) {
        evt = (evt) ? evt : ((window.event) ? window.event : null);
        if (evt) {
            var elem = (evt.srcElement) ? evt.srcElement : evt.target;
            elem.className = "hilited";
        }
    }


function setUnHilite(evt) {
    evt = (evt) ? evt : ((window.event) ? window.event : null);
        if (evt) {
            var elem = (evt.srcElement) ? evt.srcElement : evt.target;
            elem.className = "unhilited";
        }
}

function toggleDiv(divid) {
var el = document.getElementById(divid);
el.style.display = (el.style.display == 'block') ? 'none' : 'block';
}

function toggleCombined(divid){
    if(document.getElementById(divid).style.display == 'none'){
      document.getElementById(divid).style.display = 'block';
        document.getElementById("toc"+divid).className="hilited";
    }
    
    else{
      document.getElementById(divid).style.display = 'none';
        document.getElementById("toc"+divid).className="unhilited";
    }
}


function expandDiv(divid){
        document.getElementById("toc"+divid).className="hilited";
}



</script>
</head>

<body >
<h2>Here's the tested element</h2>
Here is some ordinary text<br>
<span class="unhilited" onmouseover="setHilite(event)" 
    onmouseout="setUnHilite(event)"> 
    <a href="javascript:;" onmousedown="toggleDiv(15);"><b> process section headings </b></a><br>
    and this is some potentially hot spot text.</span>

<div id=15>
    .. and this is the text<br>
    that should be hidden/collapsed

</div>


<h2>Here's the tested element 0</h2>
HHHHere is some ordinary text<br>
<a href="javascript:;" onmousedown="toggleCombined(13);" id="toc13">
    <b> process section headings </b></a><br>

<div id=13 ><script language=javascript> document.getElementById("toc"+13).className='hilited';</script>
    .. and this is the text<br>
    that should be hidden/collapsed

</div>




<h2>Here's the tested element 1</h2>
HHHHere is some ordinary text<br>
<a href="javascript:;" onmousedown="toggleCombined(14);" id="toc14">
    <script language=javascript> document.getElementById("toc"+14).className='hilited';</script>
    <b> process section headings </b></a><br>

<div id=14 >
    .. and this is the text<br>
    that should be hidden/collapsed

</div>


<h2>Here's the tested element 2</h2>
HHHHere is some ordinary text<br>
<span  id="toc16"> 
    <a href="javascript:;" onmousedown="toggleCombined(16);"><b> process section headings </b></a><br>
    and this is some potentially hot spot text.</span>

<div id=16>
    .. and this is the text<br>
    that should be hidden/collapsed

</div>



</body>
</html>

@

}}}5


#----------------------------------------------------------------
{{{5 .h5. changing text style - the basic JS recipy ./h5.
#-----------------------------------------------------------------

I used this snippet for a start then reworked it into the "working test script" below.

.ul. .pre.
11.8.2 Solution
First, define two style sheet rules, each with a different class selector. Then design 
an event handler for the element to change the element's className property to the desired 
class selector's identifier: 

                <style type="text/css">
                .unhilited {background-color:white}
                .hilited {background-color:yellow; text-decoration:underline}
                </style>
                ...
                <script type="text/javascript">
                function setHilite(evt) {
                        evt = (evt) ? evt : ((window.event) ? window.event : null);
                        if (evt) {
                                var elem = (evt.srcElement) ? evt.srcElement : evt.target;
                                elem.className = "hilited";
                        }
                }
                function setUnHilite(evt) {
                        evt = (evt) ? evt : ((window.event) ? window.event : null);
                        if (evt) {
                                var elem = (evt.srcElement) ? evt.srcElement : evt.target;
                                elem.className = "unhilited";
                        }
                }
                ...
                <span class="unhilited" onmouseover="setHilite(event)" 
                        onmouseout="setUnHilite(event)">Some potentially hot spot text.</span>

Adjusting the className property of an element as shown here is a more stable approach 
for early versions of Netscape 6 instead of manipulating styleSheet objects and their 
properties. It is perhaps the most widely used and supported way to implement dynamic styles. 
./pre. ./ul.

If I enclose one "onmousedown" inside another, JScript will most probably not pass it into the inner one.
So I should add:
(a) the TOC line IDs ?? Like "toc.1", "toc.2" etc
(b) keep a separate func, not "toggleDiv", but something else like "toggleTOCandDIV", and run 2 things
from there ??

.b. So, 2 tests: enclosing and if not, then rewriting the collapsing JS func ./b.

}}}5

#--------------------------------------------------
 .h5. Reference - Javascript and DOM ./h5.
#--------------------------------------------------


I'll need some javascript and DOM reference for further work;

a/ http://w3schools.com/htmldom/dom_reference.asp  -- ref from W3C
b/ http://w3schools.com/htmldom/default.asp -- might also be useful
c/ http://www.howtocreate.co.uk/tutorials/javascript/domcss --javascript tutorial (not necessarily best) 
d/ http://www.pageresource.com/dhtml/ryan/part4-1.html - another tutorial


}}}4


#------------------------------------------------------------
{{{4 .h4. set formatting strings for weaver ./h4.
#------------------------------------------------------------

1. Formatting strings include variables, later substituted for each new section and chunk.
One idea would be to interpolate these strings with regexps then - with "s/X/Y/ee"
Another is to split the strings into "pre-X" and "post-X" and leave the var outside.
I use both methods. The second one is clumsier, but maybe not, as the first might
require keeping state in vars and copying strings anyway.

But both solutions are used.

2. Output file header and the outer table.

ASCIIMathML.js is cut in in a trivial way. 
The library allows extension with new operators/translatons from LaTeX into MathML.
Those can be cut in here, too.

<<set formatting strings for weaver>>=

  #----SETTING FORMATTING STRINGS-----------
  
$html_head = <<head_end_1;  

<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 

head_end_1

#switch on ASCIIMathML.js library if enabled in template options:
if ($enable_ASCIIMathML) {
$html_head .= "\n" . qq(<script type="text/javascript" src="$path_to_ASCIIMathML"></script>) . "\n";
    }

$html_head .= <<head_end;

<<JS script functions>>
<<stylesheet>>

</head>
head_end

$html_body_table = "<center><table class='outertable'><tr><td>";
$html_body_table_end = "\n</td></tr></table></center></body></html>\n";

@
        

3. This is split vars around each of the HTML folding sections; they substitute simple 
HTML headings in the literate source

<<set formatting strings for weaver>>=

$folding_section_start1_str = <<'fold_sect_start_1_xxx';
        <fieldset><legend><a href="javascript:;" onmousedown="toggleCombined('$section_num');">
fold_sect_start_1_xxx


$folding_section_start2_str = <<'fold_sect_start_2_xxx';
</a></legend></fieldset>
<p>
<div id="$section_num" style="display:$fold_state"> $highlight_state  
<ul>

fold_sect_start_2_xxx


$folding_section_end_str = <<'folding_section_end_xxx';
</ul>
<p>
<br>
<i><font size=-1>
<a href="javascript:;"onmousedown="toggleCombined('$section_num_prev');">
Close the subsection</a></font>
</i> -- <i><font size=-1>
<a href="javascript:;" onmousedown="showAll();">
expand all</a> -- 
<a href="javascript:;" onmousedown="hideAll();">
collapse all</a>
</font></i>

<p>
</div>

folding_section_end_xxx

@

4. This is for the frames around the code chunks:

<<set formatting strings for weaver>>=

#$code_fieldset_start_pre = q(<pre><fieldset class='codefieldset' id='$codechunk_id'><legend class='codelegend'>);
$code_fieldset_start_pre = q(<pre><fieldset class='codefieldset'><legend class='codelegend'>);
$code_fieldset_start_post = "=</legend>";
$code_fieldset_end = "</fieldset></pre>\n";

@

#----------------------------------------
{{{5 .h5. JS functions ./h5.
#----------------------------------------

In my attempts to insert ASCIIMathML.js, I have tried some variations of
math sections markup (with "a m a t h" -- "e n d a m a t h" words and
with backquotes) -- but the stinking lib insists on garbling my file, as
sth triggers it on earlier.

So a next step is to change regexps inside the Javascript lib and forse it
to sane markups. Probably by adding ancoring the key word to the beginning
of the line and/or insisting that it is there alone.

<<JS script functions>>=

<script language="javascript">

function toggleDiv(divid) {
var el = document.getElementById(divid);
el.style.display = (el.style.display == 'block') ? 'none' : 'block';
}


function toggleCombined(divid){
    if(document.getElementById(divid).style.display == 'none'){
      document.getElementById(divid).style.display = 'block';
        document.getElementById("toc"+divid).className="hilited";
    }
    else{
      document.getElementById(divid).style.display = 'none';
        document.getElementById("toc"+divid).className="unhilited";
    }
}


function showAll(){
for(i=1; i <= 10000; i++){
    document.getElementById(i).style.display = 'block';
    document.getElementById("toc"+i).className="hilited";
    };
}

function hideAll(){
for(i=1; i <= 10000; i++){
    document.getElementById(i).style.display = 'none';
    document.getElementById("toc"+i).className="unhilited";
    };
}


function open_all_above(someid){

    elem = document.getElementById(someid);
	elem.style.display = 'block';
	document.getElementById('toc'+someid).className = 'hilited';
	
    while (elem.parentNode.id != 1) {
		if (elem.parentNode.nodeType == 1) {
		elem.parentNode.style.display = 'block';
		//document.getElementById(elem.parentNode.id).className = 'hilited';
		var toc_elem = document.getElementById('toc'+elem.id);
		if (toc_elem) {toc_elem.className = 'hilited'};
		}
		elem = elem.parentNode;
	}
}


<<Javascript: node clone and destroy clone>>

</script>

@

note that  "Javascript: node clone and destroy clone" is in the "virtual" ADD-ON section

}}}5

#--------------------------------------
{{{5 .h5. Stylesheet ./h5.
#--------------------------------------

<<stylesheet>>=
<style type="text/css" media="screen">


BODY {
        FONT-SIZE: 10pt;
        <!--FONT-FAMILY: sans-serif -->
        background: #f0f0f0;
        }
FIELDSET {
        BORDER-RIGHT: #000000 1px solid; 
        BORDER-TOP: #000000 1px solid; 
        BORDER-LEFT: #000000 1px solid; 
        BORDER-BOTTOM: #000000 1px solid; 
        PADDING-RIGHT: 5px; 
        PADDING-LEFT: 5px; 
        PADDING-BOTTOM: 2px; 
        PADDING-TOP: 5px;
        MARGIN-BOTTOM: 1px; 
        background: #f5f5f5; 
        color: #000000;
        }
LEGEND {
        BORDER-RIGHT: #a9a9a9 1px solid; 
        BORDER-BOTTOM: #a9a9a9 1px solid;
        BORDER-TOP: #a9a9a9 1px solid; 
        BORDER-LEFT: #a9a9a9 1px solid; 
        PADDING-RIGHT: 20px; 
        PADDING-LEFT: 20px; 
        PADDING-BOTTOM: 5px; 
        PADDING-TOP: 5px; 
        FONT-WEIGHT: bold;  
        BACKGROUND: #fdfdfd; 
        color: #000000;
        }
PRE     {
        PADDING-LEFT: 20px; 
        PADDING-RIGHT: 5px; 
        padding-top: 0px; 
        padding-bottom: 6px;
        MARGIN-BOTTOM: 1px; 
        BORDER-TOP: #a9a9a9 0px solid;
        BORDER-RIGHT: #a9a9a9 0px solid; 
        BORDER-LEFT: #a9a9a9 0px solid;
        BORDER-BOTTOM: #a9a9a9 0px solid;        
        background: #fefefe;
        }


.tocfieldset {
        background: #ffffff; 
        color: #000000;
        }

.codefieldset {
        BORDER-RIGHT: #000 1px solid; 
        BORDER-TOP: #000 1px solid; 
        BORDER-LEFT: #000 1px solid; 
        BORDER-BOTTOM: #000 1px solid; 
        background: #ffffff; 
        color: #000;
        MARGIN-BOTTOM: 1px; 
        PADDING-LEFT: 15px; 
        PADDING-RIGHT: 5px; 
        PADDING-BOTTOM: 10px; 
        PADDING-TOP: 1px;
        }
.codelegend {
        BORDER-RIGHT: #777 1px solid; 
        BORDER-TOP: #777 1px solid; 
        BORDER-LEFT: #777 1px solid; 
        BORDER-BOTTOM: #777 1px solid
        PADDING-RIGHT: 10px; 
        PADDING-LEFT: 10px; 
        PADDING-TOP: 2px; 
        PADDING-BOTTOM: 2px; 
        background: #ffffff; 
        color: #00b;
        FONT-WEIGHT: bold;  
        <!--font-variant: small-caps; -->
        <!--font-style: italic; -->
        }



.chunkref {
        color: #00b;    
        background: #f6f6f6;
        /font-style: italic;
        font-weight: bold;
        <!--font-variant: small-caps; -->
        }


.outertable {
        width: 99%; 
        cellpadding: 25; 
        background: #ffffff; 
        border: 1px solid;
        }

.hl     {
         ;
        PADDING-LEFT: 5px; PADDING-RIGHT: 5px; 
        padding-top: 5px; padding-bottom: 5px;
        MARGIN-BOTTOM: 1px; 
        BORDER-TOP: #a9a9a9 0px solid;
        BORDER-RIGHT: #a9a9a9 0px solid; 
        BORDER-LEFT: #a9a9a9 0px solid;
        BORDER-BOTTOM: #a9a9a9 0px solid;        
        background: #f5f5f5;    
        width: 70%;
        }

.hl-wide {
         ;
        PADDING-LEFT: 5px; 
        PADDING-RIGHT: 5px; 
        padding-top: 15px; 
        padding-bottom: 15px;
        MARGIN-BOTTOM: 1px; 
        BORDER-TOP: #a9a9a9 0px solid;
        BORDER-RIGHT: #a9a9a9 0px solid; 
        BORDER-LEFT: #a9a9a9 0px solid;
        BORDER-BOTTOM: #a9a9a9 0px solid;        
        background: #fbfbfb;    
        }

.lnum {
        color: #a0a0a0;
        }

.unhilited {background-color:white}
.hilited {background-color:#c0c0ff}

.linked_chunk {
        PADDING-RIGHT: 5px; 
        PADDING-LEFT: 5px; 
        PADDING-BOTTOM: 5px; 
        PADDING-TOP: 5px; 
        BORDER-TOP: #c0c0c0 1px solid;
        BORDER-RIGHT: #c0c0c0 1px solid; 
        BORDER-LEFT: #c0c0c0 1px solid;
        BORDER-BOTTOM: #c0c0c0 1px solid;              
        color: #505050;
        background: #ffffff;
        }

.linked_chunk_legend {
        PADDING-RIGHT: 5px; 
        PADDING-LEFT: 5px; 
        PADDING-BOTTOM: 3px; 
        PADDING-TOP: 3px; 
        BORDER-TOP: #c0c0c0 1px solid;
        BORDER-RIGHT: #c0c0c0 1px solid; 
        BORDER-LEFT: #c0c0c0 1px solid;
        BORDER-BOTTOM: #c0c0c0 1px solid;              
        color: #505050;
        background: #ffffff;
        }

a:visited { color: darkblue; }

</STYLE>

@

}}}5


}}}4


#------------------------------------------------
{{{4 .h4. Accumulate the result ./h4.
#------------------------------------------------

Some of the vars may be unused later (run perl -wc to check?)

<<accumulate result in a buffer>>=
#2. accumulate result in a buffer


# vars for the main loop over lines of the target file

 $chunkbuf = ''; # collects whole formatted project file in memory
 $tocbuf = "";  # will accumulate TOC contents, i.e. small
 @indbuf = ();  # accumulates index of code chunks, small
 @headings = ();  # the stack for nested subsections numbers/ids
 %headings_id_hash = ();
 %chunks_id_hash = ();

 $section_num = 0;
 $section_num_prev = 0;
 $section_level = 0;
 $prev_section_level = $section_level;
 $line_counter = 0;
 $in_pre_tag = 0;
@


.b. 1. ./b. We can weave "mollified" files as well as use MOLLY.pl on external
files from command line, which do not necessarily include the config and
invocation of Molly at the top. 
So, first let's get rid of that part of the file, if it exists.

The idea is this.
    .ul.
    (a) not to clobber the overall "while" loop, do it inside it
    (b) If the first line of the file is a perl invocation,
        then check for "MOLLY.pl" call, then for _ _ D A T A _ _ line
        then for first white line or sth
        This cuts the "mollifier" part out.
        OR - drop off letting out a scream "wrongly formatted file"
    (c) for lines other than first, do regular processing
    ./ul.

This version of "mollified" template  will pass many
perl scripts with DATA sections, will just print those sections.
But I do not care at the moment.

No attempt is done to filter any other files in non-lit.src format etc.
This is just to quickly cut out the Molly perly template at the top.

<<cut out the MOLLY.pl invocation at the top of LS file>>=

if ( ($line_counter == 1) && (m!^#.*perl!) ) {

        # Matched? - this must be either the mollifying template or some perl script.
        
        do {
            $_ = <LITSOURCE>;
            $line_counter++;
            if (eof LITSOURCE) {
                print "\n\t--------ERROR: wrong target file format for weaving--------\n";
                print "\tmay be a regular perl script, without a call for Molly\n\n";
                exit;
            }
        } until ( lc($_) =~ m!^do.*molly!) ;    
        
        # throw away lines until DATA, then process/weave normally
        do {
            $_ = <LITSOURCE>;
            $line_counter++;
            if (eof LITSOURCE) {
                print "\n\t--------ERROR: wrong target file format for weaving--------\n";
                print "\tdid not find the  __DATA__ keyword  in first pos on its line\n\n";
                exit;
            }
        } until ( m!^__DATA__! ) ;

        next;

} # fi cutting out MOLLY.pl template/config  if present in lit.source target file

@

.b. 2. The logic of processing is this: ./b.
(a) all project files, unless they are machine-generated code, are much below the 
sizes of RAM on modern machines. So I accumulate in memory (in a string $chunkbuf)
a copy of the whole file, with necessary transformations and formatting on the fly.

Only 3 distinctions need to be made while iterating over hte lines of the Literate Source
file, between section headings, chunks of code and the lines from the body of documentation
chunks.

<<accumulate result in a buffer>>=



while (<LITSOURCE>) {

$line_counter++;

<<cut out the MOLLY.pl invocation at the top of LS file>>

<<process code chunks>>
<<process section headings>>
<<body HTML formatters>>

    # debug
    #print "---- $_";


} #elihw over the whole input file

close LITSOURCE;

 foreach (@headings)  {

        ($section_level, $section_num_prev) = split /-/, $_;
        $folding_section_end = $folding_section_end_str;
        $folding_section_end =~ s!(\$section_num_prev)!$1!ee;
        $chunkbuf .= $folding_section_end; 

        };

@





#-----------------------------------------------
{{{5 .h5. process code chunks ./h5.
#-----------------------------------------------

"goto" is from an old version of this script, with perl-specific options, I'll delete it
later.

I use perl range operator to cut out the code section between < < chunkname > > and the
closing "at" symbol, as is required in "noweb".
This is a use of the "range" operator inside another "while" loop over each line of the
Lit Source file, and they work together, the range just flips some status var ("in" - "out")
while the outside loop continues over the lines.

Once the section name is extracted with a regexp, the index buffer (indbuf) is formed and
updated.
Then "fieldset" formatting is written around the code chunk and line numbering (if set
in an option var) is written too

<<process code chunks>>=

if ( m!^<\<(.*)>\>=! ... m!^@\s*$! ) { # -- CODE CHUNKS -- 

        s/&/&amp;/g;    # escape &
        s/</&lt;/g;     # escape <
        s/>/&gt;/g;     # escape >
        my $codechunk_id = "";

      if ( m!(&lt;&lt;(.+?)&gt;&gt;)(=)?! ) 
          {
          $chunk_title = $2;
          $reference = $1;
          $ind_str = "&lt;&lt;$2&gt;&gt; $section_num";
          if (defined $3) {$ind_str .= "<sup>def</sup>"}
          else { s!$reference!<font class='chunkref'>$reference</font>! }

          unshift @indbuf, $ind_str;

        # /.. "if" is not finished in this chunk yet.. read on/
@

Here I have to create a unique $codechunk_id, save it in %chunks_id_hash, and
assign to the created fieldset by .s. interpolating it in $code_fieldset_start_pre ./s.
.. - was buggy, so I create a wrapping "div" and assign this ID to it.
Unique IDs can be calculated from a counter -- or taken from line numbers, which 
are also unique and an increasing sequence. Those are available as $line_counter

It also allows us to preserve line numbers for the second pass, interpolated print
of the formed $chunkbuf string.

I could do the same with section IDs, too. Will have to change code, but it will
allow me to get line numbers painlessly. Check this.

Second question - how to treat splintered chunks (like this one, split by this very
text).  Push IDs as a list? - possibly the simplest idea. 

<<process code chunks>>=

            # cut-in for virtual links handling: create codechunk_id 
            if (defined $3) {
            
                $codechunk_id = 'codechunk' . $.;
                push @{$chunks_id_hash{$chunk_title}}, $codechunk_id; # with splinters

                my $splinter_number = '';
                $splinter_number = @{$chunks_id_hash{$chunk_title}}
                    if @{$chunks_id_hash{$chunk_title}} > 1;

                # simple fieldset frames around code snippets if chunk definition
                # /changing current line $_ which will get appended to $chunkbuf/
                s!^&lt;&lt;(.+)&gt;&gt;=!
                <div id=$codechunk_id>
    $code_fieldset_start_pre&lt;&lt;$1&gt;&gt<sub>$splinter_number</sub>$code_fieldset_start_post!x;

            } # fi - cut-in for virtual chunk links

        } # fi - chunks index accumulation


        # close fieldset frame at end of code chunk
        s!^@\s*$!$code_fieldset_end</div>!;


        if ( $line_numbering ) { 
        $chunkbuf .= "<font class='lnum'>" . $line_counter . "</font>   " . $_;
        }
        else{
        $chunkbuf .= $_;
        }

} # fi code chunks

@

}}}5


#----------------------------------------------------
{{{5 .h5. process section headings ./h5.
#----------------------------------------------------

This is where folding sectons are created.
The more complicated logic is needed to .i.close./i. the sections properly, when subsections
are included (i.e. creating a step and forming the "close subsection" at the end in the correct
place.

1. Several formats were possible for collapsing sections, I chose the one you can observe while
reading the weaved document: subsections are completely enclosed by their supersections;
once a child subsection is initiated, the parent subsection cannot continue its body after
the end of the child.
I.e. once you started 12.2.1 inside your 12.2, you cannot write into 12.2 after 12.2.1 is finished, 
but you can add 12.2.2, 12.2.3, 12.2.2.1 - etc. - inside the enclosing section.

The logic was tested first in an external skeleton script.

2. I may need to disassemble this code to create "view mode" for my script - see "versions - 
CHANGES" subsection


.b. First./b. get the info from the heading and set several vars for substitutions etc.:
Also: print subsections marked with a "+" as unfolded (JS stype 'block'), rather than 
collapsed ('none')

<<process section headings>>=

# -- SECTION HEADINGS 
#elsif ( m!\.(\+)?h(\d)\.(.*?)\./h\d\.! ) {     # old version, no "rawHTML" enabled yet
elsif ( m!$tag_open_symbol(\+)?h(\d{1,2})$tag_close_symbol(.*?)$tag_open_symbol/h\d{1,2}$tag_close_symbol! ) {  



        # -- using split vars for substitution to avoid
        #       regexps and need to keep old state --   

        $section_num_prev = $section_num;
        $section_num = $section_num + 1;


                #default for fold state in "settings" ??
                if ($1 eq "+") {
                    $fold_state="block";
                    $highlight_state = qq!  <script language=javascript> 
                    document.getElementById("toc"+ $section_num).className='hilited';
                    </script> !;
                } 
                else {
                    $fold_state="none";
                    $highlight_state = "";
                };
                $section_level = $2;
                $section_title = $3;

                # add-on for virtual nodes; DEBUG for now
                $stripped_section_title = $section_title;
                $stripped_section_title =~ s!\s*(.*\S)\s*!$1!;
                $headings_id_hash{$stripped_section_title} = $section_num;

@
        
.b. Secondly, ./b. copy the formatting strings and modify copies with substitutions

<<process section headings>>=        

        $folding_section_start1 = $folding_section_start1_str;
        $folding_section_start2 = $folding_section_start2_str;
        $folding_section_end = $folding_section_end_str;


        $folding_section_start1 =~ s!(\$section_num)!$1!ee;
        $folding_section_start2 =~ s!(\$section_num)!$1!ee;
        $folding_section_start2 =~ s!(\$fold_state)!$1!ee;
        $folding_section_start2 =~ s!(\$highlight_state)!$1!ee;
        $folding_section_end =~ s!(\$section_num_prev)!$1!ee;


        $section_id = $section_level . '-' . $section_num;

@

Note that "$section_id"  above is a string with a dash that contains two pieces of info

.b. Three ./b. Take care of the contained folding subsections
The idea below is this:
When will I need to insert "close subsection" and refs to its folding JS funcs?
Supposing I started with h2. Then I'll skip h3, h4, .. - and must close it when
the next met subsection is again h2.
Array @headings keeps a stack of headings met during parsing.

//Q: Should I check if ($section_level >= $prev_section_level){ .. } ??
// To cleanly close sections when a prog made a mistake and missed a level?
// OR: should I leave it as it is, as it gives a visual clue of the error?
// OR: has it been taken care of already? - reading my own code ;))))))))))

<<process section headings>>=        
        # finish previous subsection if not the first section in the file
        # ..and deal with nesting of sections according to their "depth level"

        # this is NOT the first section:
        if ( exists $headings[0] ){

                ($prev_section_level, $prev_section_num)  = split /-/, $headings[0];

                if ($section_level == $prev_section_level){

                # close prev, start new
                $chunkbuf .= $folding_section_end;
                shift @headings;
                
                }

                elsif($section_level < $prev_section_level){
                
                  # close a bunch of them, in a loop -- THEN start a new one.
                  do  {
                        ($prev_section_level, $section_num_prev) = split /-/, shift @headings;
                        
                        $folding_section_end = $folding_section_end_str;
                        $folding_section_end =~ s!(\$section_num_prev)!$1!ee;
                        $chunkbuf .= $folding_section_end;

                   } while ( $section_level < $prev_section_level );
                }
        } # fi not the first section

        # end of "finish previous subsection if not the first section in the file"
@

.b. Four ./b.
And the common operations, which form the output buffer string, "$chunkbuf", and
output buffer for printing TOC with needed indentation, "$tocbuf"

<<process section headings>>=        

        # common operations             
        unshift @headings, $section_id ;
        $chunkbuf .= $folding_section_start1;
        $chunkbuf .= "<font class='lnum'><i>(" . $section_num . ")</i></font>"
            . "&nbsp;" . $section_title
            . "</a>&nbsp;<font class='lnum' size=-1><sub><i>(line "
            . $line_counter
            . ")</i></sub></font>"; 

        $chunkbuf .= " <font size=-2><i><a href='#tocancor'>toc</a></i></font>";

        #for F-links
        $chunkbuf .= "<a name='" . $section_num . "'>"; 

        $chunkbuf .= $folding_section_start2;

        #$chunkbuf .= "\n" . "<font class='lnum'><i>------ line " . $line_counter . 
        #           " ------</i></font><br>\n";

        # TOC Navigation: open all parents, then junp to section
        my $open_all_and_jump =  
                q/&nbsp;<a href="#/
                . $section_num
                . q/" onmousedown="open_all_above(/  
                . $section_num . 
                q/);" >/ . '))</a>';


        # TOC Navigation: open_all_above(someid)
        $tocbuf .= 
                $open_all_and_jump . "&nbsp;" 
                . q/&nbsp;<a href="javascript:;" onmousedown="open_all_above(/  
                . $section_num . 
                q/);" >/ . '<i>' . $section_num . '</i></a>';

        $toc_indent = "&nbsp;"x4 . "." x (($section_level-1) * 7);

        # old-disabled
        #$toc_indent = "&nbsp;" x ($section_level * 7);
        #$toc_indent = "&nbsp;" x (($section_level-1) * 7 );
        #$tocbuf .= "\n<p>\n" if ( $section_level == 1 ); 

        $tocbuf .= $toc_indent . 
                #--disabled--#"<i>" . $section_num . "</i>" .
                qq/&nbsp;<a href="javascript:;" onmousedown="toggleCombined(/ .  
                $section_num . 
                qq/);" id="toc/ . $section_num .
                qq/"><b>/ .
                $section_title . "</a>&nbsp;<a><font class='lnum' size=-1><i>(line " .
                    $line_counter . ")</i></font>" .
                    "</b></a>";

        # and end the line
        $tocbuf .= "<br>\n";



} #; fisle: end elif headings
        

@

}}}5

#---------------------------------------------------------------------
{{{5 .h5. Doc body formatters: rawHTML and others./h5.
#--------------------------------------------------------------------

The last "else" case is the documentation chunk body.
It includes one default formatter, which is "rawHTML", and can include a number
of other markups. That section should be made extensible by users.

Secondly, docsection  body may contain LINKED_CHUNKs (or LINKED_SECTIONs), so
I'll have to add them to @indbuf here.

So here we are:

<<body HTML formatters>>=

else { # this is the body of the doc section

<<rawHTML formatter>>
<<other body formatters>>

    if (m!^\s*\[\[LINKED_CHUNK(_\d+)?\s+(.*\S)\s*\]\]\s*<br>$!) { # -- CHUNK LINKS --
        my $index_str = "&lt;&lt;$2&gt;&gt; " . $section_num . "<sup>link</sup>";
        unshift @indbuf, $index_str;
    }

} # esle, fi -- end of processing inside 'while' over the file lines.

@


.b. Body formatters ./b.
Now, the default formatter starts an "if" (selecting with a config variable $weave_markup),
and all other selection clauses must implement "elsif" clauses. Those are not obligatory, 
as "if" is extensible, of course.

For the convenience of the users who might wish to add other markups to Molly, all
non-default sections are moved to the end to .b. ADD-ON: example of additional markup
./b. folding sections of this Literate Source file.

.b. rawHTML formatter./b.
Raw HTML formatting can be done. It is non-restricted, compared to the stripped-down
"dotHTML" of my invention, and  it also allows one to convert existing HTML documentation
into the folding format.
I.e. if you have a longish manual in which section headings are marked with HTML
h1, h2, h3, ... tags, you can convert it in 3 simple steps:
.ul. .li. delete opening and closing "html" and "head" "body" tags at the very top and
bottom of your  document
./li. .li. put perl invocation as the first line
./li. .li. set up markup mode in a variable: "$weave_markup = "rawHTML";" and "mollify"
the document, putting  "do MOLLY.pl" and DATA marker
./li. ./ul.
and if there are no interfering div section etc. - no complicated markup inside the
headings, you'll get an automatically generated folding HTML document.

If you do, it is sometimes easy to clean headings with a few regexps in a good editor
like Vim

It is very convenient and I keep large manuals as folding HTML documents.


<<rawHTML formatter>>=
        if( $weave_markup eq "rawHTML" ) {      # if the doc chunks marked up with real HTML
                s!^#-----.*!!;

              #s/^{{{\d+(.*)$/$1/;      # - eliminate vim folding markup, start 
                                        # - dummy, as it's killed in "headings" processing 
              s/^}}}\d+//;      # - eliminate vim folding markup, end


              # Paragraphs and line breaks are automatic now:
              # ... unless we are dealing with the "preformat" tag
                #--note! that ranges do not work here
                $in_pre_tag = 1 if (m!<pre>!);
                $in_pre_tag = 0 if (m!</pre>!);;


                unless ($in_pre_tag) {
                (m/^\s*$/) and s/$_/<p>\n/
                or s/\n/<br>\n/;
                }

                $chunkbuf .= $_;

        } # fi - default rawHTML formatter for body of doc chunks

@
    

}}}5

}}}4


#-------------------------------------------------------
{{{4 .h4. Print out the resulting page ./h4.
#-------------------------------------------------------

Page and the outer formatting table:

<<print out>>=

# begin the page:

  # /disabled doctype line/ - it garbles output of TOC (!!)
  #print q(<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" 
  #                 "http://www.w3.org/TR/html4/loose.dtd">), "\n";

  print "<html>\n  $html_head\n <body>  $html_body_table \n";

# print out the TOC, the Chunks Index, the output buffer and close the page.

@

Next print the short instructions section (collapsible) at the top:

<<print out>>=

print <<end_of_print;
<p><fieldset class='tocfieldset'>
<legend><b>TABLE OF CONTENTS: outline of the document structure</b></legend>

<ul>
<p>
<br>
<div class='hl' align=center>
<a href="javascript:;" onmousedown="toggleDiv('tochowto');">
<b>HOW TO USE THE FOLDING DOCUMENT [expand/collapse]</b></a>
</div>
<div id='tochowto' style='display:none' style='background:#ffffff'> 
<p>
<ul>
<li><b>Collapsing is necessary when</b> you work on code and must exclude 
<br>irrelevant sections of the rest of the literate project file. This 
<br>greatly helps to clear thinking by eliminating a general feeling of 
<br>being in a maze of code and unnecessary "housekeeping" tasks.
<br>One can say that there is a limited "buffer capacity" in the human
<br>mind, and relieving it of the need to remember where things are in a 
<br>larger file, at which other points one must fill in values or adjust
<br>invocation etc. <i>immediately makes the user "more intelligent"</i>
<p>
</li><li><b>TOC: to toggle</b> a section open/closed, click on the <i>corresponding link</i>
<br>Remember to open <i>all sections above it</i> for it to become visible.
</li><li><b>TOC: to open all sections above</b> some internal subsection to make
<br>it visible, click on the <i>section number</i> in the column on the left.
</li><li><b>TOC: to open all above and jump</b> click on the leftmost symbol.
<p>
</li><li><b>To restore the default view</b> <i>reload</i> the page in the browser.
<p>
</li><li>
<b>To keep some sections open</b> upon each reload: supposing you
<br>work on the code, constantly update it and cannot bother to reopen it 
<br>again and again - mark the headings for the sections in the source file
<br>with a plus, i.e. write the opening tag (only)  as +h2, +h3. Again, all
<br>sections above must be marked open too. 
<br><b>Note that</b> "expand all" and "collapse all" disregard these settings.
<br>Reload the page after using those options to again view the text
<br>according to your preferences.
<p>
</li><li>
<b> To use the Index </b>, click on the numbered sections in the TOC above
<br>(opening them; use highlighting as a guide; when sections are visible,
<br>the slider on your browser window  will shorten too), or "expand all", 
<br>and then use your browser's Find function to highlight all chunk name 
<br>instances in the visible text
<p>
</li><li>
<b>To search for variables etc.</b>, "expand all" text - or manually 
<br>expand needed sections - and then use your browser's Find function to 
<br>highlight all and jump between the found items.
<p></li>

</div> 
<p>
<br>
<p>
<div class='hl' align=center>
<p>
<a href="javascript:;" onmousedown="toggleDiv('tocmain');">
<b>TABLE OF CONTENTS [expand/collapse]</b></a>
<a name="tocancor"></a>
<font size=-1 color=grey40><i>
<p><b>Section name</b> toggles expanded state. <b>Subsection number</b> on the left opens
<br>all parent sections to make it visible. <b>Leftmost symbol</b> opens parents and
<br>jumps to the section.
</i></font>
</div>

<div id='tocmain' style='display:$toc_expanded' style='background:#ffffff'> 
<p>
<br>
<p>
end_of_print

@

Table of Contents from the formed "tocbuf":

<<print out>>=

 print "$tocbuf" if $print_toc;
 #print "$tocbuf";

print "</div>\n";

@

Start the "Index of code chunks", collapsible:

<<print out>>=

print <<end_of_print;
<p>
<br>
<p>
<!--fieldset><legend-->
<div class='hl' align=center>
<a href="javascript:;" onmousedown="toggleDiv('indbuf');">
<b>INDEX of Code Chunks [expand/collapse]</b></a>
</div>
<!--/legend></fieldset-->
<p>
<div id='indbuf' style='display:$ind_expanded' style='background:#ffffff'> 
<ul>
<p>
A <b>"def" superscript</b> means the chunk is defined,<br>
a <b>"link"</b> that the chunk is being linked to from that section,<br>
and a <b>bare number</b> means the chunk is being used in a section.
<p>
end_of_print

@


Index of code chunks; calculated in this snippet:

<<print out>>=

         $ind_outbuf = '';
         $prev_ch_name = '';

         #for (sort @indbuf){ print $_, "<br>";}

         for (sort @indbuf){

           ($ch_name, $closing_bracket, $ref_num) = split /&gt;/, $_; 
                #/ - for editor colouring bug

            if ( $ch_name eq $prev_ch_name ){ 
              $ind_outbuf .= " <b>" . $ref_num . "</b> ";
            }
            else{
              print  $ind_outbuf, "<br>\n"; 
              $ch_namestr = $ch_name;
              $ch_namestr =~ s!&lt;&lt;!!;
              $ind_outbuf = 
                "<b>&lt;&lt;</b><font class='chunkref'>" . 
                $ch_namestr . 
                "</font><b>&gt;&gt;</b> -- <b>" . 
                $ref_num . 
                "</b> ";
            }
            $prev_ch_name = $ch_name;

         }; # rof - forming the code chunks index

        print $ind_outbuf, "<br>\n";

@


"expand all" and "collapse all" at the end of the TOC/Chunk Index section 
at the top of the page:

<<print out>>=

  # The "expand all" "collapse all" control

print <<end_of_print;
</div>\n<p>
<br>
<p><div class='hl' align=center><i>
<a href="javascript:;" onmousedown="showAll();">
expand all</a> -- 
<a href="javascript:;" onmousedown="hideAll();">
collapse all</a>
</i></div><p>
end_of_print


 print "</ul></fieldset><p>\n<br>";

@



..and..
.b. PRINT the document itself./b., formed earlier in the "chunkbuf" string in memory.
This section was greatly modified for LINKED_CHUNKs and LINKED_SECTIONs to "while"
over lines in the $chunkbuf with regexps and do the substitutions if links are presentat
in the document.


<<print out>>=
<<print out - with virtual nodes>>
@


.b. see ADD_ON for "Virtual Nodes" ./b. - there's a logical overview of 
all changes that were implemented.

Briefly here:
(a) I suppose those live inside regular subsections
(b) I suppose they are marked as "[[", then"LINKED_CHUNK" or "LINKED_CHUNK_3",
then  "literate copy of the node to clone", then "]]".
    (one or more space chars after LINKED_SECTION or LINKED_CHUNK)
(c) They live inside documentation chunks, and so I do not need to bypass them
    in regular code processing, tangler or weaver, above (?)
(d) In this final parsing of $chunkbuf str I must distinguish them from chunk
    regular refs. But if I reserve LINKED_xxx as two unique identifiers, that's it.

Seems, just an ordinary match inside the string will do.
mm.. need to distinguish from a passing usage in docs, like in (b).
Let's restrict syntax to ONLY MEANINGFUL CHARS ON THE LINE:

(e) .. and finally, I need to subst the matching line, but pass through
    and print out everything else.

(f) The code is ugly because I am still debugging it and wish to keep the two cases
(of CHUNKs and SECTIONs) completely separate in spite of duplicated code


<<print out - with virtual nodes>>=

#The FULL OUTPUT, the file body (with links handling  added):

sub print_chunk_link {

(my $name_of_linked_chunk, $virtual_id, my $created_clone_div_id, my $shown_splinter_num) = @_;

print <<"end_of_clone_sect";

        <fieldset class='linked_chunk'>
        <legend class='linked_chunk_legend'>
        $lt_esc$lt_esc$name_of_linked_chunk$gt_esc$gt_esc $shown_splinter_num
        <font size=-2><i>
        <a href="javascript:;"
        onmousedown="CreateVirtualNode('$virtual_id', '$created_clone_div_id');"
        > [open] </a>
        <a href="javascript:;"
        onmousedown="DeleteVirtualNode('$created_clone_div_id');"
        > [close]</a></i></font>
        </legend>
        <div id="$created_clone_div_id">
        </div>
        </fieldset>

end_of_clone_sect

        # a DEBUG printout:
        #for (keys %chunks_id_hash) {print "[$_] => $chunks_id_hash{$_}<br>\n"}

        #<br>DEBUG: trying to clone [[$1]] whose id is [$virtual_id] 
        #OR [$chunks_id_hash{$name_of_linked_chunk}]

} # bus -- printing out formatted CHUNK_LINK cloning line


my $clonebody_cnt = 0;

while ($chunkbuf =~ m!^(.*)$!gm ) { # -- line by line iteration over the doc as string in mem
    my $printed_line = $1;

<<if LINKED_CHUNK, create clone-destructclone fieldset>>

<<elsif LINKED_SECTION, create clone-destructclone fieldset>>

<<elsif create FLINKs>>

else{ # -- BODY OF THE DOCUMENT--  no virtual links met. Just print

            print $printed_line, "\n"; 

} # 

} # eliwh - end of line-by-line iteration over the buffer string in memory 

@


To take care of splinters of a chunk with the same name, I'll have to loop now over the 
list kept in the %chunks_id_hash, and repeat the same clone creation.

I should do it the same way I did in Tangler, it's the same data structure.. done.

<<if LINKED_CHUNK, create clone-destructclone fieldset>>=

if ($printed_line =~ m!^\s*\[\[LINKED_CHUNK(_\d+)?\s+(.*\S)\s*\]\]\s*<br>$!) { # -- CHUNK LINKS --

    my $name_of_linked_chunk = $2;

    if ($1) { # printing a single splinter LINKED_CHUNK_N of a given number
        my $splinter_cnt = (substr $1, 1);
        $virtual_id = $chunks_id_hash{$name_of_linked_chunk}[$splinter_cnt-1]; 
        my $created_clone_div_id = "clonebody" . $clonebody_cnt;
        $clonebody_cnt++;
        my $shown_splinter_num = "(" . $splinter_cnt . ")" ;

        print_chunk_link($name_of_linked_chunk, $virtual_id, $created_clone_div_id, $shown_splinter_num);

    }
    else { # regular case: printing all splinters of a LINKED_CHUNK
        for (my $splinter_cnt=0;
                exists $chunks_id_hash{$name_of_linked_chunk}[$splinter_cnt]; )
        {

        my $virtual_id = $chunks_id_hash{$name_of_linked_chunk}[$splinter_cnt++];   
        my $created_clone_div_id = "clonebody" . $clonebody_cnt;
        $clonebody_cnt++;

        my $shown_splinter_num = 
                "(" . $splinter_cnt . ")" 
                        if $splinter_cnt > 1;

        print_chunk_link($name_of_linked_chunk, $virtual_id, $created_clone_div_id, $shown_splinter_num);


        } # rof over chunk splinters
    }
    
} # fi treatment of LINKED_CHUNKs
   
@

This creates "flinks" functionality: a link of this kind points to a
local ancor in the LitSource document ("#blabla"), and it will
open all parents of a section/element, then jump right there.
Browser "back" button to return to the location of previous 
reading works.

One thing is this: while in the TOC I need to use "onmousedown" when
I call "open_all_above" JS function, here I must use "onclick".
Looks like a timing problem (?? not sure), and "onclick" seems to heal
it.
"Onclick" for the TOC func fails: it opens the parents on the first click,
then jumps on the second one. The (exact?) opposite of behaviour of the 
"flink".

<<elsif create FLINKs>>=

elsif ($printed_line =~ m!^(.*)\[\[FLINK\s+(.*\S)\s*\]\](.*)$!) { # --FLINKS--

print $1;

my $virtual_id_ancor = $headings_id_hash{$2};    # this works with sections
    my $virtual_id_ancor_href = "#" . $virtual_id_ancor; 
print <<"end_of_clone_sect";
        <a href="$virtual_id_ancor_href"
        onclick="open_all_above($virtual_id_ancor);" 
        ><i>($virtual_id_ancor)</i> $2<sup>flink</sup></a>
end_of_clone_sect

print $3,"\n";

        # a DEBUG printout:
        #for (keys %headings_id_hash) {print "[$_] => $headings_id_hash{$_}<br>\n"}
        #<br>DEBUG: trying to clone [[$2]] whose id is [$virtual_id_ancor] 
        #OR [$headings_id_hash{$2}] and ancor is $virtual_id_ancor_href
} # fisle - end of FLINKs

@



.b. Finally ./b. close the HTML formatting tags at the end of the page:

<<print out>>=

# close the page
        print $html_body_table_end;
        

#--- END OF SCRIPT ---

@

End of the project file



{{{5 .h5. Three chunks excluded from final printing./h5.


This is a working copy without splinter handling

<<if LINKED_CHUNK, create clone-destructclone fieldset - dis - no splinters>>=

    if ($printed_line =~ m!^\s*\[\[LINKED_CHUNK\s+(.*\S)\s*\]\]\s*<br>$!) { # -- CHUNK LINKS --

        my $name_of_linked_chunk = $lt_esc . $lt_esc . $1 . $gt_esc . $gt_esc;

        my $virtual_id = $chunks_id_hash{$name_of_linked_chunk};   
        my $created_clone_div_id = "clonebody" . $clonebody_cnt;
        $clonebody_cnt++;

print <<"end_of_clone_sect";

        <fieldset>
        <legend>
        <font color=green> $lt_esc$lt_esc$1$gt_esc$gt_esc </font>
        </legend>
        <div align=left><b><i><font size=-2 color=darkblue>---- Linked Chunk ---- 
        <a href="javascript:;"
        onmousedown="CreateVirtualNode('$virtual_id', '$created_clone_div_id');" id="virttoc14"
        >[open] </a>
        <a href="javascript:;"
        onmousedown="DeleteVirtualNode('$created_clone_div_id');" id="virttoc15"
        > [close]</a>
         ----</i></b></font>
        </div>
        <div id="$created_clone_div_id" name="virtual_clone">
        </div>
        </fieldset>

end_of_clone_sect

        # a DEBUG printout:
        #for (keys %chunks_id_hash) {print "[$_] => $chunks_id_hash{$_}<br>\n"}

        #<br>DEBUG: trying to clone [[$1]] whose id is [$virtual_id] 
        #OR [$chunks_id_hash{$name_of_linked_chunk}]

    }
    
@





I dislike the cludginess of cloned sections and so disabled them for now.
There is no way to easily change all IDs inside, and so any cloned sect.
conflicts with any link or subsection that is also present and open.
Clicking to close X will close it somewhere else in the document etc.

<<elsif LINKED_SECTION, create clone-destructclone fieldset>>=

=head I DO NOT LIKE SECTION LINKS. DISABLED (at least for now)

    elsif ($printed_line =~ m!^\s*\[\[LINKED_SECTION\s+(.*\S)\s*\]\]\s*<br>$!) { # --SECTION LINKS--

        my $virtual_id = $headings_id_hash{$1};    # this works with sections
        my $created_clone_div_id = "clonebody" . $clonebody_cnt;
        $clonebody_cnt++;

print <<"end_of_clone_sect";
        <fieldset><legend>
        <b><i><font color=darkblue>Linked Section:</i></b></font>
        <font color=green> $lt_esc$lt_esc$1$gt_esc$gt_esc </font></i><br>
        <font size=-2>
        <a href="javascript:;"
        onmousedown="CreateVirtualNode('$virtual_id', '$created_clone_div_id');" id="virttoc14"
        >[open] </a>
        <a href="javascript:;" 
        onmousedown="DeleteVirtualNode('$created_clone_div_id');" id="virttoc15"
        > [close]</a>
        </font></legend>
        <div id="$created_clone_div_id" name="virtual_clone"></div>
        </fieldset>
end_of_clone_sect

        # a DEBUG printout:
        #for (keys %headings_id_hash) {print "[$_] => $headings_id_hash{$_}<br>\n"}
        #<br>DEBUG: trying to clone [[$1]] whose id is [$virtual_id] OR [$headings_id_hash{$1}]

    }

=cut

@



<<print out - simple>>=

# The FULL OUTPUT, the file body:
        print $chunkbuf;

@

}}}5

}}}4

}}}3


.h3. ./h3.

#--------------------------------------------------------------------------------
{{{3 .h3.(VIRTUAL VIEW): Creation of Linked Chunk functionality./h3.
#--------------------------------------------------------------------------------

.i. This is (VIRTUAL VIEW) - a Virtual View on the code with LINKED_CHUNKs./i.

.b. The idea is ./b. to create a complete analogue of a "soft link" on a file system,
for code chunks and/or for sections of the Literate Document.

.b. Why? ./b. - to help a programmer work according to the logic of his thinking.
Supposing the skeleton of a program has been created, and the prog is being further
developed/enhanced. It means not only appending more code somewhere at the end,
but modifications of already existing code.

Same for later maintenance and debugging.

Therefore, I'd like to create a sort of "Virtual Views" on my code, in which
all relevant code chunks are visible, but the text (my thoughts, notes, etc)
is different from the text put around that code before.
Starting on a task, I will collect all needed code chunks, and work from this
"virtual view" on my code.

In other words, I'd like to create a sort of analogue of a new directory
with soft links to existing files. 

The linked code chunks are 'clones' of their originals, and I will use the line
numbers to jump to the relevant places in my editor (in which I "edit locally").
So in the weaved document I logically combine pieces of code from many places
and create a consistent logical View on parts of my code, relevant to some task.

.b.Actual Implementation ./b. below shows how it may work.

{{{4 .h4. First ideas, todo lists ./h4.

1. .s. Test creating a node copy dynamically, copying contents of an existing node. ./s.
2. Test improved navigation: new links/ancors and "jumps" btw TOC and the place.
/3/ .. and how <div>, and <pre><code>... work with my existing folding.

.b. general how to do it ./b.
    
    (a) keep a hash of "section name => id number" for all section 
    headings and all code chunks. 2 hashes, 2 independent unique namespaces.
    
    (b) invent a code word to parse on (e.g. VIRTUAL as the first word in a
    sect name or a chunk name?)
    .ul.
    .i. .b. LINKED_SECTION or LINKED_CHUNK then space char(s) and the verbatim
    name of the cloned chunk or section -- all in double square brackets ./b. ./i.
    ./ul.

    (c) add cloning JS funcs into "JS functions" subsect in the Weaver

    (d) cut into  $chunkbuf the needed refs to the new "clone" and "destroy
    clone" JS functions .i. plus ./i. some colour styling (green?)
    This can be done in the "process code chunks" and "process section 
    headings"

    (e) do something (not sure yet) to make sure the Virtual stuff gets
    into the TOC and is the local clicks are tied to the TOC colouring..


.b. How to do Virtual Nodes, better defined ./b.
.ul..li. (a) add hashes (name => id) for chunks, sections in pass 1 
./li..li. (b) ignore "Virtual" links in pass 1
./li..li. (c) do the subst while'ing over the $chunkbuf in printing
./li../ul.

So, take example from tangler, while'ing over a string and regexp matching.
Test with a relevant string. Add to weaver-printer.
Then add hashes and test with real matches, rather than a token line.

.b. OK, as a result of first tests ./b.

(a) Sections are discouraged, generally. Chunks are encouraged.
(b) sections they clone the whole thread, which can be read from the clone (this is OK);
sections foul up opening-closing other sections bcs clones IDs are the same.
(c) Same true for code chunks, but they do not open the whole hierarchies, so they
do not conflict as easily.
Being flat, one-level clones they can be assigned unique IDs by adjusting their
attribute on cloning

(d) Behaviour with rasHTML marked file not tested yet; seems to have problems (?)

(e) many splinters of one-chunk - not solved yet.

}}}4


{{{4 .h4. Actual implementation (with links to affected code sections ./h4.

.b. 1. ./b. All implementation changes are done belowe "WEAVER" section

.b. 2. ./b. Two functions, to clone a given ID under a "div" with a known ID,
    and to destroy the cloned (actually, all children) under a "div"
    with a know id, 
    were tested separately.

Then, a cut-in chunk was created in

[[LINKED_CHUNK JS script functions]]

where the functions themselves live in the subsection below this one

[[LINKED_CHUNK Javascript: node clone and destroy clone]]

The functions were created from some scrib, and can actually be combined

.b. .i. todo ./i. ./b. The "create" function needs an addition of a unique ID for the cloned
section. Otherwise the cloned section preserves the same ID as another clone,
or the original section. If those are open, closing a later clone will close
the first ones. Which is bery untidy and not possible to fix cleanly.

The problem is much less in case of LINKED_CHUNKs, as the original chunks
are not collapsible and aren't affected. Similar cnflict is poss only btw
2 clones, if those are opened at the same time.

Assignement of a unique Id upon creation will fix it.

.b. 3. ./b. Next, a number of changes was required in the code where formation of the
output $chunkbuf takes place.

(a) for code chunk links, I had to add creation of unique IDs and collecting
a hash of "chunk name => ID", to use later as arguments to the JS funcs.

this lives in 
[[LINKED_CHUNK process code chunks]]

I trew away the idea of assigning IDs to the "fieldset" elements themselves, 
and wrapped all code sections in "div" elements, which got the unique IDs,
generated from word "codechunk" and the line number.

Matching on a chunk def puts in the div, and a match on the end of code section
closes it.

.b. Same for LINKED_SECTIONs ./b. - in here:
[[LINKED_CHUNK_1 process section headings]]


.b. 4. ./b.  Next, a place to cut the interpolation in is needed.
The algorithm of my weaver constructs an output string $chunkbuf as it goes
down a literate source file, with all necessary formatting and calls to JS
folding funcs.

So, I decided to combine a second pass with the next stage, that of printout
of $chunkbuf. 

[[LINKED_CHUNK_7 print out]]

Again, a cut-in was created and a simple print-out renamed and put out of the way. 

[[LINKED_CHUNK print out - simple]]

(Actually, I can switch btw them on a configuration var, sth like "enable virtual
linking", but let's keep this functionality always on for now.

Instead of just dumping it, I now iterate line by line on this buffer string
in memory with regular expressions

.b. Two words are reserved now, and the format is: ./b. 
/i.e. to put a link into your Literate Source, which will then be prettyprinted/
.ul.
  .i.[[LINKED_CHUNK|LINKED SECTION, space(s),name of chunk/sectin verbatim]]./i.
./ul.
This must be on a line by itself, no other nonspace chars are allowed. Space before
and after is ignored.

[[LINKED_CHUNK print out - with virtual nodes]]

[[LINKED_CHUNK if LINKED_CHUNK, create clone-destructclone fieldset]]

I actually decided LINKED_SECTIONs were not good and disabled the feature.
The code is put aside in here:

[[LINKED_CHUNK elsif LINKED_SECTION, create clone-destructclone fieldset]]

[[LINKED_CHUNK elsif create FLINKs]]

.b. 5. Putting links in the Chunks Indes ./b.
is below in its own subsection

.b. 6. FOLDING_LINKs ./b.

.. are the same as "LINKED_SECTION", they just refer to a different JS function

.b. FLINKs ./b.
Instead of section links, which open a fieldset frame in which a clone of the
section is visible, I decided to offer the user FLINKS.
They work exactly like hyperlinks on ancors in the same document. However
simple links are not accessible if they are in folded sections. FLINKs will
open a section and all parents above it, so it becomes visible, then jump
to it as with a normal link.
Broweser's "back" button works and allows one to return to the previously
browsed spot. Make sure to close the flinked sections after reading, or have
your nicely folded document unnecessary unfolded.


.i..b. --- to be finished --- ./b../i.


.b. bugs and unfinished stuff ./b.
.ul..li. .s. splinters of the same chunk./s.
./li..li. .s. unique ID for just cloned div./s.
./li..li. .s. add first line number for sections (none there), ./s. .i.-- sections disabled./i.
and for chunks .i. not needed , as all other lines of a chunk are numbered anyway./i.
./li..li. .s.add chunk splinter numbering in non-links, in real chunks./s.
./li..li. .s.add links to the chunk index./s.
./li../ul. 

}}}4

{{{4 .h4.  Node Cloning/destruction - JavaScript functions ./h4.
Two Javascript functions do provide node cloning and destroying:

<<Javascript: node clone and destroy clone>>=

// DELETING a CLONE -- this works too:
function DeleteVirtualNode(someid) {
    
    elem = document.getElementById(someid);

    while (elem.childNodes.length > 0) {
    elem.removeChild(elem.firstChild);
    }  
}

// CREATING a CLONE - this works
/* for clean execution - run "delete" before any creation */
function CreateVirtualNode(clone_from_id, append_to_id){
        DeleteVirtualNode(append_to_id);

    if ( document.getElementById(append_to_id).childNodes.length == 0 ) {
    var elem = document.createElement("div");

    elem = document.getElementById(clone_from_id).cloneNode(1);
    elem.style.display = 'block';
    document.getElementById(append_to_id).appendChild(elem); 

    } /* fi - do not create duplicates of the cloned node */ 
    
}

@

Then, using them from an html doc is like this:

<<Node clone/destroy - test.html>>=

@

}}}4

{{{4 .h4. Numbering Chunk Splinters and Putting Links in the Chunks Index ./h4. 

(a) .b. links in chunks index ./b.
the format of the string accumulated in @indbuf
is simple, a string
    "<<chunk name>> 34" or "<<chunk name>> 45<sub>def</sub>"
and can be glimpsed above in 

[[LINKED_CHUNK process code chunks]]

That string is later disassembled in 

[[LINKED_CHUNK_5 print out]]

Pretty much I used a string which later will be split on a space
in place of a 2-member list or other such structure to keep 2
pieces of information, a chunk name and the section where it is used.

SO: all I have to do is form and push into @indbuf a string
    "<<chunk name>> 22<sub>link</sub>"
and, because linked chunks are inside doc section bodies, this
parsing must be added here:

[[LINKED_CHUNK body HTML formatters]]

outside any "markup formatters". Var $section_num is set in headings
processing and is available for me in the docsection body.


(b) .b.Adding numbering to splinters./b. of the same chunk is done trivially in
"process code chunks" linked above, using the length of the list in 
%chunks_id_hash as the splinter number.


}}}4

{{{4 .h4. Tests of a chunk link, section link and flink functionality ./h4.

.b. 1. Section links ./b.

[[LINKED_SECTION ADD-ON: Virtual Nodes]]

and another section

[[LINKED_SECTION Node Cloning/destruction - JavaScript functions]]

.b. .i. ...disabled now... ./i. ./b.

.b. 2 and these are code chunks ./b.  not sections

[[LINKED_CHUNK Makefile for split MOLLY-top and MOLLY-bottom]]

.. and another one with 2 splinters

[[LINKED_CHUNK accumulate result in a buffer]]

.b. 3. ..and FLINKs./b.

standalone flinks on their own lines:
[[FLINK Node Cloning/destruction - JavaScript functions]]
[[FLINK My workflow - literate programming with Molly]] 

2. multiple flinks on teh same line

Before the test line ...........
OK, two flinks: [[FLINK (VIRTUAL VIEW): example of additional markup - dotHTML]] and
[[FLINK test two - from realistic paper, partially failing]] on two lines
......... after which another line is coming

Seems without "while-ing" over matches on the same line (or "for"-ing over the list)
it's not going to work.
So: enclosing text OK, but ONE FLINK PER LINE until I bother to make the match better.
(see tangler test script for the idea)

..more to jump around..
[[FLINK Mathematical Formulae - MathML inside Mollified Literate source files]]
/end of the tests/

}}}4

}}}3

.h3. ./h3.

#-------------------------------------------------------------
{{{3 .h3. (VIRTUAL VIEW): example of additional markup - dotHTML./h3.
#-------------------------------------------------------------

.i. This is (VIRTUAL VIEW) - a Virtual View on the code with LINKED_CHUNKs./i.

.i. This subsection is a good example of the "out-of-order" processing that is
enabled in Literate Programming. Pieces for this subsection come from different
parts of the script and are laid out logically rather than in the order imposed
by the machine ./i.


.b. 1. the elsif clause ./b.The default "rawHTML" formatter opened an "if" clause, 
matching on the configuration variable $weave_markup. 

[[LINKED_CHUNK body HTML formatters]]

[[LINKED_CHUNK rawHTML formatter]]

All other formatters (and this is extensible, of course) can be added here to the chunk
below, and the actual implementation (in its own subsection, the regex line-by-line 
filters/susbtitutions in the  doc chunk bodies, must be wrapped in the same two lines:

<<my_new_markup - disabled - add to "other body formatters" below>>=

        elsif( $weave_markup eq "name_of_your_markup" ) { # comment

                #...........your line-by-line transforming.
                #..........or filtering expressions here

                #$chunkbuf .= $_;

        } #fisle for name_of_your_markup

@

This new markup of your of course must be added to the 'other body formatters' chunk.


.b. The actual working example./b. below implements a "dotHTML" formatter (this very 
file is marked with it rather than the default HTML).

This is  my own kind of simple markdown for quick typing.  

<<other body formatters>>=

        <<dotHTML formatter>>
        
@


.b. 2. Adding a new markup language involves  two more obligatory changes. ./b.

1. You'll have to mention name of your new markup as an option to the user (no new vars 
are necessary) in  the "mollifying template" in part II .b. "HOW TO USE MOLLY.pl" ./b.

[[LINKED_CHUNK full MOLLY template]]

2. And you'll have to add that into the "template project file" distributed with MOLLY.pl,
again  for other people to know and notice.

3. You might also add that new possible value for the config var  at the top of this very 
LitSource document, although it has been marked in dotHTML already and is not 
going to reform ;)) . You might want it because this file serves as the central example
itself, and so some people might copy the config section from here to start their 
own projects.


#------------------------------------------
{{{4 .h4. dotHTML formatter ./h4.
#------------------------------------------

.b. .i. .<. u .>. Note: .<. /u .>. ./i. ./b.
.ul.
Implementation of a simple "dotHTML" markup  in which angle brackets are substituted
with dots for some of the commonest HTML tags (to speed up typing and eliminate typos).

To add your own markup, such as "wiki" or one of the "markdowns", copy this section
and adjust contents. The idea is that MOLLY iterates over documentation sections
line by line, so your filter can use regular expressions for a line-by-line processing
of the documentation chunk text.

Make sure your new "markdown" does not interfere with the 3 major escapes used in 
the "mollified" Literate Source file: the double angle brackets and the "at" symbol
(imposed by "noweb" tools markup) and the double backticks ( MathML interpretation
if you enable it) plus words "a m a t h" and "e n d a m a t h" without spaces btw letters.
./ul.

.b. .i. ...original implementation notes below.. ./i. ./b.

It is quite simple, and I add needed markup "on the fly", as and when I need it.
The idea is to eliminate angle brackets in html tags, which I cannot type without errors
when typing fast, and substitute those with simple "dots", which do not requre switching
keyboard registers.

.s. This formatting also has problems with escapes.

Whether it's the fault of HTML standards or their implementation, web browsers continue to
react to html formatting .b.even inside the "pre" tags./b., which is obviously insane.
Therefore if your code chunks contain any HTML tags (e.g. processed by your code), you'll
need to escape them to display correctly.
And if you use sth like "& g t ;" in your code, the web page will also lie to you.
This is insane, and there is no good quick solution, one would have to use lengthy escape
tables etc. etc. ./s.

.ul. /* OFFTOPIC, ALSO: add < > and & escapes to the weaved code chunks -- for proper display */
    --- DONE ---
    These escapes exist in two places in the Weaver code:
    "process code sections"  and "body HTML formatters"
./ul.

.br.
This prototype script fails in some (unimportant to me)  cases.
dotHTML markup would also fail if your programming language uses dot-symbol-dot sequences
Mine do not.

The formatter is pretty straightforward, but sequencing of regexps is important.
One also needs to remember that once escaped, angle brackets are non-existent any more, 
and so the subsequent regexps must match on "& l t ;" not on the angle bracket in some
cases.

<<dotHTML formatter>>=

        elsif( $weave_markup eq "dotHTML" ) {   # dotHTML formatter here

              s/^=begin.*$//;   # - eliminate perl escaping, start
              s/^=cut.*$//;             # - eliminate perl escaping, end
              #s/^{{{\d+(.*)$/$1/;      # - eliminate vim folding markup, start 
                                        # - dummy, as it is killed in "headings" processing 
              s/^}}}\d+//;      # - eliminate vim folding markup, end

                s/&/&amp;/g;    # escape &
                s/</&lt;/g;     # escape <
                s/>/&gt;/g;     # escape >


              # Paragraphs and line breaks are automatic now:
              # ... unless we are dealing with the "preformat" tag
                #--note! that ranges do not work here
                $in_pre_tag = 1 if (m!\.pre\.!);
                $in_pre_tag = 0 if (m!\./pre\.!);;

                s/\.(\/?)pre\./<$1pre>/g;

            unless ($in_pre_tag) {
            (m/^\s*$/) and s/$_/<p>\n/
            or s/\n/<br>\n/;
            }
@

Now the regexps to cut out "#--------" lines and substitute "dot-notation" with angle brackets:

<<dotHTML formatter>>=

              # originally I separated header from the body with such a line
              #s/^#-----.*/starting the table here/;
              s/^#-----.*//;


                # add more here

                s/\.(\/?)b\./<$1b>/g;
                s/\.(\/?)i\./<$1i>/g;
                s/\.(\/?)ul\./<$1ul>/g;
                s/\.(\/?)li\./<$1li>/g;
                s/\.(\/?)ol\./<$1ol>/g;
                s/\.(\/?)s\./<$1s>/g;
                s/\.(\/?)div\./<$1div>/g;
                s/\.br\./<br>/g;
                s/\.p\./<p>/g;
                s/\.sp\./&nbsp;/g;

                s/\.(\/?)tab\./<$1ul>/g;        # "tabbing" with "ul"


                # this is some bullshit ???
                s/\.hr\./<hr /g;
                s/\.\/hr\./>/g;

                s!\.a\.(.+?)\.\/a\.!<a href=$1>$1</a>!g;

                # rudimentary &nbsp; s p a c i n g &nbsp (one word only)
                #s!\.x\.(.+?)\./x\.!join " ","&nbsp;&nbsp;",(split //, $1),"&nbsp;&nbsp;"!eg;

                # slightly better spacing (phrases, too):
                # although redundant  with more work than is needed
                if ( m!(\.x\.)(.+?)(\./x\.)!g) {
                    s!(\.x\.)(.+?)(\./x\.)!join " _ ", $1, (split / /, $2), $3!eg;
                    s!\.x\.(.+?)\./x\.!join " ", (split //, $1)!eg;
                    s!  _  ! &nbsp; !g;
                } 
@

.b. BUG! ./b. This URL breaks my "ancor" dot-tags: 
     .pre. http://www.cs.tufts.edu/~nr/noweb ./pre.
.b. Got it ./b. The substitutions above must explicitly use "slash" for the closing HTML tag
in the regexps - otherwise they match stuff like ".cs." inside URLs

If an HTML tag is not in the above chunk, either add it or write with the two following generic
tags (slow and not nice on your fingers, though). You might need to use them also when your HTML
tag conntains some options, and so is not of generic simple type.

<<dotHTML formatter>>=

                # generic for all tags with options
                s!\.&lt;\. !<!g;
                s! \.&gt;\.!>!g;


                $chunkbuf .= $_;

        } # fisle - end of "dotHTML" body formatter

@


{{{5 .h5. test of &nbsp;&nbsp; s p a c i n g &nbsp;&nbsp; regexp ./h5.

1. Hell, the thing is multiple sp /sp markups on one line. I need to iterate over
each separately and then put them in their right places, hedging with "nbsp"-ces each word.

2. The invocation is OK for $str_sp, but for boundaries of the string and word boundaries, 
they must be hedged too

3. Could subst spaces-betwen-words to sth like _ (space-underscore-space) as placeholders,
then subst them into nbspx2 in a following regexp

4. This tagging can be done cleanly and span severl lines if I do it in a different place - 
not while still reading the Literate Source file line by line, .x.but later./x. when
$chunkbuf is completely formed in RAM.


<<spacing.pl>>=
#!/usr/bin/perl
#
$str = $ARGV[0];
$str_sp = $ARGV[0];
#print "string is $str\n";

    # "sp" dot-markup to intersperse spaces: the basic formula
    #$spaced_str =~ s!(asdf)!join " ", (split //, $1)!eg;
    #
    
    # -- idea 3 from above: this works but is cumbersome --
    #$str_sp =~ s!(\.x\.)(.+?)(\./x\.)!join " _ ", $1, (split / /, $2), $3!eg;
    #$str_sp =~ s!\.x\.(.+?)\./x\.!join " ", (split //, $1)!eg;
    #$str_sp =~ s! _ ! &nbsp;&nbsp; !g;

    # -- first get the matching string in "if", then massage it:
    if ($str_sp =~ m!(\.x\.)(.+?)(\./x\.)!g) {
        $str_sp =~ s!(\.x\.)(.+?)(\./x\.)!join " _ ", $1, (split / /, $2), $3!eg;
        $str_sp =~ s!\.x\.(.+?)\./x\.!join " ", (split //, $1)!eg;
        $str_sp =~ s!  _  ! &nbsp; !g;
    }

    #print "\nthe string is $str\n";
    print "\nthe str_sp is $str_sp\n\n";
@

}}}5    

}}}4

}}}3

.h3. ./h3.

#-------------------------------------------------------------
{{{3 .h3. (VIRTUAL VIEW): ASCIIMathML and LaTeXMathML inside Molly  ./h3.
#-------------------------------------------------------------

.i. This is (VIRTUAL VIEW) - a Virtual View on the code with LINKED_CHUNKs./i.

.b. 1 ./b. .i. STATUS ./i. - Ok, but may be brittle if escapes (double backticks)
get in conflict with the target prog language or some "markdown" 
introduced later into Molly.

Also - need to double-backtick LaTeX snippets now  because of my need to
preserve $ for programming language use.

.b. WILL NEED TO DISTRIBUTE THE MODIFIED LIBRARY NOW with Molly ./b.

.b. 2  Implementation ./b.
.. was straightforward: I simply cut in an invocation of ASCIIMathML.js into the header of 
my generated output folding-HTML file. That's it.

Then I tested the lib and found that it reacts to backticks and $ signs in my code sections etc.
Then I made a customized copy.

If some symbols are missing, you can customize it by simply adding a line to teh MOLLY.weave
source code and re-tangling the MOLLY.pl

Here is how:

{{{4 .h4. Extending the lib with missing LaTeX or HTML symbols ./h4.

Documentation to the library explains how to add lines in JavaScript to your html file which will
extend the list of symbols known to the library.
.a. http://www1.chapman.edu/~jipsen/mathml/asciimathextend.html  ./a.
A copy of the page:
.ul.
.b.ASCIIMathML.js: Extending the symbol table./b.

The standard symbol table of ASCIIMathML.js does not contain many symbols. 
It can be extended by adding additional symbols on any webpage that requires
them. This is done by adding a few lines of JavaScript code.

.b. 1. For example ./b. , suppose we want to add symbols for "not less or equal"
    and "not greater or equal".

.ul. .li. We first have to find the four-digit hexadecimal Unicode value for these symbols
by looking them up at, say, 
.a. http://www.w3.org/TR/MathML2/chapter6.html#chars.entity.tables ./a.

./li. .li. Next we have to decide what input strings we want to associate with these
symbols, say "!<=" and "!>=".

./li. .li. Finally we add the following lines to the head or body of our HTML file:
.pre.
<script type="text/javascript">
define("!<=","\u2270")
define("!>=","\u2271")
</script>
./pre.
./li. ./ul.

Here we test the modified symbol table: a !<= b !>= c produces ``a !<= b !>= c``

.b. 2. To add a symbol to the LaTeX commands ./b., use the following alternate syntax:

.pre.
<script type="text/javascript">
newcommand("\\nle","\u2270")
newcommand("\\nge","\u2271")
</script>
./pre.

Now \$a \nle b \nge c\$ produces $a \nle b \nge c$.


.b. 3. If you know the numeric entity reference ./b.  of the symbol you want 
to use on an ASCIIMathML webpage, you can also refer to the symbol .b. directly ./b. 
by using that reference. 

E.g &#x2270; produces  ``&#x2270;`` . If a symbol is only used occasionally, this is certainly 
the simplest way to include it. 
./ul.

.i. /error/ My copy of the page in MOLLY does not let ampersand invocations through 
ALSO: my copy of the lib, in which I mechanically changed single backticks to double backticks
may have clobbered something, too.
CHECK IT. ./i.

Therefore, .b. to extend it in MOLLY ./b., just continue this chunk here (the "script .." --- "/script"
tags are already provided in the code, do NOT retype them, input only the contents):

<<HTML head section javascript add-ons>>=

//type in commands to extend ASCIIMathML.js here;
// no "script" "/script" wrappers needed
// ....NOT FUNCTIONAL YET.....

@

}}}4

#--------------------------------------------------------------------------
{{{4 .h4. Testing ASCIIMathML.js -- OK partially ./h4.
#--------------------------------------------------------------------------

#--------------------------------------
{{{5 .h5. test one - simple ./h5.
#--------------------------------------

Let's test if it works by adding some formulae here:
.b. I changed the ASCIIMathML.js to use .i. double backticks ./i. as escapes ./b.
So strings wrapped in double-backticks or chunks of text with formulae inside
"a m a t h" .... "e n d a m a t h" are the only ones that should be interpreted.

NOTE: they appear in blue, so errors, if the math lib picks up other parts of 
your LitProg file unnecessarily and mangles them, one could see it with
"expand all" and checking for blue insertions.

NOTE 2: the math lib stops working between the "pre" - "/pre" tags

NOTE 3: Rolling the mouse over the interpreted formula will display
a baloon with the ascii coding of the expression (very convenient)

.ul.
Let's try some interesting formulas: ``E=m c^2``
and ``e^(i pi)=-1`` 
and ``AA x in CC (sin^2x+cos^2x=1)`` 
and one more: ``sum_(i=1)^n i^3=((n(n+1))/2)^2``

(add your own -- note that text-tokens are only recognized if separated by spaces)
./ul.
OK, here we are - in between 2 math sections

.ul.

amath
Example: Solving the quadratic equation.
Suppose a x^2+b x+c=0 and a!=0. We first divide by \a to get x^2+b/a x+c/a=0. 

Then we complete the square and obtain x^2+b/a x+(b/(2a))^2-(b/(2a))^2+c/a=0. 
The first three terms factor to give (x+b/(2a))^2=(b^2)/(4a^2)-c/a.
Now we take square roots on both sides and get x+b/(2a)=+-sqrt((b^2)/(4a^2)-c/a).

Finally we move the b/(2a) to the right and simplify to get 
the two solutions: x_(1,2)=(-b+-sqrt(b^2-4a c))/(2a) 
endamath

./ul.

..and now let's check some LaTeX constants:
.ul.
.pre. \int .sp. \oint .sp. \partial .sp. \nabla ./pre. 
``\int`` .sp. ``\oint`` .sp. ``\partial`` .sp. ``\nabla``
.pre. \pm .sp. \emptyset .sp. \infty .sp. \aleph ./pre.
``\pm`` .sp. ``\emptyset`` .sp. ``\infty`` .sp. ``\aleph`` 
.pre. |\ldots| .sp. |\cdots| ./pre.
``|\ldots|`` .sp. ``|\cdots|`` 
.pre. |\ | .sp. |\quad| .sp. \diamond ./pre.
.sp. ``|\ |`` .sp. ``|\quad|`` .sp. ``\diamond`` 
./ul. 

.i. .b. end of math test ./b. ./i.

}}}5

#----------------------------------------------------------------------
{{{5 .h5. test two - from realistic paper, partially failing  ./h5.
#----------------------------------------------------------------------

.<. hr width=30% align=left .>.

.b. ..MORE from some paper: ./b.

.b. 1. Freudental Formula ./b.
``mult(\xi)=\frac{2}{(\mu+\rho|\mu+\rho)-(\xi+\rho|\xi+\rho)}\sum_{\alpha\in\Delta^{+}} mult(\alpha) \sum_{k=1}^{\infty}mult(\xi+k\alpha)(\xi+k\alpha|\alpha)``
It includes roots ``\Delta=\left\{k\delta+\alpha|k\in Z,\; \alpha\in \Delta_0\right\}``
positive roots ``\Delta^{+}=\{k\delta+\alpha|k\geq 0,\; \alpha\in \Delta_0^{+}\}\cup \{k\delta+\alpha|k\geq 1,\; \alpha\in \Delta_0\setminus \Delta_0^{+}\}``

.b. The above is rendered correctly by ASCIIMathML.js and LaTeXMathML.js ./b. 
libraries from the standalone Editor.It is rendered correctly with 
ASCIIMathML_with_modified_escapes.js from Molly 
(Testing with LaTeXMathML.js requires wrapping the formulas in dollar signs)


.b. 2. The following long formulas .i. fail on both ./i. ASCII and LaTeX libraries ./b. tested
from the standalone Editor and ASCII_with_modified_escapes from Molly
The number of incorrect renderings is small (1 or 2 elements left unrendered), and
suggests those elemens are simply missing in definitions.


 The idea is to use recurrent relations for anomalous branching
coefficients based on the summation over the special set of vectors 
``\Gamma_{\fr ak{a}\subset \fr ak{g}}``  called ````fan''. We
need to introduce some notations. Let's consider the reduction of the
representation of the affine Lie algebra ``\math \fr ak{g}`` to 
representations of affine Lie algebra ``\math \fr ak{a}``. By
``\pi_{\math \fr ak{a}}`` we denote the projection of the root space
``\fr ak{h}_{\fr ak{g}}^{\ast } to \fr ak{h}_{\fr ak{a}}^{\ast }``. The
set ``\Gamma_{\fr ak{a}\subset \fr ak{g}}``  is introduced as the
combination of projection of positive roots ``\Delta^{+}`` of algebra
``\fr ak{g}`` using formulae:

.b. .i. The above paragraph is OK ./i. ./b.
The following is rendered incompletely:

{eq:7}

 ``\prod_{\alpha \in \left( \pi _{\fr ak{a}}\circ \Delta ^{+}\right) }\left( 1-e^{-\alpha }\right) ^{\mathrm{{mult}\left( \alpha \right) -{mult}}_{\fr ak{a% }}\mathrm{\left( \alpha \right) }}=-\sum_{\gamma \in \Phi _{\fr ak{a}\subset \fr ak{g}}}s\left( \gamma \right) e^{-\gamma }``.  

 {{{5

{eq:18}

 `` \Phi _{\frak{a}\subset \frak{g}}=\left\{ \gamma \in P_{\frak{a}}\mid s\left( \gamma \right) \neq 0\right\} ``;  

 
{eq:19}

 `` \Gamma_{\frak{a}\subset \frak{g}}=\left\{ \xi -\gamma _{0}|\xi \in \Phi _{% \frak{a}\subset \frak{g}}\right\} \setminus \left\{ 0\right\} .`` 

 
{recurrent-relation}

``  k_{\xi }^{\left( \mu \right) }=-\frac{1}{s\left( \gamma _{0}\right) }\left( \sum_{w\in W}\epsilon \left( w\right) \delta _{\xi ,\pi _{\frak{a}}\circ \left( w\circ (\mu +\rho )-\rho \right) +\gamma _{0}}+\sum_{\gamma \in \Gamma _{\frak{a}\subset \frak{g}}}s\left( \gamma +\gamma _{0}\right) k_{\xi +\gamma }^{\left( \mu \right) }\right)   
``
{{{5

Fan doesn't depend on the module, but
is determined by the injection of sub-algebra into the algebra. If 
sub-algebra is Cartan sub-algebra, this relation gives...




.<. hr width=30% align=left .>.



}}}5

}}}4

}}}3


}}}2



